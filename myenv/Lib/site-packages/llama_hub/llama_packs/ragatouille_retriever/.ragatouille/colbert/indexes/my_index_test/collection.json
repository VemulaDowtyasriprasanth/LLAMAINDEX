[
  "Similarly,\nthey-axis varies between approximately 50 mil-\nliseconds per query up to 250 milliseconds (mostly\nunder 150 milliseconds) using our relatively simple\nPython-based implementation.\nDigging deeper, we see that the best quality\nin these metrics can be achieved or approached\nclosely with around 100 milliseconds of latency\nacross all three datasets, despite their various sizes\nand characteristics, and that 2-bit indexing reliably\noutperforms 1-bit indexing but the loss from more\naggressive compression is small.\nD LoTTE\nDomain coverage Table 9 presents the full dis-\ntribution of communities in the LoTTE dev dataset.\n8These settings are selected based on preliminary explo-\nration of these parameters, which indicated that performance\nfor larger probe values tends to require scoring a larger num-\nber of candidates.",
  "As\nKhashabi et al. (2021) hypothesize, Google Search\nlikely maps these natural queries to their answers\nby relying on a wide variety of signals for rele-\nvance, including expert annotations, user clicks,\nand hyperlinks as well as specialized QA compo-\nnents for various question types with access to the\npost title and question body . Using those annota-\ntions as ground truth, we evaluate the models on\ntheir capacity for retrieval using only free text of\nthe answer posts (i.e., no hyperlinks or user clicks,\nquestion title or body, etc.), posing a signi\ufb01cant\nchallenge for IR and NLP systems trained only on\npublic datasets.\nForum Queries. We collect the forum queries\nby extracting post titles from the StackExchange\ncommunities to use as queries and collect their\ncorresponding answer posts as targets. We select\nquestions in order of their popularity and sample\nquestions according to the proportional contribu-\ntion of individual communities within each topic.Q:what is the difference between root and stem in lin-\nguistics? A:A root is the form to which derivational\naf\ufb01xes are added to form a stem.",
  "This\nadapts the DPR (Karpukhin et al., 2020) evaluation\ncode.10We use the preprocessed Wikipedia Dec\n2018 dump released by Karpukhin et al. (2020).\nFor out-of-domain evaluation, we elected to fol-\nlow Thakur et al. (2021) and set the maximum\ndocument length of ColBERT, RocketQAv2, and\nColBERTv2 to 300 tokens on BEIR and LoTTE.\nFormal et al. (2021a) selected maximum sequence\nlength 256 for SPLADEv2 both on MS MARCO\nand on BEIR for both queries and documents, and\nwe retained this default when testing their system\non LoTTE. Unless otherwise stated, we keep the\ndefault query maximum sequence length for Col-\nBERTv2 and RocketQAv2, which is 32 tokens. For\nthe ArguAna test in BEIR, as the queries are them-\nselves long documents, we set the maximum query\nlength used by ColBERTv2 and RocketQAv2 to\n300.",
  "We use KL-\nDivergence as ColBERT produces scores (i.e., the\nsum of cosine similarities) with a restricted scale,\nwhich may not align directly with the output scores\nof the cross-encoder. We also employ in-batch\nnegatives per GPU, where a cross-entropy loss is\napplied to the positive score of each query against\nall passages corresponding to other queries in the\nsame batch. We repeat this procedure once to re-\nfresh the index and thus the sampled negatives.\nDenoised training with hard negatives has been\npositioned in recent work as ways to bridge the\ngap between single-vector and interaction-based\nmodels, including late interaction architectures like\nColBERT. Our results in \u00a75 reveal that such super-\nvision can improve multi-vector models dramati-\ncally, resulting in state-of-the-art retrieval quality.\n3.3 Representation\nWe hypothesize that the ColBERT vectors cluster\ninto regions that capture highly-speci\ufb01c token se-\nmantics. We test this hypothesis in Appendix A,\nwhere evidence suggests that vectors correspond-\ning to each sense of a word cluster closely, with\nonly minor variation due to context.",
  "We test this hypothesis in Appendix A,\nwhere evidence suggests that vectors correspond-\ning to each sense of a word cluster closely, with\nonly minor variation due to context. We exploit\nthis regularity with a residual representation that\ndramatically reduces the space footprint of late in-\nteraction models, completely off-the-shelf without\narchitectural or training changes. Given a set of\ncentroids C, ColBERTv2 encodes each vector vas\nthe index of its closest centroid Ctand a quantized\nvector ~rthat approximates the residual r=v\u0000Ct.\nAt search time, we use the centroid index tand\nresidual ~rrecover an approximate ~v=Ct+ ~r.\nTo encode ~r, we quantize every dimension of r\ninto one or two bits. In principle, our b-bit encod-\ning of n-dimensional vectors needs dlogjCje+bn\nbits per vector. In practice, with n= 128 , we use\nfour bytes to capture up to 232centroids and 16 or\n32 bytes (for b= 1orb= 2) to encode the resid-\nual.",
  "However, since\nall posts are submitted by anonymous users we do\nnot have demographic information regarding the\nidentify of the contributors. All posts are written\nin English.\nPassages As mentioned in \u00a74, we construct\nLoTTE collections by selecting passages from the\nStackExchange archive with positive scores. We\nremove HTML tags from passages and \ufb01lter out\nempty passages. For each passage we record its\ncorresponding query and save the query-to-passage\nmapping to keep track of the posted answers corre-\nsponding to each query.\nSearch queries We construct the list of LoTTE\nsearch queries by drawing from GooAQ queries\nthat appear in the StackExchange post archive. We\n\ufb01rst shuf\ufb02e the list of GooAQ queries so that in\ncases where multiple queries exist for the same\nanswer passage we randomly select the query to\ninclude in LoTTE rather than always selecting the\n\ufb01rst appearing query. We verify that every query\nhas at least one corresponding answer passage.",
  "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2020. Distilling Dense Representations for Rank-\ning using Tightly-Coupled Teachers. arXiv preprint\narXiv:2010.11386 .\nXiaorui Liu, Yao Li, Jiliang Tang, and Ming Yan.\n2020. A double residual compression algorithm\nfor ef\ufb01cient distributed learning. In The 23rd In-\nternational Conference on Arti\ufb01cial Intelligence and\nStatistics, AISTATS 2020, 26-28 August 2020, On-\nline [Palermo, Sicily, Italy] , volume 108 of Proceed-\nings of Machine Learning Research , pages 133\u2013143.\nPMLR.\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2021. Sparse, Dense, and Atten-\ntional Representations for Text Retrieval. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:329\u2013345.",
  "Cluster ID Most Common TokensMost Common Clusters Per Token\nToken Clusters\n917\u2018photos\u2019, \u2018photo\u2019, \u2018pictures\u2019,\n\u2018photographs\u2019, \u2018images\u2019,\n\u2018photography\u2019, \u2018photograph\u2019\u2018photos\u2019 Photos-Photo, Photos-Pictures-Photo\n\u2018photo\u2019 Photo-Image-Picture, Photo-Picture-Photograph, Photo-Picture-Photography\n\u2018pictures\u2019 Pictures-Picture-Images, Picture-Pictures-Artists, Pictures-Photo-Picture\n216932\u2018tornado\u2019, \u2018tornadoes\u2019, \u2018storm\u2019\n\u2018hurricane\u2019, \u2018storms\u2019\u2018tornado\u2019 Tornado-Hurricane-Storm, Tornadoes-Tornado-Blizzard\n\u2018tornadoes\u2019 Tornadoes-Tornado-Storms, Tornadoes-Tornado-Blizzard, Tornado-Hurricane-Storm\n\u2018storm\u2019 Storm-Storms, Storm-Storms-Weather, Storm-Storms-Tempest\nTable 6: Examples of clusters taken from all MS MARCO passages. We present the tokens that appear most\nfrequently in the selected clusters as well as additional clusters the top tokens appear in.",
  "We average latency across three\nruns of the MS MARCO dev set and the LoTTE\n\u201csearch\u201d queries. Search is executed using a Titan\nV GPU on a server with two Intel Xeon Gold 6132\nCPUs, each with 28 hardware execution contexts.\nThe \ufb01gure varies three settings of ColBERTv2.\nIn particular, we evaluate indexing with 1-bit and\n2-bit encoding (\u00a73.4) and searching by probing the\nnearest 1, 2, or 4 centroids to each query vector\n(\u00a73.5). When probing probe centroids per vector,\nwe score either probe\u0002212orprobe\u0002214candi-\ndates per query.8\nTo begin with, we notice that the quality reported\non the x-axis varies only within a relatively narrow\nrange. For instance, the axis ranges from 38.50\nthrough 39.75 for MS MARCO, and all but two of\nthe cheapest settings score above 39.00. Similarly,\nthey-axis varies between approximately 50 mil-\nliseconds per query up to 250 milliseconds (mostly\nunder 150 milliseconds) using our relatively simple\nPython-based implementation.",
  "Unless otherwise stated,\nwe compress ColBERTv2 embeddings to b= 2\nbits per dimension in our evaluation.\n5.1 In-Domain Retrieval Quality\nSimilar to related work, we train for IR tasks on MS\nMARCO Passage Ranking (Nguyen et al., 2016).\nWithin the training domain, our development-set re-\nsults are shown in Table 4, comparing ColBERTv2\nwith vanilla ColBERT as well as state-of-the-art\nsingle-vector systems.\nWhile ColBERT outperforms single-vector sys-\ntems like RepBERT, ANCE, and even TAS-B, im-\nprovements in supervision such as distillation from\ncross-encoders enable systems like SPLADEv2,MethodOf\ufb01cial Dev (7k) Local Eval (5k)\nMRR@10 R@50 R@1k MRR@10 R@50 R@1k\nModels without Distillation or Special Pretraining\nRepBERT 30.4 - 94.3 - - -\nDPR 31.1 - 95.2 - - -\nANCE 33.0 - 95.9 - - -\nLTRe 34.1 - 96.",
  "2021. Domain-matched Pre-training\nTasks for Dense Retrieval. arXiv preprint\narXiv:2107.13602 .\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2022.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation. In International\nConference on Learning Representations .\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5835\u20135847, Online. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016.",
  "On the NQ dev set, ColBERT-QA\u2019s suc-\ncess@5 (success@20) dropped only marginally\nfrom 75.3% (84.3%) to 74.3% (84.2%) and\nits downstream Open-QA answer exact match\ndropped from 47.9% to 47.7%, when using 2-bit\ncompression for retrieval and using the same check-\npoints of ColBERT-QA otherwise.\n7We contrast this with an early implementation of com-\npression for ColBERT, which used binary representations as\nin BPR (Yamada et al., 2021a) without residual centroids,\nand achieves 34.8% (35.7%) MRR@10 and 80.5% (81.8%)\nRecall@50 with 1-bit (2-bit) binarization. Like the original\nColBERT, this form of compression relied on a separate FAISS\nindex for candidate generation.",
  "On 22 of 28 out-of-domain tests,\nColBERTv2 achieves the highest quality, outper-\nforming the next best retriever by up to 8% relative\ngain, while using its compressed representations.\nThis work makes the following contributions:\n1.We propose ColBERTv2, a retriever that com-\nbines denoised supervision and residual com-\npression, leveraging the token-level decom-\nposition of late interaction to achieve high\nrobustness with a reduced space footprint.\n2.We introduce LoTTE, a new resource for out-\nof-domain evaluation of retrievers. LoTTE fo-\ncuses on natural information-seeking queries\nover long-tail topics, an important yet under-\nstudied application space.",
  "7 57.7 60.1 63.4\n(b)\nTable 5: Zero-shot evaluation results. Sub-table (a) reports results on BEIR and sub-table (b) reports results on\nthe Wikipedia Open QA and the test sets of the LoTTE benchmark. On BEIR, we test ColBERTv2 and Rock-\netQAv2 and copy the results for ANCE, TAS-B, and ColBERT from Thakur et al. (2021), for MoDIR and DPR-\nMSMARCO (DPR-M) from Xin et al. (2021), and for SPLADEv2 from Formal et al. (2021a).\n5.2 Out-of-Domain Retrieval Quality\nNext, we evaluate ColBERTv2 outside the train-\ning domain using BEIR (Thakur et al., 2021),\nWikipedia Open QA retrieval as in Khattab et al.\n(2021b), and LoTTE. We compare against a wide\nrange of recent and state-of-the-art retrieval sys-\ntems from the literature.\nBEIR.",
  "ColBERT Random\n1 16 256 4096\n# Distinct Tokens per Cluster0255075100Proportion(a) Number of distinct tokens\nappearing in each cluster.\n4 64 1024\n# Distinct Clusters per Token0255075100Proportion(b) Number of distinct clus-\nters each token appears in.\nFigure 2: Empirical CDFs analyzing semantic proper-\nties of MS MARCO token-level embeddings both en-\ncoded by ColBERT and randomly generated. The em-\nbeddings are partitioned into 218clusters and corre-\nspond to roughly 27,000 distinct tokens.\nA Analysis of ColBERT\u2019s Semantic\nSpace\nColBERT (Khattab and Zaharia, 2020) decomposes\nrepresentations and similarity computation at the\ntoken level. Because of this compositional archi-\ntecture, we hypothesize that ColBERT exhibits a\n\u201clightweight\u201d semantic space: without any special\nre-training, vectors corresponding to each sense of\na word would cluster very closely, with only minor\nvariation due to context.",
  "CorpusModels without Distillation Models with DistillationColBERTDPR-MANCEMoDIRTAS-BRocketQAv2SPLADEv2ColBERTv2\nBEIR Search Tasks (nDCG@10)\nDBPedia 39.2 23.6 28.1 28.4 38.4 35.6 43.5 44.6\nFiQA 31.7 27.5 29.5 29.6 30.0 30.2 33.6 35.6\nNQ 52.4 39.8 44.6 44.2 46.3 50.5 52.1 56.2\nHotpotQA 59.3 37.1 45.6 46.2 58.4 53.3 68.4 66.7\nNFCorpus 30.5 20.8 23.7 24.4 31.9 29.3 33.4 33.8\nT-COVID 67.7 56.1 65.4 67.6 48.1 67.5 71.",
  "We\nleave low-level systems optimizations of all sys-\ntems to future work. Another worthwhile di-\nmension for future exploration of tradeoffs is re-\nranking architectures over various systems withcross-encoders, which are known to be expensive\nyet precise due to their highly expressive capacity.\nResearch Limitations\nWhile we evaluate ColBERTv2 on a wide range of\ntests, all of our benchmarks are in English and, in\nline with related work, our out-of-domain tests eval-\nuate models that are trained on MS MARCO. We\nexpect our approach to work effectively for other\nlanguages and when all models are trained using\nother, smaller training set (e.g., NaturalQuestions),\nbut we leave such tests to future work.\nWe have observed consistent gains for Col-\nBERTv2 against existing state-of-the-art systems\nacross many diverse settings. Despite this, almost\nall IR datasets contain false negatives (i.e., rele-\nvant but unlabeled passages) and thus some cau-\ntion is needed in interpreting any individual result.",
  "5.3 Ef\ufb01ciency\nColBERTv2\u2019s residual compression approach sig-\nni\ufb01cantly reduces index sizes compared to vanilla\nColBERT. Whereas ColBERT requires 154 GiB\nto store the index for MS MARCO, ColBERTv2\nonly requires 16 GiB or 25 GiB when compressing\nembeddings to 1 or 2 bit(s) per dimension, respec-\ntively, resulting in compression ratios of 6\u201310 \u0002.\nThis storage \ufb01gure includes 4.5 GiB for storing the\ninverted list.\nThis matches the storage for a typical single-\nvector model on MS MARCO, with 4-byte lossless\n\ufb02oating-point storage for one 768-dimensional vec-\ntor for each of the 9M passages amounting to a little\nover 25 GiBs. In practice, the storage for a single-\nvector model could be even larger when using a\nnearest-neighbor index like HNSW for fast search.",
  "References\nStack Exchange Data Dump.\nLiefu Ai, Junqing Yu, Zebin Wu, Yunfeng He, and\nTao Guan. 2017. Optimized Residual Vector Quan-\ntization for Ef\ufb01cient Approximate Nearest Neighbor\nSearch. Multimedia Systems , 23(2):169\u2013181.\nS\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives.\n2007. DBpedia: A Nucleus for a Web of Open Data.\nInThe semantic web , pages 722\u2013735. Springer.\nChristopher F Barnes, Syed A Rizvi, and Nasser M\nNasrabadi. 1996. Advances in Residual Vector\nQuantization: A Review. IEEE transactions on im-\nage processing , 5(2):226\u2013262.",
  "We divide\nthe table into \u201csearch\u201d (i.e., natural queries and\nquestions) and \u201csemantic relatednes\u201d (e.g., citation-\nrelatedness and claim veri\ufb01cation) tasks to re\ufb02ect\nthe nature of queries in each dataset.5\nTable 5a reports results with the of\ufb01cial\nnDCG@10 metric. Among the models with-\n5Following Formal et al. (2021a), we conduct our evalu-\nationg using the publicly-available datasets in BEIR. Refer\nto \u00a7E for details.out distillation, we see that the vanilla ColBERT\nmodel outperforms the single-vector systems DPR,\nANCE, and MoDIR across all but three tasks. Col-\nBERT often outpaces all three systems by large\nmargins and, in fact, outperforms the TAS-B model,\nwhich utilizes distillation, on most datasets. Shift-\ning our attention to models with distillation, we see\na similar pattern: while distillation-based models\nare generally stronger than their vanilla counter-\nparts, the models that decompose scoring into term-\nlevel interactions, ColBERTv2 and SPLADEv2,\nare almost always the strongest.",
  "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or \ufb01ction: Verify-\ning scienti\ufb01c claims. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 7534\u20137550, On-\nline. Association for Computational Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. MiniLM: Deep Self-\nAttention Distillation for Task-Agnostic Compres-\nsion of Pre-Trained Transformers. arXiv preprint\narXiv:2002.10957 .\nBenchang Wei, Tao Guan, and Junqing Yu. 2014.\nProjected Residual Vector Quantization for ANN\nSearch. IEEE multimedia , 21(3):41\u201351.",
  "7 86.8 98.4 40.8 86.3 98.3\nTable 4: In-domain performance on the development\nset of MS MARCO Passage Ranking as well the \u201cLocal\nEval\u201d test set described by Khattab and Zaharia (2020).\nDev-set results for baseline systems are from their re-\nspective papers: Zhan et al. (2020b), Xiong et al. (2020)\nfor DPR and ANCE, Zhan et al. (2020a), Khattab and\nZaharia (2020), Hofst\u00e4tter et al. (2021), Gao and Callan\n(2021), Ren et al. (2021a), Formal et al. (2021a), and\nRen et al. (2021b).\nPAIR, and RocketQAv2 to achieve higher qual-\nity than vanilla ColBERT. These supervision gains\nchallenge the value of \ufb01ne-grained late interaction,\nand it is not inherently clear whether the stronger\ninductive biases of ColBERT-like models permit it\nto accept similar gains under distillation, especially\nwhen using compressed representations.",
  "3We round down to the nearest power of two larger than\n16\u0002pnembeddings , inspired by FAISS (Johnson et al., 2019).3.5 Retrieval\nGiven a query representation Q, retrieval starts with\ncandidate generation. For every vector Qiin the\nquery, the nearest nprobe\u00151centroids are found.\nUsing the inverted list, ColBERTv2 identi\ufb01es the\npassage embeddings close to these centroids, de-\ncompresses them, and computes their cosine simi-\nlarity with every query vector. The scores are then\ngrouped by passage ID for each query vector, and\nscores corresponding to the same passage are max -\nreduced. This allows ColBERTv2 to conduct an\napproximate \u201cMaxSim\u201d operation per query vector.\nThis computes a lower-bound on the true MaxSim\n(\u00a73.1) using the embeddings identi\ufb01ed via the in-\nverted list, which resembles the approximation ex-\nplored for scoring by Macdonald and Tonellotto\n(2021) but is applied for candidate generation.",
  "In practice, the storage for a single-\nvector model could be even larger when using a\nnearest-neighbor index like HNSW for fast search.\nConversely, single-vector representations could be\nthemselves compressed very aggressively (Zhan\net al., 2021a, 2022), though often exacerbating the\nloss in quality relative to late interaction methods\nlike ColBERTv2.\nWe discuss the impact of our compression\nmethod on search quality in Appendix B and\npresent query latency results on the order of 50\u2013\n250 milliseconds per query in Appendix C.\n6 Conclusion\nWe introduced ColBERTv2, a retriever that ad-\nvances the quality and space ef\ufb01ciency of multi-\nvector representations. We hypothesized that clus-\nter centroids capture context-aware semantics of\nthe token-level representations and proposed a\nresidual representation that leverages these patterns\nto dramatically reduce the footprint of multi-vector\nsystems off-the-shelf . We then explored improved\nsupervision for multi-vector retrieval and found\nthat their quality improves considerably upon distil-\nlation from a cross-encoder system.",
  "Search\nQueries are taken from GooAQ, while Forum Queries are taken directly from the StackExchange archive. The\npooled datasets combine the questions and passages from each of the subtopics.\nrums. StackExchange is a set of question-and-\nanswer communities that target individual topics\n(e.g., \u201cphysics\u201d or \u201cbicycling\u201d). We gather forums\nfrom \ufb01ve overarching domains: writing, recreation,\nscience, technology, and lifestyle. To evaluate re-\ntrievers, we collect Search andForum queries, each\nof which is associated with one or more target an-\nswer posts in its corpus. Example queries, and\nshort snippets from posts that answer them in the\ncorpora, are shown in Table 2.\nSearch Queries. We collect search queries from\nGooAQ (Khashabi et al., 2021), a recent dataset\nof Google search-autocomplete queries and their\nanswer boxes, which we \ufb01lter for queries whose\nanswers link to a speci\ufb01c StackExchange post. As\nKhashabi et al.",
  "We sweep a range of values for the number of centroids per vector ( probe ), the number of bits used\nfor residual compression, and the number of candidates. Note that retrieval quality is measured in MRR@10 for\nMS MARCO and Success@5 for LoTTE datasets. Results toward the bottom right corner (higher quality, lower\nlatency) are best.",
  "Yi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2021. Sparse, Dense, and Atten-\ntional Representations for Text Retrieval. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:329\u2013345.\nSean MacAvaney, Franco Maria Nardini, Raffaele\nPerego, Nicola Tonellotto, Nazli Goharian, and\nOphir Frieder. 2020. Ef\ufb01cient document re-ranking\nfor transformers by precomputing term representa-\ntions. In Proceedings of the 43rd International ACM\nSIGIR conference on research and development in\nInformation Retrieval, SIGIR 2020, Virtual Event,\nChina, July 25-30, 2020 , pages 49\u201358. ACM.\nCraig Macdonald and Nicola Tonellotto. 2021. On\napproximate nearest neighbour selection for multi-\nstage dense retrieval. In Proceedings of the 30th\nACM International Conference on Information &\nKnowledge Management , pages 3318\u20133322.",
  "3 ColBERTv2\nWe now introduce ColBERTv2, which improves\nthe quality of multi-vector retrieval models (\u00a73.2)\nwhile reducing their space footprint (\u00a73.3).\n3.1 Modeling\nColBERTv2 adopts the late interaction architecture\nof ColBERT, depicted in Figure 1. Queries and pas-\nsages are independently encoded with BERT (De-\nvlin et al., 2019), and the output embeddings encod-\ning each token are projected to a lower dimension.\nDuring of\ufb02ine indexing, every passage din the\ncorpus is encoded into a set of vectors, and these",
  "Moreover, generalization from training on\na large-scale dataset can propagate the biases of\nthat dataset well beyond its typical reach to new\ndomains and applications.\nWhile our contributions have made ColBERT\u2019s\nlate interaction more ef\ufb01cient at storage costs, large-\nscale distillation with hard negatives increases sys-\ntem complexity and accordingly increases train-\ning cost, when compared with the straightforward\ntraining paradigm of the original ColBERT model.\nWhile ColBERTv2 is ef\ufb01cient in terms of latency\nand storage at inference time, we suspect that un-\nder extreme resource constraints, simpler model de-\nsigns like SPLADEv2 or RocketQAv2 could lend\nthemselves to easier-to-optimize environments. We\nleave low-level systems optimizations of all sys-\ntems to future work. Another worthwhile di-\nmension for future exploration of tradeoffs is re-\nranking architectures over various systems withcross-encoders, which are known to be expensive\nyet precise due to their highly expressive capacity.",
  "To generate the top- kpas-\nsages per training query, we apply two rounds, fol-\nlowing Khattab et al. (2021b). We start from a\nmodel trained with hard triples (akin to Khattab\net al. (2021b)), train with distillation, and then use\nthe distilled model to retrieve for the second round\nof training. Preliminary experiments indicate that\nquality has low sensitivity to this initialization and\ntwo-round training, suggesting that both of them\ncould be avoided to reduce the cost of training.\nUnless otherwise stated, the results shown rep-\nresent a single run. The latency results in \u00a73 are\naverages of three runs. To evaluate for Open-QA re-\ntrieval, we use evaluation scripts from Khattab et al.\n(2021b), which checks if the short answer string\nappears in the (titled) Wikipedia passage. This\nadapts the DPR (Karpukhin et al., 2020) evaluation\ncode.10We use the preprocessed Wikipedia Dec\n2018 dump released by Karpukhin et al. (2020).",
  "k-means clustering,9though unlike ColBERT we\ndo not use it for nearest-neighbor search. Instead,\nwe implement our candidate generation mechanism\n(\u00a73.5) using PyTorch primitives in Python.\nWe conducted our experiments on an internal\ncluster, typically using up to four 12GB Titan V\nGPUs for each of the inference tasks (e.g., index-\ning, computing distillation scores, and retrieval)\nand four 80GB A100 GPUs for training, though\nGPUs with smaller RAM can be used via gradient\naccumulation. Using this infrastructure, computing\nthe distillation scores takes under a day, training a\n64-way model on MS MARCO for 400,000 steps\ntakes around \ufb01ve days, and indexing takes approx-\nimately two hours. We very roughly estimate an\nupper bound total of 20 GPU-months for all experi-\nmentation, development, and evaluation performed\nfor this work over a period of several months.\nLike ColBERT, our encoder is a\nbert-base-uncased model that is shared\nbetween the query and passage encoders and which\nhas 110M parameters.",
  "COIL (Gao et al., 2021) also generates token-level\ndocument embeddings, but the token interactions\nare restricted to lexical matching between query\nand document terms. uniCOIL (Lin and Ma, 2021)\nlimits the token embedding vectors of COIL to a\nsingle dimension, reducing them to scalar weights\nthat extend models like DeepCT (Dai and Callan,\n2020) and DeepImpact (Mallia et al., 2021). To\nproduce scalar weights, SPLADE (Formal et al.,\n2021b) and SPLADEv2 (Formal et al., 2021a) pro-\nduce a sparse vocabulary-level vector that retains\nthe term-level decomposition of late interaction\nwhile simplifying the storage into one dimension\nper token. The SPLADE family also piggybacks on\nthe language modeling capacity acquired by BERT\nduring pretraining. SPLADEv2 has been shown\nto be highly effective, within and across domains,\nand it is a central point of comparison in the exper-\niments we report on in this paper.",
  "Khattab and Zaharia (2020) train\nColBERT using the of\ufb01cial hq, d+, d\u0000itriples\nof MS MARCO. For each query, a positive d+is\nhuman-annotated, and each negative d\u0000is sampled\nfrom unannotated BM25-retrieved passages.\nSubsequent work has identi\ufb01ed several weak-\nnesses in this standard supervision approach\n(see \u00a72.3). Our goal is to adopt a simple, uniform\nsupervision scheme that selects challenging neg-\natives and avoids rewarding false positives or pe-\nnalizing false negatives. To this end, we start with\na ColBERT model trained with triples as in Khat-\ntab et al. (2021b), using this to index the training\npassages with ColBERTv2 compression.\nFor each training query, we retrieve the top- k\npassages. We feed each of those query\u2013passage\npairs into a cross-encoder reranker. We use a\n22M-parameter MiniLM (Wang et al., 2020) cross-\nencoder trained with distillation by Thakur et al.",
  "vectors are stored. At search time, the query qis\nencoded into a multi-vector representation, and its\nsimilarity to a passage dis computed as the summa-\ntion of query-side \u201cMaxSim\u201d operations, namely,\nthe largest cosine similarity between each query to-\nken embedding and all passage token embeddings:\nSq;d=NX\ni=1Mmax\nj=1Qi\u0001DT\nj (1)\nwhere Qis an matrix encoding the query with N\nvectors and Dencodes the passage with Mvectors.\nThe intuition of this architecture is to align each\nquery token with the most contextually relevant\npassage token, quantify these matches, and com-\nbine the partial scores across the query. We refer\nto Khattab and Zaharia (2020) for a more detailed\ntreatment of late interaction.\n3.2 Supervision\nTraining a neural retriever typically requires posi-\ntiveandnegative passages for each query in the\ntraining set. Khattab and Zaharia (2020) train\nColBERT using the of\ufb01cial hq, d+, d\u0000itriples\nof MS MARCO.",
  "Association for\nComputational Linguistics.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoqiao She, Hua Wu, Haifeng Wang, and Ji-\nRong Wen. 2021b. RocketQAv2: A Joint Training\nMethod for Dense Passage Retrieval and Passage Re-\nranking. arXiv preprint arXiv:2110.07367 .\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at TREC-3. NIST Special Publication .\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA Heterogenous Benchmark for Zero-shot Eval-\nuation of Information Retrieval Models. arXiv\npreprint arXiv:2104.08663 .\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERi\ufb01cation.",
  "2020. SPECTER:\nDocument-level representation learning using\ncitation-informed transformers. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 2270\u20132282,\nOnline. Association for Computational Linguistics.\nNachshon Cohen, Amit Portnoy, Besnik Fetahu, and\nAmir Ingber. 2021. SDR: Ef\ufb01cient Neural Re-\nranking using Succinct Document Representation.\narXiv preprint arXiv:2110.02065 .Zhuyun Dai and Jamie Callan. 2020. Context-aware\nterm weighting for \ufb01rst stage passage retrieval. In\nProceedings of the 43rd International ACM SIGIR\nconference on research and development in Infor-\nmation Retrieval, SIGIR 2020, Virtual Event, China,\nJuly 25-30, 2020 , pages 1533\u20131536. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.",
  "Transactions of the Association\nfor Computational Linguistics , 7:452\u2013466.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021a. Learning dense representations of\nphrases at scale. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers) , pages 6634\u20136647, Online. Association for\nComputational Linguistics.\nJinhyuk Lee, Alexander Wettig, and Danqi Chen.\n2021b. Phrase retrieval learns passage retrieval, too.\narXiv preprint arXiv:2109.08133 .\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 6086\u20136096, Florence,\nItaly. Association for Computational Linguistics.",
  "arXiv\npreprint arXiv:2104.08663 .\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERi\ufb01cation. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers) , pages 809\u2013819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.",
  "Strong out-of-the-box generalization to small\ndomain-speci\ufb01c applications can serve many users\nin practice, particularly where training data is not\navailable. Moreover, retrieval holds signi\ufb01cant\npromise for many downstream NLP tasks, as it\ncan help make language models smaller and thus\nmore ef\ufb01cient (i.e., by decoupling knowledge from\ncomputation), more transparent (i.e., by allowing\nusers to check the sources the model relied on when\nmaking a claim or prediction), and easier to update\n(i.e., by allowing developers to replace or add doc-\numents to the corpus without retraining the model)\n(Guu et al., 2020; Borgeaud et al., 2021; Khattab\net al., 2021a). Nonetheless, such work poses risks\nin terms of misuse, particularly toward misinforma-\ntion, as retrieval can surface results that are relevant\nyet inaccurate, depending on the contents of a cor-\npus. Moreover, generalization from training on\na large-scale dataset can propagate the biases of\nthat dataset well beyond its typical reach to new\ndomains and applications.",
  "2019),\nTriviaQA (TQ; Joshi et al. 2017), and SQuAD (Ra-\njpurkar et al., 2016) datasets in Table 5b. As a\nbaseline, we include the BM25 (Robertson et al.,\n1995) results using the Anserini (Yang et al., 2018a)\ntoolkit. We observe that ColBERTv2 outperforms\nBM25, vanilla ColBERT, and SPLADEv2 across\nthe three query sets, with improvements of up to\n4.6 points over SPLADEv2.\nLoTTE. Next, we analyze performance on the\nLoTTE test benchmark, which focuses on natural\nqueries over long-tail topics and exhibits a different\nannotation pattern to the datasets in the previous\nOOD evaluations. In particular, LoTTE uses auto-\nmatic Google rankings (for the \u201csearch\u201d queries)\nand organic StackExchange question\u2013answer pairs\n(for \u201cforum\u201d queries), complimenting the pooling-\nbased annotation of datasets like TREC-COVID (in\nBEIR) and the answer overlap metrics of Open-QA\nretrieval.",
  "7 76.7\nSQuAD -dev 60.0 50.6 - - 60.4 65.0\nLoTTE Search Test Queries (Success@5)\nWriting 74.7 60.3 74.4 78.0 77.1 80.1\nRecreation 68.5 56.5 64.7 72.1 69.0 72.3\nScience 53.6 32.7 53.6 55.3 55.4 56.7\nTechnology 61.9 41.8 59.6 63.4 62.4 66.1\nLifestyle 80.2 63.8 82.3 82.1 82.3 84.7\nPooled 67.3 48.3 66.4 69.8 68.9 71.6\nLoTTE Forum Test Queries (Success@5)\nWriting 71.0 64.0 68.8 71.5 73.0 76.3\nRecreation 65.6 55.4 63.8 65.",
  "2019) CC BY-SA 3.0 2681468 3452\nSCIDOCS (Cohan et al. 2020)GNU General Public\nLicense v3.025657 1000\nSciFact (Wadden et al. 2020) CC BY-NC 2.0 5183 300\nQuora Not reported 522931 10000\nTouch\u00e9-2020 (Bondarenko et al. 2020) CC BY 4.0 382545 49\nTREC-COVID (V oorhees et al. 2021)Dataset License\nAgreement171332 50\nTable 8: BEIR dataset information.\nWe also tested on the Open-QA benchmarks NQ,\nTQ, and SQuAD, each of which has approximately\n9k dev-set questions and muli-hop HoVer, whose\ndevelopment set has 4k claims. In the compression\nevaluation \u00a7B, we used models trained in-domain\non NQ and HoVer, whose training sets contain 79k\nand 18k queries, respectively.",
  "2020. Mod-\nularized transfomer-based ranking framework. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4180\u20134190, Online. Association for Computa-\ntional Linguistics.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021.\nCOIL: Revisit exact lexical match in information\nretrieval with contextualized inverted list. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3030\u20133042, Online. Association for Computational\nLinguistics.\nRobert Gray. 1984. Vector quantization. IEEE Assp\nMagazine , 1(2):4\u201329.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909 .",
  "Ellen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTREC-COVID: Constructing a Pandemic Informa-\ntion Retrieval Test Collection. In ACM SIGIR Fo-\nrum, volume 54, pages 1\u201312. ACM New York, NY ,\nUSA.\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\n2018. Retrieval of the best counterargument with-\nout prior topic knowledge. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 241\u2013251, Melbourne, Australia. Association\nfor Computational Linguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or \ufb01ction: Verify-\ning scienti\ufb01c claims.",
  "In this work, we show that late interaction re-\ntrievers naturally produce lightweight token rep-\nresentations that are amenable to ef\ufb01cient storage\noff-the-shelf and that they can bene\ufb01t drastically\nfrom denoised supervision. We couple those in\nColBERTv2 ,1a new late-interaction retriever that\nemploys a simple combination of distillation from\n1Code, models, and LoTTE data are maintained at https:\n//github.com/stanford-futuredata/ColBERTarXiv:2112.01488v3  [cs.IR]  10 Jul 2022",
  "5 20.8 23.7 24.4 31.9 29.3 33.4 33.8\nT-COVID 67.7 56.1 65.4 67.6 48.1 67.5 71.0 73.8\nTouch\u00e9 (v2) - - - - - 24.7 27.2 26.3\nBEIR Semantic Relatedness Tasks (nDCG@10)\nArguAna 23.3 41.4 41.5 41.8 42.7 45.1 47.9 46.3\nC-FEVER 18.4 17.6 19.8 20.6 22.8 18.0 23.5 17.6\nFEVER 77.1 58.9 66.9 68.0 70.0 67.6 78.6 78.5\nQuora 85.4 84.2 85.2 85.6 83.5 74.9 83.8 85.2\nSCIDOCS 14.5 10.",
  "In this work, we focus on late-interaction re-\ntrieval and investigate compression using a residual\ncompression approach that can be applied off-the-\nshelf to late interaction models, without special\ntraining. We show in Appendix A that ColBERT\u2019s\nrepresentations naturally lend themselves to resid-\nual compression. Techniques in the family of resid-\nual compression are well-studied (Barnes et al.,\n1996) and have previously been applied across sev-\neral domains, including approximate nearest neigh-\nbor search (Wei et al., 2014; Ai et al., 2017), neural\nnetwork parameter and activation quantization (Li\net al., 2021b,a), and distributed deep learning (Chen\net al., 2018; Liu et al., 2020). To the best of our\nknowledge, ColBERTv2 is the \ufb01rst approach to use\nresidual compression for scalable neural IR.",
  "A:A root is the form to which derivational\naf\ufb01xes are added to form a stem. A stem is the form\nto which in\ufb02ectional af\ufb01xes are added to form a word.\nQ:are there any airbenders left? A:the Fire Nation\nhad wiped out all Airbenders while Aang was frozen.\nTenzin and his 3 children are the only Airbenders left\nin Korra\u2019s time.\nQ:Why are there two Hydrogen atoms on some peri-\nodic tables? A:some periodic tables show hydrogen in\nboth places to emphasize that hydrogen isn\u2019t really a\nmember of the \ufb01rst group or the seventh group.\nQ:How can cache be that fast? A:the cache memory\nsits right next to the CPU on the same die (chip), it is\nmade using SRAM which is much, much faster than\nthe DRAM.\nTable 2: Examples of queries and shortened snippets of\nanswer passages from LoTTE. The \ufb01rst two examples\nshow \u201csearch\u201d queries, whereas the last two are \u201cfo-\nrum\u201d queries. Snippets are shortened for presentation.",
  "8 55.7 54.3 56.3 59.3\nLifestyle 73.1 54.4 69.8 72.4 71.2 75.8\nPooled 65.4 45.6 63.7 66.4 67.0 69.3\nLoTTE Forum Dev Queries (Success@5)\nWriting 75.5 66.2 74.4 75.5 78.1 80.8\nRecreation 69.1 56.6 65.9 69.0 68.9 71.8\nScience 58.2 51.3 56.3 56.7 59.9 62.6\nTechnology 39.6 30.7 38.8 39.9 42.1 45.0\nLifestyle 61.1 48.2 61.8 62.0 61.8 65.8\nPooled 59.1 47.8 57.4 58.9 60.6 63.",
  "2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations , pages 38\u201345, Online. Asso-\nciation for Computational Linguistics.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul N Bennett. 2021.\nZero-Shot Dense Retrieval with Momentum Adver-\nsarial Domain Invariant Representations. arXiv\npreprint arXiv:2110.07581 .\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N Bennett, Junaid Ahmed, and\nArnold Overwijk. 2020. Approximate Nearest\nNeighbor Negative Contrastive Learning for Dense\nText Retrieval. In International Conference on\nLearning Representations .\nIkuya Yamada, Akari Asai, and Hannaneh Hajishirzi.\n2021a. Ef\ufb01cient passage retrieval with hashing for\nopen-domain question answering.",
  "3.4 Indexing\nGiven a corpus of passages, the indexing stage\nprecomputes all passage embeddings and orga-\nnizes their representations to support fast nearest-\nneighbor search. ColBERTv2 divides indexing into\nthree stages, described below.\nCentroid Selection. In the \ufb01rst stage, Col-\nBERTv2 selects a set of cluster centroids C. These\nare embeddings that ColBERTv2 uses to sup-\nport residual encoding (\u00a73.3) and also for nearest-\nneighbor search (\u00a73.5). Standardly, we \ufb01nd that\nsettingjCjproportionally to the square root of\nnembeddings in the corpus works well empirically.3\nKhattab and Zaharia (2020) only clustered the vec-\ntors after computing the representations of all pas-\nsages, but doing so requires storing them uncom-\npressed. To reduce memory consumption, we apply\nk-means clustering to the embeddings produced by\ninvoking our BERT encoder over only a sample of\nall passages, proportional to the square root of the\ncollection size, an approach we found to perform\nwell in practice.\nPassage Encoding.",
  "To reduce memory consumption, we apply\nk-means clustering to the embeddings produced by\ninvoking our BERT encoder over only a sample of\nall passages, proportional to the square root of the\ncollection size, an approach we found to perform\nwell in practice.\nPassage Encoding. Having selected the cen-\ntroids, we encode every passage in the corpus. This\nentails invoking the BERT encoder and compress-\ning the output embeddings as described in \u00a73.3,\nassigning each embedding to the nearest centroid\nand computing a quantized residual. Once a chunk\nof passages is encoded, the compressed representa-\ntions are saved to disk.\nIndex Inversion. To support fast nearest-\nneighbor search, we group the embedding IDs that\ncorrespond to each centroid together, and save this\ninverted list to disk. At search time, this allows us\nto quickly \ufb01nd token-level embeddings similar to\nthose in a query.\n3We round down to the nearest power of two larger than\n16\u0002pnembeddings , inspired by FAISS (Johnson et al., 2019).3.5 Retrieval\nGiven a query representation Q, retrieval starts with\ncandidate generation.",
  "ACM.\nCraig Macdonald and Nicola Tonellotto. 2021. On\napproximate nearest neighbour selection for multi-\nstage dense retrieval. In Proceedings of the 30th\nACM International Conference on Information &\nKnowledge Management , pages 3318\u20133322.\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. WWW\u201918 Open Chal-\nlenge: Financial Opinion Mining and Question An-\nswering. In Companion Proceedings of the The Web\nConference 2018 , pages 1941\u20131942.\nAntonio Mallia, Omar Khattab, Torsten Suel, and\nNicola Tonellotto. 2021. Learning passage impacts\nfor inverted indexes. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval , pages\n1723\u20131727.\nAditya Krishna Menon, Sadeep Jayasumana, Se-\nungyeon Kim, Ankit Singh Rawat, Sashank J. Reddi,\nand Sanjiv Kumar.",
  "Sebastian Hofst\u00e4tter, Sophia Althammer, Michael\nSchr\u00f6der, Mete Sertkan, and Allan Hanbury. 2020.\nImproving Ef\ufb01cient Neural Ranking Models with\nCross-Architecture Knowledge Distillation. arXiv\npreprint arXiv:2010.02666 .\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef\ufb01-\nciently Teaching an Effective Dense Retriever with\nBalanced Topic Aware Sampling. arXiv preprint\narXiv:2104.06967 .\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\nview.net.",
  "Because of this compositional archi-\ntecture, we hypothesize that ColBERT exhibits a\n\u201clightweight\u201d semantic space: without any special\nre-training, vectors corresponding to each sense of\na word would cluster very closely, with only minor\nvariation due to context.\nIf this hypothesis is true, we would expect the\nembeddings corresponding to each token in the\nvocabulary to localize in only a small number of\nregions in the embedding space, corresponding\nto the contextual \u201csenses\u201d of the token. To val-\nidate this hypothesis, we analyze the ColBERT\nembeddings corresponding to the tokens in the\nMS MARCO Passage Ranking (Nguyen et al.,\n2016) collection: we perform k-means clustering\non the nearly 600M embeddings\u2014corresponding\nto 27,000 unique tokens\u2014into k= 218clusters.\nAs a baseline, we repeat this clustering with ran-\ndom embeddings but keep the true distribution of\ntokens.",
  "Speci\ufb01cally, we\naward a point to the system for each query where\nit \ufb01nds an accepted or upvoted (score \u00151) answer\nfrom the target page in the top-5 hits.\nAppendix D reports on the breakdown of con-\nstituent communities per topic, the construction\nprocedure of LoTTE as well as licensing considera-\ntions, and relevant statistics. Figures 5 and 6 quan-\ntitatively compare the search and forum queries.\n5 Evaluation\nWe now evaluate ColBERTv2 on passage retrieval\ntasks, testing its quality within the training domain\n(\u00a75.1) as well as outside the training domain in\nzero-shot settings (\u00a75.2). Unless otherwise stated,\nwe compress ColBERTv2 embeddings to b= 2\nbits per dimension in our evaluation.\n5.1 In-Domain Retrieval Quality\nSimilar to related work, we train for IR tasks on MS\nMARCO Passage Ranking (Nguyen et al., 2016).",
  "5 10 15 20\nAnswers per query[Forum] Pooled[Forum] Lifestyle[Forum] Technology[Forum] Science[Forum] Recreation[Forum] Writing[Search] Pooled[Search] Lifestyle[Search] Technology[Search] Science[Search] Recreation[Search] WritingFigure 6: LoTTE answers per query\nCorpusColBERTBM25ANCERocketQAv2SPLADEv2ColBERTv2\nLoTTE Search Dev Queries (Success@5)\nWriting 76.3 47.3 75.7 79.5 78.9 81.7\nRecreation 71.8 56.3 66.1 73.0 70.7 76.0\nScience 71.7 52.2 66.9 67.7 73.4 74.2\nTechnology 52.8 35.8 55.7 54.3 56.3 59.3\nLifestyle 73.1 54.4 69.8 72.4 71.2 75.8\nPooled 65.4 45.6 63.7 66.",
  "This added expressivity comes\nat a cost: existing late interaction systems impose\nan order-of-magnitude larger space footprint than\nsingle-vector models, as they must store billions\nof small vectors for Web-scale collections. Con-\nsidering this challenge, it might seem more fruit-\nful to focus instead on addressing the fragility of\nsingle-vector models (Menon et al., 2022) by in-\ntroducing new supervision paradigms for negative\nmining (Xiong et al., 2020), pretraining (Gao and\nCallan, 2021), and distillation (Qu et al., 2021).\nIndeed, recent single-vector models with highly-\ntuned supervision strategies (Ren et al., 2021b; For-\nmal et al., 2021a) sometimes perform on-par or\neven better than \u201cvanilla\u201d late interaction models,\nand it is not necessarily clear whether late inter-\naction architectures\u2014with their \ufb01xed token-level\ninductive biases\u2014admit similarly large gains from\nimproved supervision.",
  "Topic Question SetDev Test\n# Questions # Passages Subtopics # Questions # Passages Subtopics\nWritingSearch 497277kESL, Linguistics,\nWorldbuilding1071200k EnglishForum 2003 2000\nRecreationSearch 563263kSci-Fi, RPGs,\nPhotography924167kGaming,\nAnime, Movies Forum 2002 2002\nScienceSearch 538344kChemistry,\nStatistics, Academia6171.694MMath,\nPhysics, Biology Forum 2013 2017\nTechnologySearch 9161.276MWeb Apps,\nUbuntu, SysAdmin596639kApple, Android,\nUNIX, Security Forum 2003 2004\nLifestyleSearch 417269kDIY , Music, Bicycles,\nCar Maintenance661119kCooking,\nSports, Travel Forum 2076 2002\nPooledSearch 29312.4M All of the above38692.8M All of the aboveForum 10097 10025\nTable 1: Composition of LoTTE showing topics, question sets, and a sample of corresponding subtopics. Search\nQueries are taken from GooAQ, while Forum Queries are taken directly from the StackExchange archive.",
  "by PQ via a ranking-oriented loss.\nSDR (Cohen et al., 2021) uses an autoencoder to\nreduce the dimensionality of the contextual embed-\ndings used for attention-based re-ranking and then\napplies a quantization scheme for further compres-\nsion. DensePhrases (Lee et al., 2021a) is a system\nfor Open-QA that relies on a multi-vector encod-\ning of passages, though its search is conducted\nat the level of individual vectors and not aggre-\ngated with late interaction. Very recently, Lee et al.\n(2021b) propose a quantization-aware \ufb01netuning\nmethod based on PQ to reduce the space footprint\nof DensePhrases. While DensePhrases is effective\nat Open-QA, its retrieval quality\u2014as measured by\ntop-20 retrieval accuracy on NaturalQuestions and\nTriviaQA\u2014is competitive with DPR (Karpukhin\net al., 2020) and considerably less effective than\nColBERT (Khattab et al., 2021b).",
  "9 42.1 45.0\nLifestyle 61.1 48.2 61.8 62.0 61.8 65.8\nPooled 59.1 47.8 57.4 58.9 60.6 63.7\nTable 7: Zero-shot evaluation results on the dev sets of\nthe LoTTE benchmark.\nhighest ranked queries from each community as\ndetermined by 1) the query scores and 2) the query\nview counts. We only use queries which have an\naccepted answer. We ensure that each community\ncontributes at least 50 queries to the truncated set\nwhenever possible. We set the overall size of the\ntruncated set to be 2000 queries, though note that\nthe total can exceed this due to rounding and/or the\nminimum per-community query count. We remove\nall quotation marks and HTML tags.\nStatistics Figure 4 plots the number of words\nper passage in each LoTTE dev corpus. Figures 5\nand 6 plot the number of words and number of\ncorresponding answer passages respectively per\nquery, split across search and forum queries.",
  "We adopt similar\ntechniques to (1) and (2) for ColBERTv2\u2019s multi-\nvector representations (see \u00a73.2).\nQuestion Passage\nQuestion Encoder Passage EncoderMaxSim MaxSim MaxSimscore\nOffline IndexingFigure 1: The late interaction architecture, given a\nquery and a passage. Diagram from Khattab et al.\n(2021b) with permission.\n2.4 Out-of-Domain Evaluation in IR\nRecent progress in retrieval has mostly focused on\nlarge-data evaluation, where many tens of thou-\nsands of annotated training queries are associated\nwith the test domain, as in MS MARCO or Natu-\nral Questions (Kwiatkowski et al., 2019). In these\nbenchmarks, queries tend to re\ufb02ect high-popularity\ntopics like movies and athletes in Wikipedia. In\npractice, user-facing IR and QA applications often\npertain to domain-speci\ufb01c corpora, for which little\nto no training data is available and whose topics\nare under-represented in large public collections.",
  "To this end, we evaluate Col-\nBERTv2 on a wide array of out-of-domain bench-\nmarks. These include three Wikipedia Open-QA\nretrieval tests and 13 diverse retrieval and semantic-\nsimilarity tasks from BEIR (Thakur et al., 2021). In\naddition, we introduce a new benchmark, dubbed\nLoTTE , for Long-TailTopic-strati\ufb01ed Evaluation\nfor IR that features 12 domain-speci\ufb01c search\ntests, spanning StackExchange communities and\nusing queries from GooAQ (Khashabi et al., 2021).\nLoTTE focuses on relatively long-tail topics in\nits passages, unlike the Open-QA tests and many\nof the BEIR tasks, and evaluates models on their\ncapacity to answer natural search queries with a\npractical intent, unlike many of BEIR\u2019s semantic-\nsimilarity tasks. On 22 of 28 out-of-domain tests,\nColBERTv2 achieves the highest quality, outper-\nforming the next best retriever by up to 8% relative\ngain, while using its compressed representations.",
  "Springer.\nChristopher F Barnes, Syed A Rizvi, and Nasser M\nNasrabadi. 1996. Advances in Residual Vector\nQuantization: A Review. IEEE transactions on im-\nage processing , 5(2):226\u2013262.\nAlexander Bondarenko, Maik Fr\u00f6be, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, et al. 2020. Overview\nof touch\u00e9 2020: Argument Retrieval. In Interna-\ntional Conference of the Cross-Language Evalua-\ntion Forum for European Languages , pages 384\u2013\n395. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens.",
  "vector representations. Product quantization (Gray,\n1984; Jegou et al., 2010) compresses a single vector\nby splitting it into small sub-vectors and encoding\neach of them using an ID within a codebook. In\nour approach, each representation is already a ma-\ntrix that is naturally divided into a number of small\nvectors (one per token). We encode each vector\nusing its nearest centroid plus a residual. Refer\nto Appendix B for tests of the impact of compres-\nsion on retrieval quality and a comparison with a\nbaseline compression method for ColBERT akin to\nBPR (Yamada et al., 2021b).\n3.4 Indexing\nGiven a corpus of passages, the indexing stage\nprecomputes all passage embeddings and orga-\nnizes their representations to support fast nearest-\nneighbor search. ColBERTv2 divides indexing into\nthree stages, described below.\nCentroid Selection.",
  "Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing , pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nRuiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu,\nWayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng\nWang, and Ji-Rong Wen. 2021a. PAIR: Leverag-\ning passage-centric similarity relation for improving\ndense passage retrieval. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021 , pages 2173\u20132183, Online. Association for\nComputational Linguistics.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoqiao She, Hua Wu, Haifeng Wang, and Ji-\nRong Wen. 2021b.",
  "Acknowledgements\nThis research was supported in part by af\ufb01liate\nmembers and other supporters of the Stanford\nDAWN project\u2014Ant Financial, Facebook, Google,\nand VMware\u2014as well as Cisco, SAP, Virtusa, and\nthe NSF under CAREER grant CNS-1651570. Any\nopinions, \ufb01ndings, and conclusions or recommen-\ndations expressed in this material are those of the\nauthors and do not necessarily re\ufb02ect the views of\nthe National Science Foundation.\nBroader Impact & Ethical Considerations\nThis work is primarily an effort toward retrieval\nmodels that generalize better while performing\nreasonably ef\ufb01ciently in terms of space consump-\ntion. Strong out-of-the-box generalization to small\ndomain-speci\ufb01c applications can serve many users\nin practice, particularly where training data is not\navailable.",
  "Learning discrete\nrepresentations via constrained clustering for effec-\ntive and ef\ufb01cient dense retrieval. In Proceedings\nof the Fifteenth ACM International Conference on\nWeb Search and Data Mining , WSDM \u201922, page\n1328\u20131336. Association for Computing Machinery.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and\nShaoping Ma. 2020a. Learning to retrieve: How\nto train a dense retrieval model effectively and ef-\n\ufb01ciently. arXiv preprint arXiv:2010.10469 .\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and\nShaoping Ma. 2020b. Repbert: Contextualized text\nembeddings for \ufb01rst-stage retrieval. arXiv preprint\narXiv:2006.15498 .\nGiulio Zhou and Jacob Devlin. 2021. Multi-vector\nattention models for deep re-ranking.",
  "In practice, with n= 128 , we use\nfour bytes to capture up to 232centroids and 16 or\n32 bytes (for b= 1orb= 2) to encode the resid-\nual. This total of 20 or 36 bytes per vector contrasts\nwith ColBERT\u2019s use of 256-byte vector encodings\nat 16-bit precision. While many alternatives can be\nexplored for compression, we \ufb01nd that this simple\nencoding largely preserves model quality, while\nconsiderably lowering storage costs against typi-\ncal 32- or 16-bit precision used by existing late\ninteraction systems.\nThis centroid-based encoding can be considered\na natural extension of product quantization to multi-",
  "Q:Should I simply ignore it if\nauthors assume that Im male in their response to my review of\ntheir article? Q:Why is the 2s orbital lower in energy than\nthe 2p orbital when the electrons in 2s are usually farther from\nthe nucleus? Q:Are there reasons to use colour \ufb01lters\nwith digital cameras? Q:How does the current know how\nmuch to \ufb02ow, before having seen the resistor? Q:What\nis the difference between Fact and Truth? Q:hAs a DM,\nhow can I handle my Druid spying on everything with Wild\nshape as a spider? Q:What does 1x1 convolution mean\nin a neural network?\nTable 3: Comparison of a random sample of search\nqueries (top) vs. forum queries (bottom).\n2021b), we evaluate retrieval quality by comput-\ning the success@5 (S@5) metric. Speci\ufb01cally, we\naward a point to the system for each query where\nit \ufb01nds an accepted or upvoted (score \u00151) answer\nfrom the target page in the top-5 hits.",
  "sages. This is known to lead to arti\ufb01cial lexical\nbias (Lee et al., 2019), where crowdworkers copy\nterms from the passages into their questions as in\nthe Open-SQuAD benchmark.\nWikipedia Open QA. As a further test of out-\nof-domain generalization, we evaluate the MS\nMARCO-trained ColBERTv2, SPLADEv2, and\nvanilla ColBERT on retrieval for open-domain\nquestion answering, similar to the out-of-domain\nsetting of Khattab et al. (2021b). We report\nSuccess@5 (sometimes referred to as Recall@5),\nwhich is the percentage of questions whose short\nanswer string overlaps with one or more of the\ntop-5 passages. For the queries, we use the de-\nvelopment set questions of the open-domain ver-\nsions (Lee et al., 2019; Karpukhin et al., 2020) of\nNatural Questions (NQ; Kwiatkowski et al. 2019),\nTriviaQA (TQ; Joshi et al.",
  "To the best of our\nknowledge, ColBERTv2 is the \ufb01rst approach to use\nresidual compression for scalable neural IR.\n2.3 Improving the Quality of Single-Vector\nRepresentations\nInstead of compressing multi-vector representa-\ntions as we do, much recent work has focused\non improving the quality of single-vector mod-\nels, which are often very sensitive to the speci\ufb01cs\nof supervision. This line of work can be decom-\nposed into three directions: (1) distillation of more\nexpressive architectures (Hofst\u00e4tter et al., 2020;\nLin et al., 2020) including explicit denoising (Qu\net al., 2021; Ren et al., 2021b), (2) hard negative\nsampling (Xiong et al., 2020; Zhan et al., 2020a,\n2021b), and (3) improved pretraining (Gao and\nCallan, 2021; O \u02d8guz et al., 2021).",
  "Similarly, on the HoVer (Jiang et al., 2020) dev\nset, Baleen\u2019s retrieval R@100 dropped from 92.2%\nto only 90.6% but its sentence-level exact match\nremained roughly the same, going from 39.2% to\n39.4%. We hypothesize that the supervision meth-\nods applied in ColBERTv2 (\u00a73.2) can also be ap-\nplied to lift quality in downstream tasks by improv-\ning the recall of retrieval for these tasks. We leave\nsuch exploration for future work.\nC Retrieval Latency\nFigure 3 evaluates the latency of ColBERTv2\nacross three collections of varying sizes, namely,\nMS MARCO, LoTTE Pooled (dev), and LoTTE\nLifestyle (dev), which contain approximately 9M\npassages, 2.4M answer posts, and 270k answer\nposts, respectively. We average latency across three\nruns of the MS MARCO dev set and the LoTTE\n\u201csearch\u201d queries.",
  "These supervision gains\nchallenge the value of \ufb01ne-grained late interaction,\nand it is not inherently clear whether the stronger\ninductive biases of ColBERT-like models permit it\nto accept similar gains under distillation, especially\nwhen using compressed representations. Despite\nthis, we \ufb01nd that with denoised supervision and\nresidual compression, ColBERTv2 achieves the\nhighest quality across all systems. As we discuss\nin \u00a75.3, it exhibits space footprint competitive with\nthese single-vector models and much lower than\nvanilla ColBERT.\nBesides the of\ufb01cial dev set, we evaluated Col-\nBERTv2, SPLADEv2, and RocketQAv2 on the\n\u201cLocal Eval\u201d test set described by Khattab and Za-\nharia (2020) for MS MARCO, which consists of\n5000 queries disjoint with the training and the of-\n\ufb01cial dev sets.",
  "a cross-encoder and hard-negative mining (\u00a73.2)\nto boost quality beyond any existing method, and\nthen uses a residual compression mechanism (\u00a73.3)\nto reduce the space footprint of late interaction by\n6\u201310\u0002while preserving quality. As a result, Col-\nBERTv2 establishes state-of-the-art retrieval qual-\nity both within andoutside its training domain with\na competitive space footprint with typical single-\nvector models.\nWhen trained on MS MARCO Passage Rank-\ning, ColBERTv2 achieves the highest MRR@10 of\nany standalone retriever. In addition to in-domain\nquality, we seek a retriever that generalizes \u201czero-\nshot\u201d to domain-speci\ufb01c corpora and long-tail top-\nics, ones that are often under-represented in large\npublic training sets. To this end, we evaluate Col-\nBERTv2 on a wide array of out-of-domain bench-\nmarks. These include three Wikipedia Open-QA\nretrieval tests and 13 diverse retrieval and semantic-\nsimilarity tasks from BEIR (Thakur et al., 2021).",
  "Shift-\ning our attention to models with distillation, we see\na similar pattern: while distillation-based models\nare generally stronger than their vanilla counter-\nparts, the models that decompose scoring into term-\nlevel interactions, ColBERTv2 and SPLADEv2,\nare almost always the strongest.\nLooking more closely into the comparison be-\ntween SPLADEv2 and ColBERTv2, we see that\nColBERTv2 has an advantage on six benchmarks\nand ties SPLADEv2 on two, with the largest im-\nprovements attained on NQ, TREC-COVID, and\nFiQA-2018, all of which feature natural search\nqueries. On the other hand, SPLADEv2 has the\nlead on \ufb01ve benchmarks, displaying the largest\ngains on Climate-FEVER (C-FEVER) and Hot-\nPotQA. In C-FEVER, the input queries are sen-\ntences making climate-related claims and, as a re-\nsult, do not re\ufb02ect the typical characteristics of\nsearch queries. In HotPotQA, queries are written\nby crowdworkers who have access to the target pas-",
  "We have observed consistent gains for Col-\nBERTv2 against existing state-of-the-art systems\nacross many diverse settings. Despite this, almost\nall IR datasets contain false negatives (i.e., rele-\nvant but unlabeled passages) and thus some cau-\ntion is needed in interpreting any individual result.\nNonetheless, we intentionally sought out bench-\nmarks with dissimilar annotation biases: for in-\nstance, TREC-COVID (in BEIR) annotates the\npool of documents retrieved by the systems submit-\nted at the time of the competition, LoTTE uses au-\ntomatic Google rankings (for \u201csearch\u201d queries) and\nStackExchange question\u2013answer pairs (for \u201cforum\u201d\nqueries), and the Open-QA tests rely on passage-\nanswer overlap for factoid questions. ColBERTv2\nperformed well in all of these settings. We discuss\nother issues pertinent to LoTTE in Appendix \u00a7D.\nWe have compared with a wide range of strong\nbaselines\u2014including sparse retrieval and single-\nvector models\u2014and found reliable patterns across\ntests.",
  "These queries are obtained from\nlabeled 50k queries that are provided in the of\ufb01cial\nMS MARCO Passage Ranking task as additional\nvalidation data.4On this test set, ColBERTv2 ob-\ntains 40.8% MRR@10, considerably outperform-\ning the baselines, including RocketQAv2 which\nmakes use of document titles in addition to the\npassage text unlike the other systems.\n4These are sampled from delta between qrels.dev.tsv\nand qrels.dev.small.tsv on https://microsoft.\ngithub.io/msmarco/Datasets . We refer to Khattab and\nZaharia (2020) for details. All our query IDs will be made\npublic to aid reproducibility.",
  "In this\nwork, we introduce ColBERTv2, a retriever\nthat couples an aggressive residual compres-\nsion mechanism with a denoised supervision\nstrategy to simultaneously improve the quality\nand space footprint of late interaction. We\nevaluate ColBERTv2 across a wide range\nof benchmarks, establishing state-of-the-art\nquality within and outside the training domain\nwhile reducing the space footprint of late\ninteraction models by 6\u201310 \u0002.\n1 Introduction\nNeural information retrieval (IR) has quickly domi-\nnated the search landscape over the past 2\u20133 years,\ndramatically advancing not only passage and doc-\nument search (Nogueira and Cho, 2019) but also\nmany knowledge-intensive NLP tasks like open-\ndomain question answering (Guu et al., 2020),\nmulti-hop claim veri\ufb01cation (Khattab et al., 2021a),\nand open-ended generation (Paranjape et al., 2022).",
  "Statistics Figure 4 plots the number of words\nper passage in each LoTTE dev corpus. Figures 5\nand 6 plot the number of words and number of\ncorresponding answer passages respectively per\nquery, split across search and forum queries.\nDev Results Table 7 presents out-of-domain eval-\nuation results on the LoTTE dev queries. Continu-\ning the trend we observed in 5, ColBERTv2 consis-\ntently outperforms all other models we tested.Licensing and Anonymity The original Stack-\nExchange post archive is licensed under a Cre-\native Commons BY-SA 4.0 license (sta). Personal\ndata is removed from the archive before being up-\nloaded, though all posts are public; when we re-\nlease LoTTE publicly we will include URLs to the\noriginal posts for proper attribution as required by\nthe license. The GooAQ dataset is licensed under\nan Apache license, version 2.0 (Khashabi et al.,\n2021). We will also release LoTTE with a CC BY-\nSA 4.0 license.",
  "2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426 .\nVera Boteva, Demian Gholipour, Artem Sokolov, and\nStefan Riezler. 2016. A Full-text Learning to Rank\nDataset for Medical Information Retrieval. In Eu-\nropean Conference on Information Retrieval , pages\n716\u2013722. Springer.\nChia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur\nAgrawal, Wei Zhang, and Kailash Gopalakrishnan.\n2018. Adacomp : Adaptive residual gradient com-\npression for data-parallel distributed training.",
  "Many neural IR methods follow a single-vector\nsimilarity paradigm: a pretrained language model\nis used to encode each query and each document\ninto a single high-dimensional vector, and rele-\nvance is modeled as a simple dot product between\nboth vectors. An alternative is late interaction , in-\ntroduced in ColBERT (Khattab and Zaharia, 2020),\nwhere queries and documents are encoded at a \ufb01ner-\ngranularity into multi-vector representations, and\n\u0003Equal contribution.relevance is estimated using rich yet scalable in-\nteractions between these two sets of vectors. Col-\nBERT produces an embedding for every token in\nthe query (and document) and models relevance\nas the sum of maximum similarities between each\nquery vector and all vectors in the document.\nBy decomposing relevance modeling into token-\nlevel computations, late interaction aims to reduce\nthe burden on the encoder: whereas single-vector\nmodels must capture complex query\u2013document re-\nlationships within one dot product, late interaction\nencodes meaning at the level of tokens and del-\negates query\u2013document matching to the interac-\ntion mechanism.",
  "We report Success@5 for each corpus on\nboth search queries and forum queries.\nOverall, we see that ANCE and vanilla Col-\nBERT outperform BM25 on all topics, and that\nthe three methods using distillation are generally\nthe strongest. Similar to the Wikipedia-OpenQA\nresults, we \ufb01nd that ColBERTv2 outperforms the\nbaselines across all topics for both query types, im-\nproving upon SPLADEv2 and RocketQAv2 by up\nto 3.7 and 8.1 points, respectively. Considering\nthe baselines, we observe that while RocketQAv2\ntends to have a slight advantage over SPLADEv2\non the \u201csearch\u201d queries, SPLADEv2 is consider-\nably more effective on the \u201cforum\u201d tests. We hy-\npothesize that the search queries, obtained from\nGoogle (through GooAQ) are more similar to MSMARCO than the forum queries and, as a result,\nthe latter stresses generalization more heavily, re-\nwarding term-decomposed models like SPLADEv2\nand ColBERTv2.",
  "The SPLADE family also piggybacks on\nthe language modeling capacity acquired by BERT\nduring pretraining. SPLADEv2 has been shown\nto be highly effective, within and across domains,\nand it is a central point of comparison in the exper-\niments we report on in this paper.\n2.2 Vector Compression for Neural IR\nThere has been a surge of recent interest in com-\npressing representations for IR. Izacard et al. (2020)\nexplore dimension reduction, product quantization\n(PQ), and passage \ufb01ltering for single-vector retriev-\ners. BPR (Yamada et al., 2021a) learns to directly\nhash embeddings to binary codes using a differen-\ntiable tanh function. JPQ (Zhan et al., 2021a) and\nits extension, RepCONC (Zhan et al., 2022), use\nPQ to compress embeddings, and jointly train the\nquery encoder along with the centroids produced",
  "arXiv preprint\narXiv:2002.10957 .\nBenchang Wei, Tao Guan, and Junqing Yu. 2014.\nProjected Residual Vector Quantization for ANN\nSearch. IEEE multimedia , 21(3):41\u201351.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations , pages 38\u201345, Online. Asso-\nciation for Computational Linguistics.",
  "Dataset License # Passages # Test Queries\nArguAna (Wachsmuth et al. 2018) CC BY 4.0 8674 1406\nClimate-Fever (Diggelmann et al. 2020) Not reported 5416593 1535\nDBPedia (Auer et al. 2007) CC BY-SA 3.0 4635922 400\nFEVER (Thorne et al. 2018) CC BY-SA 3.0\nFiQA-2018 (Maia et al. 2018) Not reported 57638 648\nHotpotQA (Yang et al. 2018b) CC BY-SA 4.0 5233329 7405\nNFCorpus (Boteva et al. 2016) Not reported 3633 323\nNQ (Kwiatkowski et al. 2019) CC BY-SA 3.0 2681468 3452\nSCIDOCS (Cohan et al. 2020)GNU General Public\nLicense v3.025657 1000\nSciFact (Wadden et al.",
  "D LoTTE\nDomain coverage Table 9 presents the full dis-\ntribution of communities in the LoTTE dev dataset.\n8These settings are selected based on preliminary explo-\nration of these parameters, which indicated that performance\nfor larger probe values tends to require scoring a larger num-\nber of candidates.\n0 200 400 600\nWords per passagePooledLifestyleTechnologyScienceRecreationWritingFigure 4: LoTTE words per passage\n5 10 15 20\nWords per query[Forum] Pooled[Forum] Lifestyle[Forum] Technology[Forum] Science[Forum] Recreation[Forum] Writing[Search] Pooled[Search] Lifestyle[Search] Technology[Search] Science[Search] Recreation[Search] Writing\nFigure 5: LoTTE words per query\nThe topics covered by LoTTE cover a wide range\nof linguistic phenomena given the diversity in top-\nics and communities represented. However, since\nall posts are submitted by anonymous users we do\nnot have demographic information regarding the\nidentify of the contributors. All posts are written\nin English.",
  "We feed each of those query\u2013passage\npairs into a cross-encoder reranker. We use a\n22M-parameter MiniLM (Wang et al., 2020) cross-\nencoder trained with distillation by Thakur et al.\n(2021).2This small model has been shown to ex-\nhibit very strong performance while being rela-\ntively ef\ufb01cient for inference, making it suitable\nfor distillation.\nWe then collect w-way tuples consisting of a\nquery, a highly-ranked passage (or labeled posi-\ntive), and one or more lower-ranked passages. In\nthis work, we use w= 64 passages per example.\nLike RocketQAv2 (Ren et al., 2021b), we use a\n2https://huggingface.co/cross-encoder/\nms-marco-MiniLM-L-6-v2KL-Divergence loss to distill the cross-encoder\u2019s\nscores into the ColBERT architecture. We use KL-\nDivergence as ColBERT produces scores (i.e., the\nsum of cosine similarities) with a restricted scale,\nwhich may not align directly with the output scores\nof the cross-encoder.",
  "In\npractice, user-facing IR and QA applications often\npertain to domain-speci\ufb01c corpora, for which little\nto no training data is available and whose topics\nare under-represented in large public collections.\nThis out-of-domain regime has received recent\nattention with the BEIR (Thakur et al., 2021) bench-\nmark. BEIR combines several existing datasets\ninto a heterogeneous suite for \u201czero-shot IR\u201d tasks,\nspanning bio-medical, \ufb01nancial, and scienti\ufb01c do-\nmains. While the BEIR datasets provide a use-\nful testbed, many capture broad semantic related-\nness tasks\u2014like citations, counter arguments, or\nduplicate questions\u2013instead of natural search tasks,\nor else they focus on high-popularity entities like\nthose in Wikipedia. In \u00a74, we introduce LoTTE, a\nnew dataset for out-of-domain retrieval, exhibiting\nnatural search queries over long-tail topics.\n3 ColBERTv2\nWe now introduce ColBERTv2, which improves\nthe quality of multi-vector retrieval models (\u00a73.2)\nwhile reducing their space footprint (\u00a73.3).",
  "We then explored improved\nsupervision for multi-vector retrieval and found\nthat their quality improves considerably upon distil-\nlation from a cross-encoder system. The proposed\nColBERTv2 considerably outperforms existing re-\ntrievers in within-domain and out-of-domain evalu-\nations, which we conducted extensively across 28\ndatasets, establishing state-of-the-art quality while\nexhibiting competitive space footprint.",
  "For\nthe ArguAna test in BEIR, as the queries are them-\nselves long documents, we set the maximum query\nlength used by ColBERTv2 and RocketQAv2 to\n300. For Climate-FEVER, as the queries are rela-\ntively long sentence claims, we set the maximum\nquery length used by ColBERTv2 to 64.\nWe use the open source BEIR implementation11\nand SPLADEv2 evaluation12code as the basis for\nour evaluations of SPLADEv2 and ANCE as well\nas for BM25 on LoTTE. We use the Anserini (Yang\net al., 2018a) toolkit for BM25 on the Wikipedia\nOpen-QA retrieval tests as in Khattab et al. (2021b).\nWe use the implementation developed by the Rock-\netQAv2 authors for evaluating RocketQAv2.13\n10https://github.com/facebookresearch/DPR/blob/\nmain/dpr/data/qa_validation.py\n11https://github.com/UKPLab/beir\n12https://github.com/naver/splade\n13https://github.com/PaddlePaddle/RocketQA",
  "(2021b), and LoTTE. We compare against a wide\nrange of recent and state-of-the-art retrieval sys-\ntems from the literature.\nBEIR. We start with BEIR, reporting the quality\nof models that do not incorporate distillation from\ncross-encoders, namely, ColBERT (Khattab and\nZaharia, 2020), DPR-MARCO (Xin et al., 2021),\nANCE (Xiong et al., 2020), and MoDIR (Xin et al.,\n2021), as well as models that do utilize distil-\nlation, namely, TAS-B (Hofst\u00e4tter et al., 2021),\nSPLADEv2 (Formal et al., 2021a), and also Rock-\netQAv2, which we test ourselves using the of\ufb01cial\ncheckpoint trained on MS MARCO.",
  "In Proceedings of\nthe 43rd International ACM SIGIR conference on re-\nsearch and development in Information Retrieval, SI-\nGIR 2020, Virtual Event, China, July 25-30, 2020 ,\npages 39\u201348. ACM.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n\ufb01eld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics , 7:452\u2013466.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021a. Learning dense representations of\nphrases at scale.",
  "stackexchange.com 20601 16 182\nparenting.stackexchange.com 18357 10 87\ncrafts.stackexchange.com 3094 4 50\noutdoors.stackexchange.com 13324 16 76\ncoffee.stackexchange.com 2249 11 50\nmusic.stackexchange.com 47399 65 287\ndiy.stackexchange.com 82659 135 732\nbicycles.stackexchange.com 35567 40 229\nmechanics.stackexchange.com 27680 98 246\nTable 9: Per-community distribution of LoTTE dev dataset passages and questions.",
  "We verify that every query\nhas at least one corresponding answer passage.\nForum queries For each LoTTE topic and its\nconstituent communities we \ufb01rst compute the frac-\ntion of the total queries attributed to each individ-\nual community. We then use this distribution to\nconstruct a truncated query set by selecting the",
  "Table 6 presents examples to highlight the se-\nmantic space captured by the centroids. The most\nfrequently appearing tokens in cluster #917 relate\nto photography; these include, for example, \u2018pho-\ntos\u2019 and \u2018photographs\u2019. If we then examine the\nadditional clusters in which these tokens appear,\nwe \ufb01nd that there is substantial semantic overlap\nbetween these new clusters (e.g., Photos-Photo,\nPhoto-Image-Picture) and cluster #917. We ob-\nserve a similar effect with tokens appearing in clus-\nter #216932, comprising tornado-related terms.\nThis analysis indicates that cluster centroids can\nsummarize the ColBERT representations with high\nprecision. In \u00a73.3, we propose a residual compres-\nsion mechanism that uses these centroids along\nwith minor re\ufb01nements at the dimension level to\nef\ufb01ciently encode late-interaction vectors.\nB Impact of Compression\nOur residual compression approach (\u00a73.3) pre-\nserves approximately the same quality as the un-\ncompressed embeddings.",
  "0 70.0 67.6 78.6 78.5\nQuora 85.4 84.2 85.2 85.6 83.5 74.9 83.8 85.2\nSCIDOCS 14.5 10.8 12.2 12.4 14.9 13.1 15.8 15.4\nSciFact 67.1 47.8 50.7 50.2 64.3 56.8 69.3 69.3\n(a)CorpusColBERTBM25ANCERocketQAv2SPLADEv2ColBERTv2\nOOD Wikipedia Open QA (Success@5)\nNQ-dev 65.7 44.6 - - 65.6 68.9\nTQ-dev 72.6 67.6 - - 74.7 76.7\nSQuAD -dev 60.0 50.6 - - 60.4 65.0\nLoTTE Search Test Queries (Success@5)\nWriting 74.7 60.3 74.4 78.0 77.",
  "stackexchange.com 76450 66 302\nastronomy.stackexchange.com 14580 15 88\nearthscience.stackexchange.com 6734 10 50\nengineering.stackexchange.com 12064 16 77\ndatascience.stackexchange.com 23234 15 156\nphilosophy.stackexchange.com 27061 34 124\nTechnologysuperuser.com 418266 441 648\nelectronics.stackexchange.com 205891 118 314\naskubuntu.com 296291 132 480\nserverfault.com 323943 148 506\nwebapps.stackexchange.com 31831 77 55\nLifestylepets.stackexchange.com 10070 20 87\nlifehacks.stackexchange.com 7893 2 50\ngardening.stackexchange.com 20601 16 182\nparenting.stackexchange.com 18357 10 87\ncrafts.stackexchange.com 3094 4 50\noutdoors.stackexchange.com 13324 16 76\ncoffee.stackexchange.",
  "In the compression\nevaluation \u00a7B, we used models trained in-domain\non NQ and HoVer, whose training sets contain 79k\nand 18k queries, respectively.\nF Implementation & Hyperparameters\nWe implement ColBERTv2 using Python 3.7,\nPyTorch 1.9, and HuggingFace Transformers\n4.10 (Wolf et al., 2020), extending the original im-\nplementation of ColBERT by Khattab and Zaharia\n(2020). We use FAISS 1.7 (Johnson et al., 2019) for",
  "The GooAQ dataset is licensed under\nan Apache license, version 2.0 (Khashabi et al.,\n2021). We will also release LoTTE with a CC BY-\nSA 4.0 license. The search queries can be used for\nnon-commercial research purposes only as per the\nGooAQ license.\nE Datasets in BEIR\nTable 8 lists the BEIR datasets we used in our evalu-\nation, including their respective license information\nas well as the numbers of documents as well as the\nnumber of test set queries. We refer to Thakur et al.\n(2021) for a more detailed description of each of\nthe datasets.\nOur Touch\u00e9 evaluation uses an updated version\nof the data in BEIR, which we use for evaluating the\nmodels we run (i.e., ColBERTv2 and RocketQAv2)\nas well as SPLADEv2.\nDataset License # Passages # Test Queries\nArguAna (Wachsmuth et al. 2018) CC BY 4.0 8674 1406\nClimate-Fever (Diggelmann et al.",
  "2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim veri\ufb01cation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n3441\u20133460, Online. Association for Computational\nLinguistics.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data .\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601\u20131611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.",
  "B Impact of Compression\nOur residual compression approach (\u00a73.3) pre-\nserves approximately the same quality as the un-\ncompressed embeddings. In particular, when ap-\nplied to a vanilla ColBERT model on MS MARCO\nwhose MRR@10 is 36.2% and Recall@50 is\n82.1%, the quality of the model with 2-bit compres-\nsion is 36.2% MRR@10 and 82.3% Recall@50.\nWith 1-bit compression, the model achieves 35.5%\nMRR@10 and 81.6% Recall@50.7\nWe also tested the residual compression ap-\nproach on late-interaction retrievers that conduct\ndownstream tasks, namely, ColBERT-QA (Khat-\ntab et al., 2021b) for the NaturalQuestions open-\ndomain QA task, and Baleen (Khattab et al., 2021a)\nfor multi-hop reasoning on HoVer for claim veri\ufb01-\ncation.",
  "Topic Communities # Passages # Search queries # Forum queries\nWritingell.stackexchange.com 108143 433 1196\nliterature.stackexchange.com 4778 7 58\nwriting.stackexchange.com 29330 23 163\nlinguistics.stackexchange.com 12302 22 116\nworldbuilding.stackexchange.com 122519 12 470\nRecreationrpg.stackexchange.com 89066 91 621\nboardgames.stackexchange.com 20340 67 179\nsci\ufb01.stackexchange.com 102561 343 852\nphoto.stackexchange.com 51058 62 350\nSciencechemistry.stackexchange.com 39435 245 267\nstats.stackexchange.com 144084 137 949\nacademia.stackexchange.com 76450 66 302\nastronomy.stackexchange.com 14580 15 88\nearthscience.stackexchange.com 6734 10 50\nengineering.stackexchange.com 12064 16 77\ndatascience.stackexchange.",
  "ColBERTv2\nperformed well in all of these settings. We discuss\nother issues pertinent to LoTTE in Appendix \u00a7D.\nWe have compared with a wide range of strong\nbaselines\u2014including sparse retrieval and single-\nvector models\u2014and found reliable patterns across\ntests. However, we caution that empirical trends\ncan change as innovations are introduced to each of\nthese families of models and that it can be dif\ufb01cult\nto ensure exact apple-to-apple comparisons across\nfamilies of models, since each of them calls for\ndifferent sophisticated tuning strategies. We thus\nprimarily used results and models from the rich\nrecent literature on these problems, with models\nlike RocketQAv2 and SPLADEv2.\nOn the representational side, we focus on reduc-\ning the storage cost using residual compression,\nachieving strong gains in reducing footprint while\nlargely preserving quality. Nonetheless, we have\nnot exhausted the space of more sophisticated opti-\nmizations possible, and we would expect more so-\nphisticated forms of residual compression and com-\nposing our approach with dropping tokens (Zhou\nand Devlin, 2021) to open up possibilities for fur-\nther reductions in space footprint.",
  "In International Conference on\nLearning Representations .\nIkuya Yamada, Akari Asai, and Hannaneh Hajishirzi.\n2021a. Ef\ufb01cient passage retrieval with hashing for\nopen-domain question answering. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing\n(Volume 2: Short Papers) , pages 979\u2013986, Online.\nAssociation for Computational Linguistics.\nIkuya Yamada, Akari Asai, and Hannaneh Hajishirzi.\n2021b. Ef\ufb01cient passage retrieval with hashing for\nopen-domain question answering. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers) , pages 979\u2013986, Online.\nAssociation for Computational Linguistics.\nPeilin Yang, Hui Fang, and Jimmy Lin. 2018a.\nAnserini: Reproducible ranking baselines using\nlucene.",
  "Like ColBERT, our encoder is a\nbert-base-uncased model that is shared\nbetween the query and passage encoders and which\nhas 110M parameters. We retain the default vector\ndimension suggested by Khattab and Zaharia\n(2020) and used in subsequent work, namely,\nd=128. For the experiments reported in this paper,\nwe train on MS MARCO training set. We use\nsimple defaults with limited manual exploration on\nthe of\ufb01cial development set for the learning rate\n(10\u00005), batch size (32 examples), and warm up\n(for 20,000 steps) with linear decay.\nHyperparameters corresponding to retrieval are\nexplored in \u00a7C. We default to probe = 2, but\nuseprobe = 4 on the largest datasets, namely,\nMS MARCO and Wikipedia. By default we set\ncandidates =probe\u0003212, but for Wikipedia\nwe set candidates =probe\u0003213and for MS\nMARCO we set candidates =probe\u0003214. We\nleave extensive tuning of hyperparameters to future\nwork.",
  "2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 6086\u20136096, Florence,\nItaly. Association for Computational Linguistics.\nYue Li, Wenrui Ding, Chunlei Liu, Baochang Zhang,\nand Guodong Guo. 2021a. TRQ: Ternary Neural\nNetworks With Residual Quantization. In Proceed-\nings of the AAAI Conference on Arti\ufb01cial Intelli-\ngence , volume 35, pages 8538\u20138546.\nZefan Li, Bingbing Ni, Teng Li, Xiaokang Yang, Wen-\njun Zhang, and Wen Gao. 2021b. Residual Quanti-\nzation for Low Bit-width Neural Networks. IEEE\nTransactions on Multimedia .\nJimmy Lin and Xueguang Ma. 2021. A Few Brief\nNotes on DeepImpact, COIL, and a Conceptual\nFramework for Information Retrieval Techniques.\narXiv preprint arXiv:2106.14807 .",
  "Table 2: Examples of queries and shortened snippets of\nanswer passages from LoTTE. The \ufb01rst two examples\nshow \u201csearch\u201d queries, whereas the last two are \u201cfo-\nrum\u201d queries. Snippets are shortened for presentation.\nThese queries tend to have a wider variety than\nthe \u201csearch\u201d queries, while the search queries may\nexhibit more natural patterns. Table 3 compares a\nrandom samples of search and forum queries. It\ncan be seen that search queries tend to be brief,\nknowledge-based questions with direct answers,\nwhereas forum queries tend to re\ufb02ect more open-\nended questions. Both query sets target topics that\nexceed the scope of a general-purpose knowledge\nrepository such as Wikipedia.\nFor search as well as forum queries, the result-\ning evaluation set consists of a query and a target\nset of StackExchange answer posts (in particular,\nthe answer posts from the target StackExchange\npage). Similar to evaluation in the Open-QA lit-\nerature (Karpukhin et al., 2020; Khattab et al.,",
  "4 - 94.3 - - -\nDPR 31.1 - 95.2 - - -\nANCE 33.0 - 95.9 - - -\nLTRe 34.1 - 96.2 - - -\nColBERT 36.0 82.9 96.8 36.7 - -\nModels with Distillation or Special Pretraining\nTAS-B 34.7 - 97.8 - - -\nSPLADEv2 36.8 - 97.9 37.9 84.9 98.0\nPAIR 37.9 86.4 98.2 - - -\ncoCondenser 38.2 - 98.4 - - -\nRocketQAv2 38.8 86.2 98.1 39.8 85.8 97.9\nColBERTv2 39.7 86.8 98.4 40.8 86.3 98.3\nTable 4: In-domain performance on the development\nset of MS MARCO Passage Ranking as well the \u201cLocal\nEval\u201d test set described by Khattab and Zaharia (2020).",
  "2021a. Baleen: Robust Multi-Hop Reasoning at\nScale via Condensed Retrieval. In Thirty-Fifth Con-\nference on Neural Information Processing Systems .\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021b. Relevance-guided supervision for openqa\nwith ColBERT. Transactions of the Association for\nComputational Linguistics , 9:929\u2013944.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Ef-\n\ufb01cient and effective passage search via contextual-\nized late interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on re-\nsearch and development in Information Retrieval, SI-\nGIR 2020, Virtual Event, China, July 25-30, 2020 ,\npages 39\u201348. ACM.",
  "Springer.\nChia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur\nAgrawal, Wei Zhang, and Kailash Gopalakrishnan.\n2018. Adacomp : Adaptive residual gradient com-\npression for data-parallel distributed training. In\nProceedings of the Thirty-Second AAAI Conference\non Arti\ufb01cial Intelligence, (AAAI-18), the 30th inno-\nvative Applications of Arti\ufb01cial Intelligence (IAAI-\n18), and the 8th AAAI Symposium on Educational\nAdvances in Arti\ufb01cial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018 , pages\n2827\u20132835. AAAI Press.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel Weld. 2020. SPECTER:\nDocument-level representation learning using\ncitation-informed transformers. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 2270\u20132282,\nOnline. Association for Computational Linguistics.",
  "Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769\u2013\n6781, Online. Association for Computational Lin-\nguistics.\nDaniel Khashabi, Amos Ng, Tushar Khot, Ashish Sab-\nharwal, Hannaneh Hajishirzi, and Chris Callison-\nBurch. 2021. GooAQ: Open Question Answer-\ning with Diverse Answer Types. arXiv preprint\narXiv:2104.08727 .Omar Khattab, Christopher Potts, and Matei Zaharia.\n2021a. Baleen: Robust Multi-Hop Reasoning at\nScale via Condensed Retrieval. In Thirty-Fifth Con-\nference on Neural Information Processing Systems .",
  "Association for Computational Linguistics.\nPeilin Yang, Hui Fang, and Jimmy Lin. 2018a.\nAnserini: Reproducible ranking baselines using\nlucene. Journal of Data and Information Quality\n(JDIQ) , 10(4):1\u201320.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018b. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2369\u20132380, Brussels, Belgium. Association\nfor Computational Linguistics.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2021a. Jointly Optimiz-\ning Query Encoder and Product Quantization to Im-\nprove Retrieval Performance. In Proceedings of the\n30th ACM International Conference on Information\n& Knowledge Management , pages 2487\u20132496.",
  "2021a.\nSPLADE v2: Sparse Lexical and Expansion\nModel for Information Retrieval. arXiv preprint\narXiv:2109.10086 .\nThibault Formal, Benjamin Piwowarski, and St\u00e9phane\nClinchant. 2021b. SPLADE: Sparse Lexical and Ex-\npansion Model for First Stage Ranking. In Proceed-\nings of the 44th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval , pages 2288\u20132292.\nLuyu Gao and Jamie Callan. 2021. Unsupervised cor-\npus aware language model pre-training for dense\npassage retrieval. arXiv preprint arXiv:2108.05540 .\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Mod-\nularized transfomer-based ranking framework. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4180\u20134190, Online. Association for Computa-\ntional Linguistics.",
  "By default we set\ncandidates =probe\u0003212, but for Wikipedia\nwe set candidates =probe\u0003213and for MS\nMARCO we set candidates =probe\u0003214. We\nleave extensive tuning of hyperparameters to future\nwork.\nWe train on MS MARCO using 64-way tuples\nfor distillation, sampling them from the top-500\nretrieved passages per query. The training set of\nMS MARCO contains approximately 800k queries,\nthough only about 500k have associated labels. We\napply distillation using all 800k queries, where\neach training example contains exactly one \u201cposi-\ntive\u201d, de\ufb01ned as a passage labeled as positive or the\ntop-ranked passage by the cross-encoder teacher,\nirrespective of its label.\nWe train for 400k steps, initializing from a pre-\n9https://github.com/facebookresearch/faiss\ufb01netuned checkpoint using 32-way training exam-\nples and 150k steps. To generate the top- kpas-\nsages per training query, we apply two rounds, fol-\nlowing Khattab et al. (2021b).",
  "ColBERTv2:\nEffective and Ef\ufb01cient Retrieval via Lightweight Late Interaction\nKeshav Santhanam\u0003\nStanford UniversityOmar Khattab\u0003\nStanford UniversityJon Saad-Falcon\nGeorgia Institute of Technology\nChristopher Potts\nStanford UniversityMatei Zaharia\nStanford University\nAbstract\nNeural information retrieval (IR) has greatly\nadvanced search and other knowledge-\nintensive language tasks. While many neural\nIR methods encode queries and documents\ninto single-vector representations, late\ninteraction models produce multi-vector repre-\nsentations at the granularity of each token and\ndecompose relevance modeling into scalable\ntoken-level computations. This decomposition\nhas been shown to make late interaction more\neffective, but it in\ufb02ates the space footprint of\nthese models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever\nthat couples an aggressive residual compres-\nsion mechanism with a denoised supervision\nstrategy to simultaneously improve the quality\nand space footprint of late interaction.",
  "In 8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\nview.net.\nGautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola\nDe Cao, Sebastian Riedel, and Edouard Grave. 2020.\nA memory ef\ufb01cient baseline for open domain ques-\ntion answering. arXiv preprint arXiv:2012.15156 .\nHerve Jegou, Matthijs Douze, and Cordelia Schmid.\n2010. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and\nmachine intelligence , 33(1):117\u2013128.\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim veri\ufb01cation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n3441\u20133460, Online.",
  "8 68.9 71.6\nLoTTE Forum Test Queries (Success@5)\nWriting 71.0 64.0 68.8 71.5 73.0 76.3\nRecreation 65.6 55.4 63.8 65.7 67.1 70.8\nScience 41.8 37.1 36.5 38.0 43.7 46.1\nTechnology 48.5 39.4 46.8 47.3 50.8 53.6\nLifestyle 73.0 60.6 73.1 73.7 74.0 76.9\nPooled 58.2 47.2 55.7 57.7 60.1 63.4\n(b)\nTable 5: Zero-shot evaluation results. Sub-table (a) reports results on BEIR and sub-table (b) reports results on\nthe Wikipedia Open QA and the test sets of the LoTTE benchmark.",
  "ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\nlian, Massimiliano Ciaramita, and Markus Leippold.\n2020. CLIMATE-FEVER: A Dataset for Veri\ufb01ca-\ntion of Real-World Climate Claims. arXiv preprint\narXiv:2012.00614 .\nThibault Formal, Carlos Lassance, Benjamin Pi-\nwowarski, and St\u00e9phane Clinchant. 2021a.\nSPLADE v2: Sparse Lexical and Expansion\nModel for Information Retrieval. arXiv preprint\narXiv:2109.10086 .",
  "This computes a lower-bound on the true MaxSim\n(\u00a73.1) using the embeddings identi\ufb01ed via the in-\nverted list, which resembles the approximation ex-\nplored for scoring by Macdonald and Tonellotto\n(2021) but is applied for candidate generation.\nThese lower bounds are summed across the\nquery tokens, and the top-scoring ncandidate can-\ndidate passages based on these approximate scores\nare selected for ranking, which loads the complete\nset of embeddings of each passage, and conducts\nthe same scoring function using all embeddings\nper document following Equation 1. The result\npassages are then sorted by score and returned.\n4 LoTTE: Long-Tail, Cross-Domain\nRetrieval Evaluation\nWe introduce LoTTE (pronounced latte), a new\ndataset for Long-TailTopic-strati\ufb01ed Evaluation\nfor IR. To complement the out-of-domain tests of\nBEIR (Thakur et al., 2021), as motivated in \u00a72.4,\nLoTTE focuses on natural user queries that pertain\ntolong-tail topics , ones that might not be covered\nby an entity-centric knowledge base like Wikipedia.",
  "As a baseline, we repeat this clustering with ran-\ndom embeddings but keep the true distribution of\ntokens. Figure 2 presents empirical cumulative dis-\ntribution function (eCDF) plots representing the\nnumber of distinct non-stopword tokens appear-\ning in each cluster (2a) and the number of distinct\nclusters in which each token appears (2b).6Most\ntokens appear in a very small fraction of the num-\nber of centroids: in particular, we see that roughly\n90% of clusters have \u001416 distinct tokens with\n6We rank tokens by number of clusters they appear in and\ndesignate the top-1% (under 300) as stopwords.the ColBERT embeddings, whereas less than 50%\nof clusters have\u001416 distinct tokens with the ran-\ndom embeddings. This suggests that the centroids\neffectively map the ColBERT semantic space.\nTable 6 presents examples to highlight the se-\nmantic space captured by the centroids. The most\nfrequently appearing tokens in cluster #917 relate\nto photography; these include, for example, \u2018pho-\ntos\u2019 and \u2018photographs\u2019.",
  "2.We introduce LoTTE, a new resource for out-\nof-domain evaluation of retrievers. LoTTE fo-\ncuses on natural information-seeking queries\nover long-tail topics, an important yet under-\nstudied application space.\n3.We evaluate ColBERTv2 across a wide range\nof settings, establishing state-of-the-art qual-\nity within and outside the training domain.2 Background & Related Work\n2.1 Token-Decomposed Scoring in Neural IR\nMany neural IR approaches encode passages as\na single high-dimensional vector, trading off the\nhigher quality of cross-encoders for improved ef-\n\ufb01ciency and scalability (Karpukhin et al., 2020;\nXiong et al., 2020; Qu et al., 2021). Col-\nBERT\u2019s (Khattab and Zaharia, 2020) late inter-\naction paradigm addresses this tradeoff by com-\nputing multi-vector embeddings and using a scal-\nable \u201cMaxSim\u201d operator for retrieval.",
  "Q:what is xerror in rpart? Q:is sub question one word?\nQ:how to open a garage door without making noise? Q:\nis docx and dotx the same? Q:are upvotes and downvotes\nanonymous? Q:what is the difference between descriptive\nessay and narrative essay? Q:how to change default\nuser pro\ufb01le in chrome? Q:does autohotkey need to be\ninstalled? Q:how do you tag someone on facebook with\na youtube video? Q:has mjolnir ever been broken?\nQ:Snoopy can balance on an edge atop his doghouse. Is any\nreason given for this? Q:How many Ents were at the\nEntmoot? Q:What does a hexagonal sun tell us about\nthe camera lens/sensor? Q:Should I simply ignore it if\nauthors assume that Im male in their response to my review of\ntheir article? Q:Why is the 2s orbital lower in energy than\nthe 2p orbital when the electrons in 2s are usually farther from\nthe nucleus?",
  "Col-\nBERT\u2019s (Khattab and Zaharia, 2020) late inter-\naction paradigm addresses this tradeoff by com-\nputing multi-vector embeddings and using a scal-\nable \u201cMaxSim\u201d operator for retrieval. Several\nother systems leverage multi-vector representa-\ntions, including Poly-encoders (Humeau et al.,\n2020), PreTTR (MacAvaney et al., 2020), and\nMORES (Gao et al., 2020), but these target\nattention-based re-ranking as opposed to Col-\nBERT\u2019s scalable MaxSim end-to-end retrieval.\nME-BERT (Luan et al., 2021) generates token-\nlevel document embeddings similar to ColBERT,\nbut retains a single embedding vector for queries.\nCOIL (Gao et al., 2021) also generates token-level\ndocument embeddings, but the token interactions\nare restricted to lexical matching between query\nand document terms.",
  "2021a. Jointly Optimiz-\ning Query Encoder and Product Quantization to Im-\nprove Retrieval Performance. In Proceedings of the\n30th ACM International Conference on Information\n& Knowledge Management , pages 2487\u20132496.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2021b. Optimizing Dense\nRetrieval Model Training with Hard Negatives. In\nProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval , pages 1503\u20131512.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2022. Learning discrete\nrepresentations via constrained clustering for effec-\ntive and ef\ufb01cient dense retrieval. In Proceedings\nof the Fifteenth ACM International Conference on\nWeb Search and Data Mining , WSDM \u201922, page\n1328\u20131336. Association for Computing Machinery.",
  "2020b. Repbert: Contextualized text\nembeddings for \ufb01rst-stage retrieval. arXiv preprint\narXiv:2006.15498 .\nGiulio Zhou and Jacob Devlin. 2021. Multi-vector\nattention models for deep re-ranking. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing , pages 5452\u20135456,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.",
  "We present the tokens that appear most\nfrequently in the selected clusters as well as additional clusters the top tokens appear in.\n38.50 38.75 39.00 39.25 39.50 39.75\nMRR@1050100150200250Query Latency (ms)MS MARCO\n68.0 68.5 69.0 69.5\nSuccess@5LoTTE Pooled (dev)\n74.0 74.5 75.0 75.5 76.0\nSuccess@5LoTTE Lifestyle (dev)\nprobe\n1\n2\n4\nbits\n2\n1\ncandidates\nprobe x 2^14\nprobe x 2^12\nFigure 3: Latency vs. retrieval quality with varying parameter con\ufb01gurations for three datasets of different collec-\ntion sizes. We sweep a range of values for the number of centroids per vector ( probe ), the number of bits used\nfor residual compression, and the number of candidates. Note that retrieval quality is measured in MRR@10 for\nMS MARCO and Success@5 for LoTTE datasets.",
  "Aditya Krishna Menon, Sadeep Jayasumana, Se-\nungyeon Kim, Ankit Singh Rawat, Sashank J. Reddi,\nand Sanjiv Kumar. 2022. In defense of dual-\nencoders for neural ranking.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human-generated MAchine\nreading COmprehension dataset. arXiv preprint\narXiv:1611.09268 .\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage Re-ranking with BERT. arXiv preprint\narXiv:1901.04085 .Barlas O \u02d8guz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis, Vladimir Karpukhin, Aleksandra Piktus,\nXilun Chen, Sebastian Riedel, Wen-tau Yih, Sonal\nGupta, et al. 2021. Domain-matched Pre-training\nTasks for Dense Retrieval.",
  "To complement the out-of-domain tests of\nBEIR (Thakur et al., 2021), as motivated in \u00a72.4,\nLoTTE focuses on natural user queries that pertain\ntolong-tail topics , ones that might not be covered\nby an entity-centric knowledge base like Wikipedia.\nLoTTE consists of 12 test sets, each with 500\u20132000\nqueries and 100k\u20132M passages.\nThe test sets are explicitly divided by topic, and\neach test set is accompanied by a validation set of\nrelated but disjoint queries andpassages. We elect\nto make the passage texts disjoint to encourage\nmore realistic out-of-domain transfer tests, allow-\ning for minimal development on related but distinct\ntopics. The test (and dev) sets include a \u201cpooled\u201d\nsetting. In the pooled setting, the passages and\nqueries are aggregated across all test (or dev) topics\nto evaluate out-of-domain retrieval across a larger\nand more diverse corpus.\nTable 1 outlines the composition of LoTTE. We\nderive the topics and passage corpora from the\nanswer posts across various StackExchange fo-"
]