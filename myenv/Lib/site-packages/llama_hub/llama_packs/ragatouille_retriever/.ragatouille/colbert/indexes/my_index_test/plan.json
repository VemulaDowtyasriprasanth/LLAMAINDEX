{
    "config": {
        "query_token_id": "[unused0]",
        "doc_token_id": "[unused1]",
        "query_token": "[Q]",
        "doc_token": "[D]",
        "ncells": null,
        "centroid_score_threshold": null,
        "ndocs": null,
        "load_index_with_mmap": false,
        "index_path": null,
        "nbits": 8,
        "kmeans_niters": 20,
        "resume": false,
        "similarity": "cosine",
        "bsize": 64,
        "accumsteps": 1,
        "lr": 1e-5,
        "maxsteps": 400000,
        "save_every": null,
        "warmup": 20000,
        "warmup_bert": null,
        "relu": false,
        "nway": 64,
        "use_ib_negatives": true,
        "reranker": false,
        "distillation_alpha": 1.0,
        "ignore_scores": false,
        "model_name": null,
        "query_maxlen": 32,
        "attend_to_mask_tokens": false,
        "interaction": "colbert",
        "dim": 128,
        "doc_maxlen": 256,
        "mask_punctuation": true,
        "checkpoint": "colbert-ir\/colbertv2.0",
        "triples": "\/future\/u\/okhattab\/root\/unit\/experiments\/2021.10\/downstream.distillation.round2.2_score\/round2.nway6.cosine.ib\/examples.64.json",
        "collection": [
            "list with 129 elements starting with...",
            [
                "Similarly,\nthey-axis varies between approximately 50 mil-\nliseconds per query up to 250 milliseconds (mostly\nunder 150 milliseconds) using our relatively simple\nPython-based implementation.\nDigging deeper, we see that the best quality\nin these metrics can be achieved or approached\nclosely with around 100 milliseconds of latency\nacross all three datasets, despite their various sizes\nand characteristics, and that 2-bit indexing reliably\noutperforms 1-bit indexing but the loss from more\naggressive compression is small.\nD LoTTE\nDomain coverage Table 9 presents the full dis-\ntribution of communities in the LoTTE dev dataset.\n8These settings are selected based on preliminary explo-\nration of these parameters, which indicated that performance\nfor larger probe values tends to require scoring a larger num-\nber of candidates.",
                "As\nKhashabi et al. (2021) hypothesize, Google Search\nlikely maps these natural queries to their answers\nby relying on a wide variety of signals for rele-\nvance, including expert annotations, user clicks,\nand hyperlinks as well as specialized QA compo-\nnents for various question types with access to the\npost title and question body . Using those annota-\ntions as ground truth, we evaluate the models on\ntheir capacity for retrieval using only free text of\nthe answer posts (i.e., no hyperlinks or user clicks,\nquestion title or body, etc.), posing a signi\ufb01cant\nchallenge for IR and NLP systems trained only on\npublic datasets.\nForum Queries. We collect the forum queries\nby extracting post titles from the StackExchange\ncommunities to use as queries and collect their\ncorresponding answer posts as targets. We select\nquestions in order of their popularity and sample\nquestions according to the proportional contribu-\ntion of individual communities within each topic.Q:what is the difference between root and stem in lin-\nguistics? A:A root is the form to which derivational\naf\ufb01xes are added to form a stem.",
                "This\nadapts the DPR (Karpukhin et al., 2020) evaluation\ncode.10We use the preprocessed Wikipedia Dec\n2018 dump released by Karpukhin et al. (2020).\nFor out-of-domain evaluation, we elected to fol-\nlow Thakur et al. (2021) and set the maximum\ndocument length of ColBERT, RocketQAv2, and\nColBERTv2 to 300 tokens on BEIR and LoTTE.\nFormal et al. (2021a) selected maximum sequence\nlength 256 for SPLADEv2 both on MS MARCO\nand on BEIR for both queries and documents, and\nwe retained this default when testing their system\non LoTTE. Unless otherwise stated, we keep the\ndefault query maximum sequence length for Col-\nBERTv2 and RocketQAv2, which is 32 tokens. For\nthe ArguAna test in BEIR, as the queries are them-\nselves long documents, we set the maximum query\nlength used by ColBERTv2 and RocketQAv2 to\n300."
            ]
        ],
        "queries": "\/future\/u\/okhattab\/data\/MSMARCO\/queries.train.tsv",
        "index_name": "my_index_test",
        "overwrite": false,
        "root": ".ragatouille\/",
        "experiment": "colbert",
        "index_root": null,
        "name": "2024-01\/04\/15.53.44",
        "rank": 0,
        "nranks": 1,
        "amp": true,
        "gpus": 0
    },
    "num_chunks": 1,
    "num_partitions": 2048,
    "num_embeddings_est": 22185.00018310547,
    "avg_doclen_est": 171.97674560546875
}
