[
  "Foundations and Trends \u00aein Information Retrieval 13, 1 (2018), 1\u2013126.\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\nlocal and distributed representations of text for web search. In Proceedings of\nthe 26th International Conference on World Wide Web . International World Wide\nWeb Conferences Steering Commi/t_tee, 1291\u20131299.\n[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\nfor e\ufb03cient retrieval and ranking using deep neural networks. arXiv preprint\narXiv:1907.03693 (2019).\n[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchine\nReading COmprehension Dataset. (2016).",
  "We\ndivide each embedding into s=16 sub-vectors, each encoded using\none byte. To represent the index used for the second stage of our\nend-to-end retrieval procedure, we use 16-bit values per dimension.\n4.1.3 Hardware & Time Measurements. To evaluate the latency\nof neural re-ranking models in \u00a74.2, we use a single Tesla V100 GPU\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\n\u00a74.3 and the indexing experiments in \u00a74.5, we use another server\nwith the same CPU and system memory speci/f_ications but which\nhas four Titan V GPUs a/t_tached, each with 12 GiBs of memory.\nAcross all experiments, only one GPU is dedicated per query for\n5h/t_tps://github.com/huggingface/transformers",
  "2017. TREC\nComplex Answer Retrieval Overview.. In TREC .\n[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro/f_t. 2016. A deep relevance\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\non Conference on Information and Knowledge Management . ACM, 55\u201364.\n[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani,\nChen Wu, W Bruce Cro/f_t, and Xueqi Cheng. 2019. A deep look into neural\nranking models for information retrieval. arXiv preprint arXiv:1903.06902 (2019).\n[9] Sebastian Hofst \u00a8a/t_ter and Allan Hanbury. 2019. Let\u2019s measure run time! Extending\nthe IR replicability infrastructure to include performance aspects. arXiv preprint\narXiv:1907.04614 (2019).",
  "Shi/f_ting our a/t_tention to ColBERT\u2019s end-to-end retrieval e\ufb00ec-\ntiveness, we see its major gains in MRR@10 over all of these end-to-\nend models. In fact, using ColBERT in the end-to-end setup is supe-\nrior in terms of MRR@10 to re-ranking with the same model due\nto the improved recall. Moving beyond MRR@10, we also see large\ngains in Recall@ kforkequals to 50, 200, and 1000. For instance,\nits Recall@50 actually exceeds the o\ufb03cial BM25\u2019s Recall@1000 and\neven all but docTTTTTquery\u2019s Recall@200, emphasizing the value\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.",
  "[31] Colin Ra\ufb00el, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the\nlimits of transfer learning with a uni/f_ied text-to-text transformer. arXiv preprint\narXiv:1910.10683 (2019).\n[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication (1995).\n[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\n2019. Distilling task-speci/f_ic knowledge from BERT into simple neural networks.\narXiv preprint arXiv:1903.12136 (2019).\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin.",
  "Moving to\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.\nInstead of summarizing qanddinto individual embeddings, these\nrankers model word- and phrase-level relationships across qandd\nand match them using a deep neural network (e.g., with CNNs/MLPs\n[22] or kernels [ 36]). In the simplest case, they feed the neural net-\nwork an interaction matrix that re/f_lects the similiarity between\nevery pair of words across qandd. Further right, Figure 2 (c) illus-\ntrates a more powerful interaction-based paradigm, which models\nthe interactions between words within as well as across qanddat\nthe same time, as in BERT\u2019s transformer architecture [25]./T_hese increasingly expressive architectures are in tension. While\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\nperior for IR tasks [ 8,21], a representation-focused model\u2014by iso-\nlating the computations among qandd\u2014makes it possible to pre-\ncompute document representations o\ufb04ine [ 41], greatly reducing\nthe computational load per query.",
  "Following the re-ranking setup of MS MARCO,\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\nre-rank the MS MARCO\u2019s o\ufb03cial top-1000 documents per query.\n1h/t_tps://blog.google/products/search/search-language-understanding-bert/\n2h/t_tps://azure.microso/f_t.com/en-us/blog/bing-delivers-its-largest-improvement-\nin-search-experience-using-azure-gpus/arXiv:2004.12832v2  [cs.IR]  4 Jun 2020",
  "ACM Reference format:\nOmar Kha/t_tab and Matei Zaharia. 2020. ColBERT: E\ufb03cient and E\ufb00ective Pas-\nsage Search via Contextualized Late Interaction over BERT. In Proceedings\nof Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, Virtual Event, China, July 25\u201330,\n2020 (SIGIR \u201920), 10 pages.\nDOI: 10.1145/3397271.3401075\n1 INTRODUCTION\nOver the past few years, the Information Retrieval (IR) community\nhas witnessed the introduction of a host of neural ranking models,\nincluding DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrast\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\non the /f_irst page.",
  "Request permissions from permissions@acm.org.\nSIGIR \u201920, Virtual Event, China\n\u00a92020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n978-1-4503-8016-4/20/07. . . $15.00\nDOI: 10.1145/3397271.3401075\n0.15 0.20 0.25 0.30 0.35 0.40\nMRR@10101102103104105Query Latency (ms)\nBM25doc2queryKNRMDuet\nDeepCTfT+ConvKNRM\ndocTTTTTqueryBERT-baseBERT-large\nColBERT (re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) Model\nBoW Model with NLU Augmentation\nNeural Matching Model\nDeep Language Model\nColBERT (ours)Figure 1: E\ufb00ectiveness (MRR@10) versus Mean /Q_uery La-\ntency (log-scale) for a number of representative ranking\nmodels on MS MARCO Ranking [24]. /T_he /f_igure also shows\nColBERT.",
  "/T_his padded sequence of input tokens is then passed into BERT\u2019sdeep transformer architecture, which computes a contextualized\nrepresentation of each token.\nWe denote the padding with masked tokens as query augmen-\ntation , a step that allows BERT to produce query-based embeddings\nat the positions corresponding to these masks. /Q_uery augmentation\nis intended to serve as a so/f_t, di\ufb00erentiable mechanism for learning\nto expand queries with new terms or to re-weigh existing terms\nbased on their importance for matching the query. As we show in\n\u00a74.4, this operation is essential for ColBERT\u2019s e\ufb00ectiveness.\nGiven BERT\u2019s representation of each token, our encoder passes\nthe contextualized output representations through a linear layer\nwith no activations. /T_his layer serves to control the dimension\nof ColBERT\u2019s embeddings, producing m-dimensional embeddings\nfor the layer\u2019s output size m. As we discuss later in more detail,\nwe typically /f_ix mto be much smaller than BERT\u2019s /f_ixed hidden\ndimension.",
  "While highly competitive in e\ufb00ec-\ntiveness, ColBERT is orders of magnitude cheaper than BERT base,\nin particular, by over 170 \u0002in latency and 13,900 \u0002in FLOPs. /T_his\nhighlights the expressiveness of our proposed late interaction mech-\nanism, particularly when coupled with a powerful pre-trained LM\nlike BERT. While ColBERT\u2019s re-ranking latency is slightly higher\nthan the non-BERT re-ranking models shown (i.e., by 10s of mil-\nliseconds), this di\ufb00erence is explained by the time it takes to gather,\nstack, and transfer the document embeddings to the GPU. In partic-\nular, the query encoding and interaction in ColBERT consume only\n13 milliseconds of its total execution time. We note that ColBERT\u2019s\nlatency and FLOPs can be considerably reduced by padding queries\nto a shorter length, using smaller vector dimensions (the MRR@10\nof which is tested in \u00a74.5), employing quantization of the document\n6h/t_tps://github.com/mit-han-lab/torchpro/f_ile",
  "2018. From neural re-ranking to neural ranking: Learning a sparse\nrepresentation for inverted indexing. In Proceedings of the 27th ACM International\nConference on Information and Knowledge Management . ACM, 497\u2013506.\n[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval . Ph.D.\nDissertation. Carnegie Mellon University.",
  "In addition to MRR@10 and\nlatency in milliseconds, the table reports Recall@50, Recall@200,\nand Recall@1000, important metrics for a full-retrieval model that\nessentially /f_ilters down a large collection on a per-query basis.\nWe compare against BM25, in particular MS MARCO\u2019s o\ufb03cial\nBM25 ranking as well as a well-tuned baseline based on the Anserini\ntoolkit.7While many other traditional models exist, we are not\naware of any that substantially outperform Anserini\u2019s BM25 im-\nplementation (e.g., see RM3 in [ 28], LMDir in [ 2], or Microso/f_t\u2019s\nproprietary feature-based RankSVM on the leaderboard).\nWe also compare against doc2query, DeepCT, and docTTTT-\nTquery. All three rely on a traditional bag-of-words model (pri-\nmarily BM25) for retrieval. Crucially, however, they re-weigh the\nfrequency of terms per document and/or expand the set of terms\nin each document before building the BM25 index.",
  "/T_he /f_igure also shows\nColBERT. Neural re-rankers run on top of the o\ufb03cial BM25\ntop-1000 results and use a Tesla V100 GPU. Methodology and\ndetailed results are in \u00a74.\nto prior learning-to-rank methods that rely on hand-cra/f_ted fea-\ntures, these models employ embedding-based representations of\nqueries and documents and directly model local interactions (i.e.,\n/f_ine-granular relationships) between their contents. Among them,\na recent approach has emerged that /f_ine-tunes deep pre-trained\nlanguage models (LMs) like ELMo [ 29] and BERT [ 5] for estimating\nrelevance. By computing deeply-contextualized semantic repre-\nsentations of query\u2013document pairs, these LMs help bridge the\npervasive vocabulary mismatch [ 21,42] between documents and\nqueries [ 30].",
  "69 [2] 82 [2] 91 [2]\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\nColBERT L2(re-rank) 34.8 36.4 - 75.3 80.5 81.4\nColBERT L2(end-to-end) 36.0 36.7 458 82.9 92.3 96.8\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from the\nentire 8.8M document collection.\nretrieval (i.e., for methods with neural computations) but we use\nup to all four GPUs during indexing.\n4.2 /Q_uality\u2013Cost Tradeo\ufb00: Top- kRe-ranking\nIn this section, we examine ColBERT\u2019s e\ufb03ciency and e\ufb00ectiveness\nat re-ranking the top- kresults extracted by a bag-of-words retrieval\nmodel, which is the most typical se/t_ting for testing and deploying\nneural ranking models.",
  "Despite successfully reducing latency, these approaches generally\nreduce precision substantially relative to BERT.\nTo reconcile e\ufb03ciency and contextualization in IR, we propose\nColBERT , a ranking model based on contextualized late interac-\ntion over BERT . As the name suggests, ColBERT proposes a novel\nlate interaction paradigm for estimating relevance between a query\nqand a document d. Under late interaction, qanddare separately\nencoded into two sets of contextual embeddings, and relevance is\nevaluated using cheap and pruning-friendly computations between\nboth sets\u2014that is, fast computations that enable ranking without\nexhaustively evaluating every possible candidate.\nFigure 2 contrasts our proposed late interaction approach with\nexisting neural matching paradigms. On the le/f_t, Figure 2 (a) illus-\ntrates representation-focused rankers, which independently compute\nan embedding for qand another for dand estimate relevance as\na single similarity score between two vectors [ 12,41]. Moving to\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.",
  "[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\nContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).\n[4] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\nneural networks for so/f_t-matching n-grams in ad-hoc search. In Proceedings of the\neleventh ACM international conference on web search and data mining . 126\u2013134.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805 (2018).\n[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\nComplex Answer Retrieval Overview.. In TREC .\n[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro/f_t.",
  "As the /f_igure shows, BERT considerably improves search preci-\nsion, raising MRR@10 by almost 7% against the best previous meth-\nods; simultaneously, it increases latency by up to tens of thousands\nof milliseconds even with a high-end GPU. /T_his poses a challenging\ntradeo\ufb00 since raising query response times by as li/t_tle as 100ms is\nknown to impact user experience and even measurably diminish\nrevenue [ 17]. To tackle this problem, recent work has started ex-\nploring using Natural Language Understanding (NLU) techniques\nto augment traditional retrieval models like BM25 [ 32]. For exam-\nple, Nogueira et al. [26,28] expand documents with NLU-generated\nqueries before indexing with BM25 scores and Dai & Callan [ 2] re-\nplace BM25\u2019s term frequency with NLU-estimated term importance.\nDespite successfully reducing latency, these approaches generally\nreduce precision substantially relative to BERT.\nTo reconcile e\ufb03ciency and contextualization in IR, we propose\nColBERT , a ranking model based on contextualized late interac-\ntion over BERT .",
  "/T_he most e\ufb00ective among these three, docTTTTquery,\ndemonstrates a massive 9% gain over vanilla BM25 by /f_ine-tuning\nthe recent language model T5.\n7h/t_tp://anserini.io/\n8In practice, a myriad of reasons could still cause DeepCT\u2019s latency to di\ufb00er\nslightly from BM25\u2019s. For instance, the top- kpruning strategy employed, if any, could\ninteract di\ufb00erently with a changed distribution of scores.",
  "We test a model [B] that replaces\nColBERT\u2019s maximum similarity with average similarity. /T_he results\nsuggest the importance of individual terms in the query paying\nspecial a/t_tention to particular terms in the document. Similarly,\nthe /f_igure emphasizes the importance of our query augmentation\nmechanism: without query augmentation [C], ColBERT has a no-\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\nretrieval not only on recall but also on MRR@10. By retrieving\ndirectly from the full collection, ColBERT is able to retrieve to the\ntop-10 documents missed entirely from BM25\u2019s top-1000.\n0 10000 20000 30000 40000 50000\nThroughput (documents/minute)Basic ColBERT Indexing\n+multi-GPU document processing\n+per-batch maximum sequence length\n+length-based bucketing\n+multi-core pre-processingFigure 6: E\ufb00ect of ColBERT\u2019s indexing optimizations on the\no\ufb00line indexing throughput.",
  "ColBERT introduces a late interaction architecture that indepen-\ndently encodes the query and the document using BERT and then\nemploys a cheap yet powerful interaction step that models their\n/f_ine-grained similarity. By delaying and yet retaining this /f_ine-\ngranular interaction, ColBERT can leverage the expressiveness of\ndeep LMs while simultaneously gaining the ability to pre-compute\ndocument representations o\ufb04ine, considerably speeding up query\nprocessing. Beyond reducing the cost of re-ranking the documents\nretrieved by a traditional model, ColBERT\u2019s pruning-friendly in-\nteraction mechanism enables leveraging vector-similarity indexes\nfor end-to-end retrieval directly from a large document collection.\nWe extensively evaluate ColBERT using two recent passage search\ndatasets. Results show that ColBERT\u2019s e\ufb00ectiveness is competitive\nwith existing BERT-based models (and outperforms every non-\nBERT baseline), while executing two orders-of-magnitude faster\nand requiring four orders-of-magnitude fewer FLOPs per query.\nACM Reference format:\nOmar Kha/t_tab and Matei Zaharia. 2020.",
  "Finally, the output\nembeddings are normalized so each has L2 norm equal to one.\n/T_he result is that the dot-product of any two embeddings becomes\nequivalent to their cosine similarity, falling in the \u00bb\u00001;1\u00bcrange.\nDocument Encoder. Our document encoder has a very similar\narchitecture. We /f_irst segment a document dinto its constituent to-\nkens d1d2:::dm, to which we prepend BERT\u2019s start token [CLS] fol-\nlowed by our special token [D]that indicates a document sequence.\nUnlike queries, we do not append [mask] tokens to documents. Af-\nter passing this input sequence through BERT and the subsequent\nlinear layer, the document encoder /f_ilters out the embeddings corre-\nsponding to punctuation symbols, determined via a pre-de/f_ined list.\n/T_his /f_iltering is meant to reduce the number of embeddings per doc-\nument, as we hypothesize that (even contextualized) embeddings\nof punctuation are unnecessary for e\ufb00ectiveness.",
  "Fi-\nnally, we sort the kdocuments by their total scores.\nRelative to existing neural rankers (especially, but not exclu-\nsively, BERT-based ones), this computation is very cheap that, in\nfact, its cost is dominated by the cost of gathering and transferring\nthe pre-computed embeddings. To illustrate, ranking kdocuments\nvia typical BERT rankers requires feeding BERT kdi\ufb00erent inputs\neach of length l=jqj+jdijfor query qand documents di, where\na/t_tention has quadratic cost in the length of the sequence. In con-\ntrast, ColBERT feeds BERT only a single, much shorter sequence of\nlength l=jqj. Consequently, ColBERT is not only cheaper, it also\nscales much be/t_ter with kas we examine in \u00a74.2.\n3.6 End-to-end Top- kRetrieval with ColBERT\nAs mentioned before, ColBERT\u2019s late-interaction operator is speci/f_i-\ncally designed to enable end-to-end retrieval from a large collection,\nlargely to improve recall relative to term-based retrieval approaches.",
  "(3)We show how to leverage ColBERT both for re-ranking on\ntop of a term-based retrieval model ( \u00a73.5) and for searching\na full collection using vector similarity indexes ( \u00a73.6).\n(4)We evaluate ColBERT on MS MARCO and TREC CAR, two\nrecent passage search collections.\n2 RELATED WORK\nNeural Matching Models. Over the past few years, IR researchers\nhave introduced numerous neural architectures for ranking. In\nthis work, we compare against KNRM [ 4,36], Duet [ 20,22], Con-\nvKNRM [ 4], and fastText+ConvKNRM [ 10]. KNRM proposes a\ndi\ufb00erentiable kernel-pooling technique for extracting matching\nsignals from an interaction matrix, while Duet combines signals\nfrom exact-match-based as well as embedding-based similarities\nfor ranking. Introduced in 2018, ConvKNRM learns to match n-\ngrams in the query and the document. Lastly, fastText+ConvKNRM\n(abbreviated fT+ConvKNRM) tackles the absence of rare words\nfrom typical word embeddings lists by adopting sub-word token\nembeddings.",
  "Exploiting that these operations are independent across\ndocuments in a batch, we parallelize the pre-processing across the\navailable CPU cores.\nOnce the document representations are produced, they are saved\nto disk using 32-bit or 16-bit values to represent each dimension.\nAs we describe in \u00a73.5 and 3.6, these representations are either\nsimply loaded from disk for ranking or are subsequently indexed\nfor vector-similarity search, respectively.\n3.5 Top- kRe-ranking with ColBERT\nRecall that ColBERT can be used for re-ranking the output of an-\nother retrieval model, typically a term-based model, or directly\nfor end-to-end retrieval from a document collection. In this sec-\ntion, we discuss how we use ColBERT for ranking a small set of\nk(e.g., k=1000) documents given a query q. Since kis small, we\nrely on batch computations to exhaustively score each document\n3/T_he public BERT implementations we saw simply pad to a pre-de/f_ined length.(unlike our approach in \u00a73.6).",
  "vectors, and storing the embeddings on GPU if su\ufb03cient memory\nexists. We leave these directions for future work.\n0.27 0.29 0.31 0.33 0.35 0.37\nMRR@10103104105106107108109Million FLOPs (log-scale)\nk=10205010020050010002000\nk=10 20 50 100\n200500\n10002000\nBERTbase (our training)\nColBERT\nFigure 4: FLOPs (in millions) and MRR@10 as functions\nof the re-ranking depth k. Since the o\ufb03cial BM25 ranking\nis not ordered, the initial top- kretrieval is conducted with\nAnserini\u2019s BM25.\nDiving deeper into the quality\u2013cost tradeo\ufb00 between BERT and\nColBERT, Figure 4 demonstrates the relationships between FLOPs\nand e\ufb00ectiveness (MRR@10) as a function of the re-ranking depth\nkwhen re-ranking the top- kresults by BM25, comparing ColBERT\nand BERT base(our training).",
  "Even though ColBERT\u2019s late-interaction framework can be ap-\nplied to a wide variety of architectures (e.g., CNNs, RNNs, trans-\nformers, etc.), we choose to focus this work on bi-directional transformer-\nbased encoders (i.e., BERT) owing to their state-of-the-art e\ufb00ective-\nness yet very high computational cost.\n3.1 Architecture\nFigure 3 depicts the general architecture of ColBERT, which com-\nprises: (a) a query encoder fQ, (b) a document encoder fD, and (c)\nthe late interaction mechanism. Given a query qand document d,\nfQencodes qinto a bag of /f_ixed-size embeddings Eqwhile fDen-\ncodes dinto another bag Ed. Crucially, each embeddings in Eqand\nEdiscontextualized based on the other terms in qord, respectively.\nWe describe our BERT-based encoders in \u00a73.2.\nUsing EqandEd, ColBERT computes the relevance score be-\ntween qanddvia late interaction, which we de/f_ine as a summation\nof maximum similarity (MaxSim) operators.",
  "2019. Cedr:\nContextualized embeddings for document ranking. In Proceedings of the 42nd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval . ACM, 1101\u20131104.\n[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\nBe/t_ter than One?. In Advances in Neural Information Processing Systems . 14014\u2013\n14024.\n[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\nRe-ranking. arXiv preprint arXiv:1903.07666 (2019).\n[21] Bhaskar Mitra, Nick Craswell, et al .2018. An introduction to neural information\nretrieval. Foundations and Trends \u00aein Information Retrieval 13, 1 (2018), 1\u2013126.\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\nlocal and distributed representations of text for web search.",
  "3.2 /Q_uery & Document Encoders\nPrior to late interaction, ColBERT encodes each query or document\ninto a bag of embeddings, employing BERT-based encoders. We\nshare a single BERT model among our query and document en-\ncoders but distinguish input sequences that correspond to queries\nand documents by prepending a special token [Q]to queries and\nanother token [D]to documents.\n/Q_uery Encoder. Given a textual query q, we tokenize it into its\nBERT-based WordPiece [ 35] tokens q1q2:::ql. We prepend the token\n[Q]to the query. We place this token right a/f_ter BERT\u2019s sequence-\nstart token [CLS] . If the query has fewer than a pre-de/f_ined number\nof tokens Nq, we pad it with BERT\u2019s special [mask] tokens up\nto length Nq(otherwise, we truncate it to the /f_irst Nqtokens).\n/T_his padded sequence of input tokens is then passed into BERT\u2019sdeep transformer architecture, which computes a contextualized\nrepresentation of each token.",
  "/T_his\namounts to roughly 3M queries generated by concatenating the\ntitle of a Wikipedia page with the heading of one of its sections.\n/T_hat section\u2019s passages are marked as relevant to the corresponding\nquery. Our evaluation is conducted on the test set used in TREC\n2017 CAR, which contains 2,254 queries.\n4.1.2 Implementation. Our ColBERT models are implemented\nusing Python 3 and PyTorch 1. We use the popular transformers5\nlibrary for the pre-trained BERT model. Similar to [ 25], we /f_ine-tune\nall ColBERT models with learning rate 3 \u000210\u00006with a batch size\n32. We /f_ix the number of embeddings per query at Nq=32. We set\nour ColBERT embedding dimension mto be 128; \u00a74.5 demonstrates\nColBERT\u2019s robustness to a wide range of embedding dimensions.\nFor MS MARCO, we initialize the BERT components of the Col-\nBERT query and document encoders using Google\u2019s o\ufb03cial pre-\ntrained BERT basemodel.",
  "By computing deeply-contextualized semantic repre-\nsentations of query\u2013document pairs, these LMs help bridge the\npervasive vocabulary mismatch [ 21,42] between documents and\nqueries [ 30]. Indeed, in the span of just a few months, a number\nof ranking models based on BERT have achieved state-of-the-art\nresults on various retrieval benchmarks [ 3,18,25,39] and have\nbeen proprietarily adapted for deployment by Google1and Bing2.\nHowever, the remarkable gains delivered by these LMs come\nat a steep increase in computational cost. Hofst \u00a8a/t_ter et al. [9] and\nMacAvaney et al. [18] observe that BERT-based models in the lit-\nerature are 100-1000 \u0002more computationally expensive than prior\nmodels\u2014some of which are arguably notinexpensive to begin with\n[13]. /T_his quality\u2013cost tradeo\ufb00 is summarized by Figure 1, which\ncompares two BERT-based rankers [ 25,27] against a representative\nset of ranking models.",
  "/T_his /f_iltering is meant to reduce the number of embeddings per doc-\nument, as we hypothesize that (even contextualized) embeddings\nof punctuation are unnecessary for e\ufb00ectiveness.\nIn summary, given q=q0q1:::qlandd=d0d1:::dn, we compute\nthe bags of embeddings EqandEdin the following manner, where\n# refers to the [mask] tokens:\nEq:=Normalize\u00b9CNN\u00b9BERT\u00b9\u201c\u00bbQ\u00bcq0q1:::ql## :::#\u201d\u00ba\u00ba\u00ba (1)\nEd:=Filter\u00b9Normalize\u00b9CNN\u00b9BERT\u00b9\u201c\u00bbD\u00bcd0d1:::dn\u201d\u00ba\u00ba\u00ba\u00ba (2)\n3.3 Late Interaction\nGiven the representation of a query qand a document d, the rel-\nevance score of dtoq, denoted as Sq;d, is estimated via late in-\nteraction between their bags of contextualized embeddings. As\nmentioned before, this is conducted as a sum of maximum sim-\nilarity computations, namely cosine similarity (implemented as\ndot-products due to the embedding normalization) or squared L2\ndistance.",
  "For instance,\nits Recall@50 actually exceeds the o\ufb03cial BM25\u2019s Recall@1000 and\neven all but docTTTTTquery\u2019s Recall@200, emphasizing the value\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\n4.4 Ablation Studies\n0.220.240.260.280.300.320.340.36\nMRR@10BERT [CLS]-based dot-product (5-layer)  [A]\nColBERT via average similarity (5-layer)  [B]\nColBERT without query augmentation (5-layer)  [C]\nColBERT (5-layer)  [D]\nColBERT (12-layer)  [E]\nColBERT + e2e retrieval (12-layer)  [F]\nFigure 5: Ablation results on MS MARCO (Dev). Between\nbrackets is the number of BERT layers used in each model.\n/T_he results from \u00a74.2 indicate that ColBERT is highly e\ufb00ective\ndespite the low cost and simplicity of its late interaction mechanism.",
  "1129\u20131132.\n[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\nCross-domain modeling of sentence-level evidence for document retrieval. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) . 3481\u20133487.\n[40] O/f_ir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\n/Q_uantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\n[41] Hamed Zamani, Mostafa Dehghani, W Bruce Cro/f_t, Erik Learned-Miller, and\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\nrepresentation for inverted indexing. In Proceedings of the 27th ACM International\nConference on Information and Knowledge Management . ACM, 497\u2013506.\n[42] Le Zhao.",
  "For\nthe /f_irst stage, we concurrently issue Nqvector-similarity queries\n(corresponding to each of the embeddings in Eq) onto our faiss in-\ndex. /T_his retrieves the top- k0(e.g., k0=k\u009d2) matches for that vector\n4h/t_tps://github.com/facebookresearch/faiss",
  "Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\nBM25 (o\ufb03cial) 16.7 16.5 - -\nKNRM 19.8 19.8 3 592M (0.085 \u0002)\nDuet 24.3 24.5 22 159B (23 \u0002)\nfastText+ConvKNRM 29.0 27.7 28 78B (11 \u0002)\nBERT base[25] 34.7 - 10,700 97T (13,900 \u0002)\nBERT base(our training) 36.0 - 10,700 97T (13,900 \u0002)\nBERT large [25] 36.5 35.9 32,900 340T (48,600 \u0002)\nColBERT (over BERT base) 34.9 34.9 61 7B (1 \u0002)\nTable 1: \u201cRe-ranking\u201d results on MS MARCO. Each neural model re-ranks the o\ufb03cial top-1000 results produced by BM25.",
  "While this implementation supports multi-threading, it only utilizes\nparallelism across di\ufb00erent queries. We thus report single-threaded\nlatency for these models, noting that simply parallelizing their\ncomputation over shards of the index can substantially decrease\ntheir already-low latency. For DeepCT, we only estimate its latency\nusing that of BM25 (as denoted by (est.) in the table), since DeepCT\nre-weighs BM25\u2019s term frequency without modifying the index\notherwise.8As discussed in \u00a74.1, we use ColBERT L2for end-to-\nend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency, we measure the time for\nfaiss -based candidate /f_iltering and the subsequent re-ranking. In\nthis experiment, faiss uses all available CPU cores.",
  "Our extensive ablation\nstudy ( \u00a74.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERT-\nbased encoders are all essential to ColBERT\u2019s e\ufb00ectiveness.\nOur main contributions are as follows.\n(1)We propose late interaction (\u00a73.1) as a paradigm for e\ufb03cient\nand e\ufb00ective neural ranking.\n(2)We present ColBERT ( \u00a73.2 & 3.3), a highly-e\ufb00ective model\nthat employs novel BERT-based query and document en-\ncoders within the late interaction paradigm.",
  "NSF under CAREER grant CNS-1651570. Any opinions, /f_indings,\nand conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily re/f_lect the views of the\nNational Science Foundation.\nREFERENCES\n[1] Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\nto Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35th\nInternational Conference on Data Engineering (ICDE) . IEEE, 1250\u20131261.\n[2]Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\nImportance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687\n(2019).\n[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\nContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).",
  "3.4 O\ufb00line Indexing: Computing & Storing\nDocument Embeddings\nBy design, ColBERT isolates almost all of the computations between\nqueries and documents, largely to enable pre-computing document\nrepresentations o\ufb04ine. At a high level, our indexing procedure is\nstraight-forward: we proceed over the documents in the collection\nin batches, running our document encoder fDon each batch and\nstoring the output embeddings per document. Although indexing\na set of documents is an o\ufb04ine process, we incorporate a few\nsimple optimizations for enhancing the throughput of indexing. As\nwe show in \u00a74.5, these optimizations can considerably reduce the\no\ufb04ine cost of indexing.\nTo begin with, we exploit multiple GPUs, if available, for faster\nencoding of batches of documents in parallel. When batching, we\npad all documents to the maximum length of a document within\nthe batch.3To make capping the sequence length on a per-batch\nbasis more e\ufb00ective, our indexer proceeds through documents in\ngroups of B(e.g., B=100,000) documents.",
  "arXiv preprint\narXiv:1912.01385 (2019).\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013. Learning deep structured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM international conference on\nInformation & Knowledge Management . 2333\u20132338.\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. E\ufb03cient Interaction-based Neural\nRanking with Locality Sensitive Hashing. In /T_he World Wide Web Conference .\nACM, 2858\u20132864.\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand /Q_un Liu. 2019. Tinybert: Distilling bert for natural language understanding.\narXiv preprint arXiv:1909.10351 (2019).\n[15] Je\ufb00 Johnson, Ma/t_thijs Douze, and Herv \u00b4e J\u00b4egou.",
  "Note that any BERT-based model\nmust incur the computational cost of processing each document\nat least once. While ColBERT encodes each document with BERT\nexactly once, existing BERT-based rankers would repeat similar\ncomputations on possibly hundreds of documents for each query.\nSe/t_ting Dimension( m) Bytes/Dim Space(GiBs) MRR@10\nRe-rank Cosine 128 4 286 34.9\nEnd-to-end L2 128 2 154 36.0\nRe-rank L2 128 2 143 34.8\nRe-rank Cosine 48 4 54 34.4\nRe-rank Cosine 24 2 27 33.9\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\nTable 4 reports the space footprint of ColBERT under various\nse/t_tings as we reduce the embeddings dimension and/or the bytes\nper dimension.",
  "To\nimprove memory e\ufb03ciency, every embedding is divided into s(e.g.,\ns=16) sub-vectors, each represented using one byte. Moreover,\nthe index conducts the similarity computations in this compressed\ndomain, leading to cheaper computations and thus faster search.\n4 EXPERIMENTAL EVALUATION\nWe now turn our a/t_tention to empirically testing ColBERT, address-\ning the following research questions.\nRQ1: In a typical re-ranking setup, how well can ColBERT bridge\nthe existing gap (highlighted in \u00a71) between highly-e\ufb03cient and\nhighly-e\ufb00ective neural models? ( \u00a74.2)\nRQ2: Beyond re-ranking, can ColBERT e\ufb00ectively support end-\nto-end retrieval directly from a large collection? ( \u00a74.3)\nRQ3: What does each component of ColBERT (e.g., late interac-\ntion, query augmentation) contribute to its quality? ( \u00a74.4)\nRQ4: What are ColBERT\u2019s indexing-related costs in terms of\no\ufb04ine computation and memory overhead?",
  "(unlike our approach in \u00a73.6). To begin with, our query serving sub-\nsystem loads the indexed documents representations into memory,\nrepresenting each document as a matrix of embeddings.\nGiven a query q, we compute its bag of contextualized embed-\ndings Eq(Equation 1) and, concurrently, gather the document repre-\nsentations into a 3-dimensional tensor Dconsisting of kdocument\nmatrices. We pad the kdocuments to their maximum length to\nfacilitate batched operations, and move the tensor Dto the GPU\u2019s\nmemory. On the GPU, we compute a batch dot-product of Eqand\nD, possibly over multiple mini-batches. /T_he output materializes a\n3-dimensional tensor that is a collection of cross-match matrices\nbetween qand each document. To compute the score of each docu-\nment, we reduce its matrix across document terms via a max-pool\n(i.e., representing an exhaustive implementation of our MaxSim\ncomputation) and reduce across query terms via a summation. Fi-\nnally, we sort the kdocuments by their total scores.",
  "[9] Sebastian Hofst \u00a8a/t_ter and Allan Hanbury. 2019. Let\u2019s measure run time! Extending\nthe IR replicability infrastructure to include performance aspects. arXiv preprint\narXiv:1907.04614 (2019).\n[10] Sebastian Hofst \u00a8a/t_ter, Navid Rekabsaz, Carsten Eickho\ufb00, and Allan Hanbury.\n2019. On the e\ufb00ect of low-frequency terms on neural-IR models. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 1137\u20131140.\n[11] Sebastian Hofst \u00a8a/t_ter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\nTREC Deep Learning\u201919\u2013Simple Contextualization for Re-ranking. arXiv preprint\narXiv:1912.01385 (2019).\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013.",
  "While more sophisticated matching is possible with other choices\nsuch as deep convolution and a/t_tention layers (i.e., as in typical\ninteraction-focused models), a summation of maximum similarity\ncomputations has two distinctive characteristics. First, it stands\nout as a particularly cheap interaction mechanism, as we examine\nits FLOPs in \u00a74.2. Second, and more importantly, it is amenable\nto highly-e\ufb03cient pruning for top- kretrieval, as we evaluate in\n\u00a74.3. /T_his enables using vector-similarity algorithms for skipping\ndocuments without materializing the full interaction matrix or even\nconsidering each document in isolation. Other cheap choices (e.g.,\na summation of average similarity scores, instead of maximum) are\npossible; however, many are less amenable to pruning. In \u00a74.4, we\nconduct an extensive ablation study that empirically veri/f_ies the ad-\nvantage of our MaxSim-based late interaction against alternatives.\n3.2 /Q_uery & Document Encoders\nPrior to late interaction, ColBERT encodes each query or document\ninto a bag of embeddings, employing BERT-based encoders.",
  "2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings\nof the 40th International ACM SIGIR conference on research and development in\ninformation retrieval . 55\u201364.\n[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\nbaselines using Lucene. Journal of Data and Information /Q_uality (JDIQ) 10, 4\n(2018), 1\u201320.\n[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining\nthe\u201d Neural Hype\u201d Weak Baselines and the Additivity of E\ufb00ectiveness Gains\nfrom Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR\nConference on Research and Development in Information Retrieval . 1129\u20131132.\n[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\nCross-domain modeling of sentence-level evidence for document retrieval.",
  "over all document embeddings. We map each of those to its docu-\nment of origin, producing Nq\u0002k0document IDs, only K\u0014Nq\u0002k0\nof which are unique. /T_hese Kdocuments likely contain one or more\nembeddings that are highly similar to the query embeddings. For\nthe second stage, we re/f_ine this set by exhaustively re-ranking only\nthose Kdocuments in the usual manner described in \u00a73.5.\nIn our faiss -based implementation, we use an IVFPQ index (\u201cin-\nverted /f_ile with product quantization\u201d). /T_his index partitions the\nembedding space into P(e.g., P=1000) cells based on k-means clus-\ntering and then assigns each document embedding to its nearest cell\nbased on the selected vector-similarity metric. For serving queries,\nwhen searching for the top- k0matches for a single query embed-\nding, only the nearest p(e.g., p=10) partitions are searched. To\nimprove memory e\ufb03ciency, every embedding is divided into s(e.g.,\ns=16) sub-vectors, each represented using one byte.",
  "We also\nreport the re-ranking latency, which we measure using a single\nTesla V100 GPU, and the FLOPs per query for each neural ranking\nmodel. For ColBERT, our reported latency subsumes the entire\ncomputation from gathering the document representations, moving\nthem to the GPU, tokenizing then encoding the query, and applying\nlate interaction to compute document scores. For the baselines,\nwe measure the scoring computations on the GPU and exclude\nthe CPU-based text preprocessing (similar to [ 9]). In principle,\nthe baselines can pre-compute the majority of this preprocessing\n(e.g., document tokenization) o\ufb04ine and parallelize the rest acrossdocuments online, leaving only a negligible cost. We estimate the\nFLOPs per query of each model using the torchpro/f_ile6library.\nWe now proceed to study the results, which are reported in Ta-\nble 1. To begin with, we notice the fast progress from KNRM in\n2017 to the BERT-based models in 2019, manifesting itself in over\n16% increase in MRR@10.",
  "Between\nbrackets is the number of BERT layers used in each model.\n/T_he results from \u00a74.2 indicate that ColBERT is highly e\ufb00ective\ndespite the low cost and simplicity of its late interaction mechanism.\nTo be/t_ter understand the source of this e\ufb00ectiveness, we examine a\nnumber of important details in ColBERT\u2019s interaction and encoder\narchitecture. For this ablation, we report MRR@10 on the validation\nset of MS MARCO in Figure 5, which shows our main re-ranking\nColBERT model [E], with MRR@10 of 34.9%.\nDue to the cost of training all models, we train a copy of our\nmain model that retains only the /f_irst 5 layers of BERT out of 12\n(i.e., model [D]) and similarly train all our ablation models for 200k\niterations with /f_ive BERT layers. To begin with, we ask if the /f_ine-\ngranular interaction in late interaction is necessary.",
  "For instance, at k=10, BERT requires nearly\n180\u0002more FLOPs than ColBERT; at k=1000, BERT\u2019s overhead\njumps to 13,900\u0002. It then reaches 23,000 \u0002atk=2000. In fact, our\ninformal experimentation shows that this orders-of-magnitude gap\nin FLOPs makes it practical to run ColBERT entirely on the CPU,\nalthough CPU-based re-ranking lies outside our scope.\nMethod MAP MRR@10\nBM25 (Anserini) 15.3 -\ndoc2query 18.1 -\nDeepCT 24.6 33.2\nBM25 + BERT base 31.0 -\nBM25 + BERT large 33.5 -\nBM25 + ColBERT 31.3 44.3\nTable 3: Results on TREC CAR.\nHaving studied our results on MS MARCO, we now consider\nTREC CAR, whose o\ufb03cial metric is MAP.",
  "In contrast\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\non the /f_irst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR \u201920, Virtual Event, China\n\u00a92020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n978-1-4503-8016-4/20/07. . .",
  "Since /f_ine-\ntuning this model is signi/f_icantly slower than BERT base, we train\non TREC CAR for only 125k iterations.\nIn our re-ranking results, unless stated otherwise, we use 4 bytes\nper dimension in our embeddings and employ cosine as our vector-\nsimilarity function. For end-to-end ranking, we use (squared) L2\ndistance, as we found our faiss index was faster at L2-based re-\ntrieval. For our faiss index, we set the number of partitions to\nP=2,000, and search the nearest p=10 to each query embedding to\nretrieve k0=k=1000 document vectors per query embedding. We\ndivide each embedding into s=16 sub-vectors, each encoded using\none byte. To represent the index used for the second stage of our\nend-to-end retrieval procedure, we use 16-bit values per dimension.\n4.1.3 Hardware & Time Measurements.",
  "While SNRM employs sparsity to al-\nlow using inverted indexes, we relax this assumption and compare\na (dense) BERT-based representation-focused model against our\nlate-interaction ColBERT in our ablation experiments in \u00a74.4. For a\ndetailed overview of existing neural ranking models, we refer the\nreaders to two recent surveys of the literature [8, 21].\nLanguage Model Pretraining for IR. Recent work in NLU\nemphasizes the importance pre-training language representation\nmodels in an unsupervised fashion before subsequently /f_ine-tuning\nthem on downstream tasks. A notable example is BERT [ 5], a bi-\ndirectional transformer-based language model whose /f_ine-tuning\nadvanced the state of the art on various NLU benchmarks. Nogueira et\nal.[25], MacAvaney et al. [18], and Dai & Callan [ 3] investigate\nincorporating such LMs (mainly BERT, but also ELMo [ 29]) on dif-\nferent ranking datasets. As illustrated in Figure 2 (c), the common\napproach (and the one adopted by Nogueira et al.",
  "Instead of apply-\ning MaxSim between one of the query embeddings and all of one\ndocument\u2019s embeddings, we can use fast vector-similarity data\nstructures to e\ufb03ciently conduct this search between the query\nembedding and alldocument embeddings across the full collec-\ntion. For this, we employ an o\ufb00-the-shelf library for large-scale\nvector-similarity search, namely faiss [15] from Facebook.4In par-\nticular, at the end of o\ufb04ine indexing ( \u00a73.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings into faiss .\nSubsequently, when serving queries, we use a two-stage pro-\ncedure to retrieve the top- kdocuments from the entire collection.\nBoth stages rely on ColBERT\u2019s scoring: the /f_irst is an approximate\nstage aimed at /f_iltering while the second is a re/f_inement stage. For\nthe /f_irst stage, we concurrently issue Nqvector-similarity queries\n(corresponding to each of the embeddings in Eq) onto our faiss in-\ndex.",
  "/T_his layer serves to control the dimension\nof ColBERT\u2019s embeddings, producing m-dimensional embeddings\nfor the layer\u2019s output size m. As we discuss later in more detail,\nwe typically /f_ix mto be much smaller than BERT\u2019s /f_ixed hidden\ndimension.\nWhile ColBERT\u2019s embedding dimension has limited impact on\nthe e\ufb03ciency of query encoding, this step is crucial for controlling\nthe space footprint of documents, as we show in \u00a74.5. In addition, it\ncan have a signi/f_icant impact on query execution time, particularly\nthe time taken for transferring the document representations onto\nthe GPU from system memory (where they reside before processing\na query). In fact, as we show in \u00a74.2, gathering, stacking, and\ntransferring the embeddings from CPU to GPU can be the most\nexpensive step in re-ranking with ColBERT. Finally, the output\nembeddings are normalized so each has L2 norm equal to one.\n/T_he result is that the dot-product of any two embeddings becomes\nequivalent to their cosine similarity, falling in the \u00bb\u00001;1\u00bcrange.\nDocument Encoder.",
  "In addition, the collection\nincludes roughly 55k queries (with labels) that are provided as ad-\nditional validation data. We re-purpose a random sample of 5k\nqueries among those (i.e., ones not in our development or trainingsets) as a \u201clocal\u201d evaluation set. Along with the o\ufb03cial develop-\nment set, we use this held-out set for testing our models as well as\nbaselines in \u00a74.3. We do so to avoid submi/t_ting multiple variants\nof the same model at once, as the organizers discourage too many\nsubmissions by the same team.\nTREC CAR. Introduced by Dietz [ 6]et al. in 2017, TREC CAR\nis a synthetic dataset based on Wikipedia that consists of about\n29M passages. Similar to related work [ 25], we use the /f_irst four of\n/f_ive pre-de/f_ined folds for training and the /f_i/f_th for validation. /T_his\namounts to roughly 3M queries generated by concatenating the\ntitle of a Wikipedia page with the heading of one of its sections.",
  "4.5 Indexing /T_hroughput & Footprint\nLastly, we examine the indexing throughput and space footprint\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\ndocuments with ColBERT and four other ablation se/t_tings, which\nindividually enable optimizations described in \u00a73.4 on top of basic\nbatched indexing. Based on these throughputs, ColBERT can index\nMS MARCO in about three hours. Note that any BERT-based model\nmust incur the computational cost of processing each document\nat least once. While ColBERT encodes each document with BERT\nexactly once, existing BERT-based rankers would repeat similar\ncomputations on possibly hundreds of documents for each query.",
  "2019. Tinybert: Distilling bert for natural language understanding.\narXiv preprint arXiv:1909.10351 (2019).\n[15] Je\ufb00 Johnson, Ma/t_thijs Douze, and Herv \u00b4e J\u00b4egou. 2017. Billion-scale similarity\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\n2013. Online controlled experiments at large scale. In SIGKDD .\n[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\nContextualized embeddings for document ranking. In Proceedings of the 42nd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval .",
  "Query Document\nMaxSim\u2211\nMaxSim MaxSims\nQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\nDocument\n(c) All-to-all Interaction\n(e.g., BERT)(b) Query-Document Interaction\n(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction\n(i.e., the proposed ColBERT)(a) Representation-based Similarity\n(e.g., DSSM, SNRM)Query Document\ns\nQuery Document\nsFigure 2: Schematic diagrams illustrating query\u2013document matching paradigms in neural IR. /T_he /f_igure contrasts existing\napproaches (sub-/f_igures (a), (b), and (c)) with the proposed late interaction paradigm (sub-/f_igure (d)).\nOther methods, including ColBERT (full retrieval), directly retrieve\nthe top-1000 results from the entire collection.\nAs the /f_igure shows, BERT considerably improves search preci-\nsion, raising MRR@10 by almost 7% against the best previous meth-\nods; simultaneously, it increases latency by up to tens of thousands\nof milliseconds even with a high-end GPU.",
  "We conduct this experiment on MS\nMARCO (Dev). We note here that as the o\ufb03cial top-1000 ranking\ndoes not provide the BM25 order (and also lacks documents beyond\nthe top-1000 per query), the models in this experiment re-rank the\nAnserini [ 37] toolkit\u2019s BM25 output. Consequently, both MRR@10\nvalues at k=1000 are slightly higher from those reported in Table 1.\nStudying the results in Figure 4, we notice that not only is Col-\nBERT much cheaper than BERT for the same model size (i.e., 12-\nlayer \u201cbase\u201d transformer encoder), it also scales be/t_ter with the\nnumber of ranked documents. In part, this is because ColBERT\nonly needs to process the query once, irrespective of the number of\ndocuments evaluated. For instance, at k=10, BERT requires nearly\n180\u0002more FLOPs than ColBERT; at k=1000, BERT\u2019s overhead\njumps to 13,900\u0002. It then reaches 23,000 \u0002atk=2000.",
  "We now proceed to study the results, which are reported in Ta-\nble 1. To begin with, we notice the fast progress from KNRM in\n2017 to the BERT-based models in 2019, manifesting itself in over\n16% increase in MRR@10. As described in \u00a71, the simultaneous\nincrease in computational cost is di\ufb03cult to miss. Judging by their\nrather monotonic pa/t_tern of increasingly larger cost and higher ef-\nfectiveness, these results appear to paint a picture where expensive\nmodels are necessary for high-quality ranking.\nIn contrast with this trend, ColBERT (which employs late inter-\naction over BERT base) performs no worse than the original adap-\ntation of BERT basefor ranking by Nogueira and Cho [ 25,27] and\nis only marginally less e\ufb00ective than BERT large and our training\nof BERT base(described above). While highly competitive in e\ufb00ec-\ntiveness, ColBERT is orders of magnitude cheaper than BERT base,\nin particular, by over 170 \u0002in latency and 13,900 \u0002in FLOPs.",
  "( \u00a74.4)\nRQ4: What are ColBERT\u2019s indexing-related costs in terms of\no\ufb04ine computation and memory overhead? ( \u00a74.5)\n4.1 Methodology\n4.1.1 Datasets & Metrics. Similar to related work [ 2,27,28],\nwe conduct our experiments on the MS MARCO Ranking [ 24]\n(henceforth, MS MARCO) and TREC Complex Answer Retrieval\n(TREC-CAR) [ 6] datasets. Both of these recent datasets provide\nlarge training data of the scale that facilitates training and evaluat-\ning deep neural networks. We describe both in detail below.\nMS MARCO. MS MARCO is a dataset (and a corresponding\ncompetition) introduced by Microso/f_t in 2016 for reading compre-\nhension and adapted in 2018 for retrieval. It is a collection of 8.8M\npassages from Web pages, which were gathered from Bing\u2019s results\nto 1M real-world queries. Each query is associated with sparse\nrelevance judgements of one (or very few) documents marked as\nrelevant and no documents explicitly indicated as irrelevant.",
  "For MS MARCO, we initialize the BERT components of the Col-\nBERT query and document encoders using Google\u2019s o\ufb03cial pre-\ntrained BERT basemodel. Further, we train all models for 200k itera-\ntions. For TREC CAR, we follow related work [ 2,25] and use a dif-\nferent pre-trained model to the o\ufb03cial ones. To explain, the o\ufb03cial\nBERT models were pre-trained on Wikipedia, which is the source\nof TREC CAR\u2019s training and test sets. To avoid leaking test data\ninto train, Nogueira and Cho\u2019s [ 25] pre-train a randomly-initialized\nBERT model on the Wiki pages corresponding to training subset of\nTREC CAR. /T_hey release their BERT large pre-trained model, which\nwe /f_ine-tune for ColBERT\u2019s experiments on TREC CAR. Since /f_ine-\ntuning this model is signi/f_icantly slower than BERT base, we train\non TREC CAR for only 125k iterations.",
  "/T_his quality\u2013cost tradeo\ufb00 is summarized by Figure 1, which\ncompares two BERT-based rankers [ 25,27] against a representative\nset of ranking models. /T_he /f_igure uses MS MARCO Ranking [ 24],\na recent collection of 9M passages and 1M queries from Bing\u2019s\nlogs. It reports retrieval e\ufb00ectiveness (MRR@10) on the o\ufb03cial\nvalidation set as well as average query latency (log-scale) using a\nhigh-end server that dedicates one Tesla V100 GPU per query for\nneural re-rankers. Following the re-ranking setup of MS MARCO,\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\nre-rank the MS MARCO\u2019s o\ufb03cial top-1000 documents per query.",
  "We begin with the MS MARCO dataset. We\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\nsentative set of neural matching models that have been previously\ntested on MS MARCO. In addition, we compare against the nat-\nural adaptation of BERT for ranking by Nogueira and Cho [ 25],\nin particular, BERT baseand its deeper counterpart BERT large. We\nalso report results for \u201cBERT base(our training)\u201d, which is based on\nNogueira and Cho\u2019s base model (including hyperparameters) but\nis trained with the same loss function as ColBERT ( \u00a73.3) for 200k\niterations, allowing for a more direct comparison of the results.\nWe report the competition\u2019s o\ufb03cial metric, namely MRR@10,\non the validation set (Dev) and the evaluation set (Eval). We also\nreport the re-ranking latency, which we measure using a single\nTesla V100 GPU, and the FLOPs per query for each neural ranking\nmodel.",
  "/T_his includes\ndoc2query [ 28] and DeepCT [ 2]. /T_he doc2query model expands\neach document with a pre-de/f_ined number of synthetic queries\nqueries generated by a seq2seq transformer model that is trained to\ngenerate queries given a document. It then relies on a BM25 index\nfor retrieval from the (expanded) documents. DeepCT uses BERT\nto produce the term frequency component of BM25 in a context-\naware manner, essentially representing a feasible realization of the\nterm-independence assumption with neural networks [ 23]. Lastly,\ndocTTTTTquery [ 26] is identical to doc2query except that it /f_ine-\ntunes a pre-trained model (namely, T5 [ 31]) for generating the\npredicted queries.\nConcurrently with our dra/f_ting of this paper, Hofst \u00a8a/t_ter et al. [11]\npublished their Transformer-Kernel (TK) model.",
  "3.6 End-to-end Top- kRetrieval with ColBERT\nAs mentioned before, ColBERT\u2019s late-interaction operator is speci/f_i-\ncally designed to enable end-to-end retrieval from a large collection,\nlargely to improve recall relative to term-based retrieval approaches.\n/T_his section is concerned with cases where the number of docu-\nments to be ranked is too large for exhaustive evaluation of each\npossible candidate document, particularly when we are only inter-\nested in the highest scoring ones. Concretely, we focus here on\nretrieving the top- kresults directly from a large document collec-\ntion with N(e.g., N=10;000 ;000) documents, where k\u001cN.\nTo do so, we leverage the pruning-friendly nature of the MaxSim\noperations at the backbone of late interaction. Instead of apply-\ning MaxSim between one of the query embeddings and all of one\ndocument\u2019s embeddings, we can use fast vector-similarity data\nstructures to e\ufb03ciently conduct this search between the query\nembedding and alldocument embeddings across the full collec-\ntion.",
  "In addition, doing so allows using ColBERT for end-to-\nend neural retrieval directly from a large document collection. Our\nresults show that ColBERT is more than 170 \u0002faster and requires\n14,000\u0002fewer FLOPs/query than existing BERT-based models, all\nwhile only minimally impacting quality and while outperforming\nevery non-BERT baseline.\nAcknowledgments. OK was supported by the Eltoukhy Family\nGraduate Fellowship at the Stanford School of Engineering. /T_his\nresearch was supported in part by a\ufb03liate members and other\nsupporters of the Stanford DAWN project\u2014Ant Financial, Facebook,\nGoogle, Infosys, NEC, and VMware\u2014as well as Cisco, SAP, and the",
  "BERT Optimizations. As discussed in \u00a71, these LM-based\nrankers can be highly expensive in practice. While ongoing ef-\nforts in the NLU literature for distilling [ 14,33], compressing [ 40],\nand pruning [ 19] BERT can be instrumental in narrowing this gap,\nQuery Document\nQuery Encoder, fQ Document Encoder, fDMaxSim MaxSim MaxSimscore\nOffline IndexingFigure 3: /T_he general architecture of ColBERT given a query\nqand a document d.\nthey generally achieve signi/f_icantly smaller speedups than our re-\ndesigned architecture for IR, due to their generic nature, and more\naggressive optimizations o/f_ten come at the cost of lower quality.\nE\ufb03cient NLU-based Models. Recently, a direction emerged\nthat employs expensive NLU computation o\ufb04ine. /T_his includes\ndoc2query [ 28] and DeepCT [ 2]. /T_he doc2query model expands\neach document with a pre-de/f_ined number of synthetic queries\nqueries generated by a seq2seq transformer model that is trained to\ngenerate queries given a document.",
  "Each neural model re-ranks the o\ufb03cial top-1000 results produced by BM25.\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\nMethod MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\nBM25 (o\ufb03cial) 16.7 - - - - 81.4\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\nDeepCT 24.3 - 62 (est.)",
  "To begin with, we ask if the /f_ine-\ngranular interaction in late interaction is necessary. Model [A]\ntackles this question: it uses BERT to produce a single embedding\nvector for the query and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals Nq\u0002128=32\u0002128).\nRelevance is estimated as the inner product of the query\u2019s and the\ndocument\u2019s embeddings, which we found to perform be/t_ter than\ncosine similarity for single-vector re-ranking. As the results show,\nthis model is considerably less e\ufb00ective than ColBERT, reinforcing\nthe importance of late interaction.\nSubsequently, we ask if our MaxSim-based late interaction is bet-\nter than other simple alternatives. We test a model [B] that replaces\nColBERT\u2019s maximum similarity with average similarity. /T_he results\nsuggest the importance of individual terms in the query paying\nspecial a/t_tention to particular terms in the document.",
  "All three rely on a traditional bag-of-words model (pri-\nmarily BM25) for retrieval. Crucially, however, they re-weigh the\nfrequency of terms per document and/or expand the set of terms\nin each document before building the BM25 index. In particular,\ndoc2query expands each document with a pre-de/f_ined number\nof synthetic queries generated by a seq2seq transformer model\n(which docTTTTquery replaced with a pre-trained language model,\nT5 [31]). In contrast, DeepCT uses BERT to produce the term fre-\nquency component of BM25 in a context-aware manner.\nFor the latency of Anserini\u2019s BM25, doc2query, and docTTTT-\nquery, we use the authors\u2019 [ 26,28] Anserini-based implementation.\nWhile this implementation supports multi-threading, it only utilizes\nparallelism across di\ufb00erent queries. We thus report single-threaded\nlatency for these models, noting that simply parallelizing their\ncomputation over shards of the index can substantially decrease\ntheir already-low latency.",
  "Table 4 reports the space footprint of ColBERT under various\nse/t_tings as we reduce the embeddings dimension and/or the bytes\nper dimension. Interestingly, the most space-e\ufb03cient se/t_ting, that\nis, re-ranking with cosine similarity with 24-dimensional vectors\nstored as 2-byte /f_loats, is only 1% worse in MRR@10 than the most\nspace-consuming one, while the former requires only 27 GiBs to\nrepresent the MS MARCO collection.\n5 CONCLUSIONS\nIn this paper, we introduced ColBERT, a novel ranking model that\nemploys contextualized late interaction over deep LMs (in particular,\nBERT) for e\ufb03cient retrieval. By independently encoding queries\nand documents into /f_ine-grained representations that interact via\ncheap and pruning-friendly computations, ColBERT can leverage\nthe expressiveness of deep LMs while greatly speeding up query\nprocessing. In addition, doing so allows using ColBERT for end-to-\nend neural retrieval directly from a large document collection.",
  "In this work, we observe that\nthe /f_ine-grained matching of interaction-based models and the pre-\ncomputation of document representations of representation-based\nmodels can be combined by retaining yet judiciously delaying the\nquery\u2013document interaction. Figure 2 (d) illustrates an architec-\nture that precisely does so. As illustrated, every query embedding\ninteracts with all document embeddings via a MaxSim operator,\nwhich computes maximum similarity (e.g., cosine similarity), and\nthe scalar outputs of these operators are summed across query\nterms. /T_his paradigm allows ColBERT to exploit deep LM-based\nrepresentations while shi/f_ting the cost of encoding documents of-\n/f_line and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to lever-\nage vector-similarity search indexes (e.g., [ 1,15]) to retrieve the\ntop-kresults directly from a large document collection, substan-\ntially improving recall over models that only re-rank the output of\nterm-based retrieval.\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds.",
  "When batching, we\npad all documents to the maximum length of a document within\nthe batch.3To make capping the sequence length on a per-batch\nbasis more e\ufb00ective, our indexer proceeds through documents in\ngroups of B(e.g., B=100,000) documents. It sorts these documents\nby length and then feeds batches of b(e.g., b=128) documents of\ncomparable length through our encoder. /T_his length-based bucket-\ning is sometimes refered to as a BucketIterator in some libraries\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\nwe found that a non-trivial portion of the indexing time is spent on\npre-processing the text sequences, primarily BERT\u2019s WordPiece to-\nkenization. Exploiting that these operations are independent across\ndocuments in a batch, we parallelize the pre-processing across the\navailable CPU cores.\nOnce the document representations are produced, they are saved\nto disk using 32-bit or 16-bit values to represent each dimension.",
  "Introduced in 2018, ConvKNRM learns to match n-\ngrams in the query and the document. Lastly, fastText+ConvKNRM\n(abbreviated fT+ConvKNRM) tackles the absence of rare words\nfrom typical word embeddings lists by adopting sub-word token\nembeddings.\nIn 2018, Zamani et al. [41] introduced SNRM, a representation-\nfocused IR model that encodes each query and each document as\na single, sparse high-dimensional vector of \u201clatent terms\u201d. By pro-\nducing a sparse-vector representation for each document, SNRM\nis able to use a traditional IR inverted index for representing docu-\nments, allowing fast end-to-end retrieval. Despite highly promising\nresults and insights, SNRM\u2019s e\ufb00ectiveness is substantially outper-\nformed by the state of the art on the datasets with which it was\nevaluated (e.g., see [ 18,38]). While SNRM employs sparsity to al-\nlow using inverted indexes, we relax this assumption and compare\na (dense) BERT-based representation-focused model against our\nlate-interaction ColBERT in our ablation experiments in \u00a74.4.",
  "[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by /Q_uery Prediction. arXiv preprint arXiv:1904.08375 (2019).\n[29] Ma/t_thew E Peters, Mark Neumann, Mohit Iyyer, Ma/t_t Gardner, Christopher\nClark, Kenton Lee, and Luke Ze/t_tlemoyer. 2018. Deep contextualized word\nrepresentations. arXiv preprint arXiv:1802.05365 (2018).\n[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Under-\nstanding the Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531\n(2019).\n[31] Colin Ra\ufb00el, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019.",
  "ColBERT: E\ufb00icient and E\ufb00ective Passage Search via\nContextualized Late Interaction over BERT\nOmar Kha/t_tab\nStanford University\nokha/t_tab@stanford.eduMatei Zaharia\nStanford University\nmatei@cs.stanford.edu\nABSTRACT\nRecent progress in Natural Language Understanding (NLU) is driv-\ning fast-paced advances in Information Retrieval (IR), largely owed\nto /f_ine-tuning deep language models (LMs) for document ranking.\nWhile remarkably e\ufb00ective, the ranking models based on these LMs\nincrease computational cost by orders of magnitude over prior ap-\nproaches, particularly as they must feed each query\u2013document pair\nthrough a massive neural network to compute a single relevance\nscore. To tackle this, we present ColBERT, a novel ranking model\nthat adapts deep LMs (in particular, BERT) for e\ufb03cient retrieval.\nColBERT introduces a late interaction architecture that indepen-\ndently encodes the query and the document using BERT and then\nemploys a cheap yet powerful interaction step that models their\n/f_ine-grained similarity.",
  "As illustrated in Figure 2 (c), the common\napproach (and the one adopted by Nogueira et al. on MS MARCO\nand TREC CAR) is to feed the query\u2013document pair through BERT\nand use an MLP on top of BERT\u2019s [CLS] output token to produce a\nrelevance score. Subsequent work by Nogueira et al. [27] introduced\nduoBERT, which /f_ine-tunes BERT to compare the relevance of a\npair of documents given a query. Relative to their single-document\nBERT, this gives duoBERT a 1% MRR@10 advantage on MS MARCO\nwhile increasing the cost by at least 1.4 \u0002.\nBERT Optimizations. As discussed in \u00a71, these LM-based\nrankers can be highly expensive in practice.",
  "For its latency, we measure the time for\nfaiss -based candidate /f_iltering and the subsequent re-ranking. In\nthis experiment, faiss uses all available CPU cores.\nLooking at Table 2, we /f_irst see Anserini\u2019s BM25 baseline at 18.7\nMRR@10, noticing its very low latency as implemented in Anserini\n(which extends the well-known Lucene system), owing to both\nvery cheap operations and decades of bag-of-words top- kretrieval\noptimizations. /T_he three subsequent baselines, namely doc2query,\nDeepCT, and docTTTTquery, each brings a decisive enhancement\nto e\ufb00ectiveness. /T_hese improvements come at negligible overheads\nin latency, since these baselines ultimately rely on BM25-based\nretrieval. /T_he most e\ufb00ective among these three, docTTTTquery,\ndemonstrates a massive 9% gain over vanilla BM25 by /f_ine-tuning\nthe recent language model T5.",
  "3 COLBERT\nColBERT prescribes a simple framework for balancing the quality\nand cost of neural IR, particularly deep language models like BERT.\nAs introduced earlier, delaying the query\u2013document interaction can\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\nand even support practical end-to-end neural retrieval (i.e., through\npruning via vector-similarity search). ColBERT addresses how to\ndo so while still preserving the e\ufb00ectiveness of state-of-the-art\nmodels, which condition the bulk of their computations on the\njoint query\u2013document pair.",
  "We describe our BERT-based encoders in \u00a73.2.\nUsing EqandEd, ColBERT computes the relevance score be-\ntween qanddvia late interaction, which we de/f_ine as a summation\nof maximum similarity (MaxSim) operators. In particular, we /f_ind\nthe maximum cosine similarity of each /v.alt2Eqwith vectors in Ed,\nand combine the outputs via summation. Besides cosine, we also\nevaluate squared L2 distance as a measure of vector similarity. In-\ntuitively, this interaction mechanism so/f_tly searches for each query\nterm tq\u2014in a manner that re/f_lects its context in the query\u2014against\nthe document\u2019s embeddings, quantifying the strength of the \u201cmatch\u201d\nvia the largest similarity score between tqand a document term td.\nGiven these term scores, it then estimates the document relevance\nby summing the matching evidence across all query terms.\nWhile more sophisticated matching is possible with other choices\nsuch as deep convolution and a/t_tention layers (i.e., as in typical\ninteraction-focused models), a summation of maximum similarity\ncomputations has two distinctive characteristics.",
  "Having studied our results on MS MARCO, we now consider\nTREC CAR, whose o\ufb03cial metric is MAP. Results are summarized\nin Table 3, which includes a number of important baselines (BM25,\ndoc2query, and DeepCT) in addition to re-ranking baselines thathave been tested on this dataset. /T_hese results directly mirror those\nwith MS MARCO.\n4.3 End-to-end Top- kRetrieval\nBeyond cheap re-ranking, ColBERT is amenable to top- kretrieval di-\nrectly from a full collection. Table 2 considers full retrieval, wherein\neach model retrieves the top-1000 documents directly from MS\nMARCO\u2019s 8.8M documents per query. In addition to MRR@10 and\nlatency in milliseconds, the table reports Recall@50, Recall@200,\nand Recall@1000, important metrics for a full-retrieval model that\nessentially /f_ilters down a large collection on a per-query basis.",
  "Sq;d:=\u00d5\ni2\u00bbjEqj\u00bcmax\nj2\u00bbjEdj\u00bcEqi\u0001ET\ndj(3)\nColBERT is di\ufb00erentiable end-to-end. We /f_ine-tune the BERT\nencoders and train from scratch the additional parameters (i.e., the\nlinear layer and the [Q] and [D] markers\u2019 embeddings) using the\nAdam [ 16] optimizer. Notice that our interaction mechanism has\nno trainable parameters. Given a triple hq;d+;d\u0000iwith query q,\npositive document d+and negative document d\u0000, ColBERT is used\nto produce a score for each document individually and is optimized\nvia pairwise so/f_tmax cross-entropy loss over the computed scores\nofd+andd\u0000.\n3.4 O\ufb00line Indexing: Computing & Storing\nDocument Embeddings\nBy design, ColBERT isolates almost all of the computations between\nqueries and documents, largely to enable pre-computing document\nrepresentations o\ufb04ine.",
  "2016. MS MARCO: A Human-Generated MAchine\nReading COmprehension Dataset. (2016).[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085 (2019).\n[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\ndocTTTTTquery. (2019).\n[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\nDocument Ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).\n[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by /Q_uery Prediction. arXiv preprint arXiv:1904.08375 (2019).",
  "Concurrently with our dra/f_ting of this paper, Hofst \u00a8a/t_ter et al. [11]\npublished their Transformer-Kernel (TK) model. At a high level, TK\nimproves the KNRM architecture described earlier: while KNRM\nemploys kernel pooling on top of word-embedding-based inter-\naction, TK uses a Transformer [ 34] component for contextually\nencoding queries and documents before kernel pooling. TK estab-\nlishes a new state-of-the-art for non-BERT models on MS MARCO\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\nwhile ColBERT reaches up to 36%. Moreover, due to indexing docu-\nment representations o\ufb04ine and employing a MaxSim-based late\ninteraction mechanism, ColBERT is much more scalable, enabling\nend-to-end retrieval which is not supported by TK.\n3 COLBERT\nColBERT prescribes a simple framework for balancing the quality\nand cost of neural IR, particularly deep language models like BERT.",
  "Each query is associated with sparse\nrelevance judgements of one (or very few) documents marked as\nrelevant and no documents explicitly indicated as irrelevant. Per\nthe o\ufb03cial evaluation, we use MRR@10 to measure e\ufb00ectiveness.\nWe use three sets of queries for evaluation. /T_he o\ufb03cial devel-\nopment and evaluation sets contain roughly 7k queries. However,\nthe relevance judgements of the evaluation set are held-out by Mi-\ncroso/f_t and e\ufb00ectiveness results can only be obtained by submi/t_ting\nto the competition\u2019s organizers. We submi/t_ted our main re-ranking\nColBERT model for the results in \u00a74.2. In addition, the collection\nincludes roughly 55k queries (with labels) that are provided as ad-\nditional validation data. We re-purpose a random sample of 5k\nqueries among those (i.e., ones not in our development or trainingsets) as a \u201clocal\u201d evaluation set.",
  "[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. 2017. A/t_tention is all\nyou need. In Advances in neural information processing systems . 5998\u20136008.\n[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, /Q_uoc V Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al .\n2016. Google\u2019s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.\n2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings\nof the 40th International ACM SIGIR conference on research and development in\ninformation retrieval . 55\u201364.",
  "As Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds. For instance, when used for re-\nranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170 \u0002speedup\n(and requires 14,000 \u0002fewer FLOPs) relative to existing BERT-based\nmodels, while being more e\ufb00ective than every non-BERT baseline\n(\u00a74.2 & 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feed\ndocuments through BERT\u2014is also practical: it can index the MS\nMARCO collection of 9M passages in about 3 hours using a single\nserver with four GPUs ( \u00a74.5), retaining its e\ufb00ectiveness with a space\nfootprint of as li/t_tle as few tens of GiBs. Our extensive ablation\nstudy ( \u00a74.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERT-\nbased encoders are all essential to ColBERT\u2019s e\ufb00ectiveness.\nOur main contributions are as follows."
]