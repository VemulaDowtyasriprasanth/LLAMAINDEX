{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidoc Autoretrieval Pack\n",
    "\n",
    "This is the LlamaPack version of our structured hierarchical retrieval guide in the [core repo](https://docs.llamaindex.ai/en/stable/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll load in LlamaIndex Github issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GITHUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 issues in the repo page 1\n",
      "Resulted in 100 documents\n",
      "Found 100 issues in the repo page 2\n",
      "Resulted in 200 documents\n",
      "Found 100 issues in the repo page 3\n",
      "Resulted in 300 documents\n",
      "Found 7 issues in the repo page 4\n",
      "Resulted in 307 documents\n",
      "No more issues found, stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_hub.github_repo_issues import (\n",
    "    GitHubRepositoryIssuesReader,\n",
    "    GitHubIssuesClient,\n",
    ")\n",
    "\n",
    "github_client = GitHubIssuesClient()\n",
    "loader = GitHubRepositoryIssuesReader(\n",
    "    github_client,\n",
    "    owner=\"run-llama\",\n",
    "    repo=\"llama_index\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "orig_docs = loader.load_data()\n",
    "\n",
    "# limit = 100\n",
    "limit = 10\n",
    "\n",
    "docs = []\n",
    "for idx, doc in enumerate(orig_docs):\n",
    "    doc.metadata[\"index_id\"] = doc.id_\n",
    "    if idx >= limit:\n",
    "        break\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from llama_index import SummaryIndex, Document, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.async_utils import run_jobs\n",
    "\n",
    "\n",
    "async def aprocess_doc(doc, include_summary: bool = True):\n",
    "    \"\"\"Process doc.\"\"\"\n",
    "    print(f\"Processing {doc.id_}\")\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    date_tokens = metadata[\"created_at\"].split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_tokens[0])\n",
    "    month = int(date_tokens[1])\n",
    "    day = int(date_tokens[2])\n",
    "\n",
    "    assignee = (\n",
    "        \"\" if \"assignee\" not in doc.metadata else doc.metadata[\"assignee\"]\n",
    "    )\n",
    "    size = \"\"\n",
    "    if len(doc.metadata[\"labels\"]) > 0:\n",
    "        size_arr = [l for l in doc.metadata[\"labels\"] if \"size:\" in l]\n",
    "        size = size_arr[0].split(\":\")[1] if len(size_arr) > 0 else \"\"\n",
    "    new_metadata = {\n",
    "        \"state\": metadata[\"state\"],\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"assignee\": assignee,\n",
    "        \"size\": size,\n",
    "        \"index_id\": doc.id_,\n",
    "    }\n",
    "\n",
    "    # now extract out summary\n",
    "    summary_index = SummaryIndex.from_documents([doc])\n",
    "    query_str = \"Give a one-sentence concise summary of this issue.\"\n",
    "    query_engine = summary_index.as_query_engine(\n",
    "        service_context=ServiceContext.from_defaults(\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    "        )\n",
    "    )\n",
    "    summary_txt = str(query_engine.query(query_str))\n",
    "\n",
    "    new_doc = Document(text=summary_txt, metadata=new_metadata)\n",
    "    return new_doc\n",
    "\n",
    "\n",
    "async def aprocess_docs(docs):\n",
    "    \"\"\"Process metadata on docs.\"\"\"\n",
    "\n",
    "    new_docs = []\n",
    "    tasks = []\n",
    "    for doc in docs:\n",
    "        task = aprocess_doc(doc)\n",
    "        tasks.append(task)\n",
    "\n",
    "    new_docs = await run_jobs(tasks, show_progress=True, workers=5)\n",
    "\n",
    "    # new_docs = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9488\n",
      "Processing 9421\n",
      "Processing 9665\n",
      "Processing 9343\n",
      "Processing 7726\n",
      "Processing 9658\n",
      "Processing 9655\n",
      "Processing 9653\n",
      "Processing 9652\n",
      "Processing 9664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "new_docs = await aprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'year': 2023,\n",
       " 'month': 12,\n",
       " 'day': 21,\n",
       " 'assignee': '',\n",
       " 'size': '',\n",
       " 'index_id': '9653'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs[5].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weaviate Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "# cloud\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"RR3SptbaO2l5Xqb2GbEZtUKXOVRcrDEYhAHw\")\n",
    "client = weaviate.Client(\n",
    "    \"https://jerry-cluster-gk9v5ken.weaviate.network\",\n",
    "    auth_client_secret=auth_config,\n",
    ")\n",
    "\n",
    "doc_metadata_index_name = \"LlamaIndex_auto\"\n",
    "doc_chunks_index_name = \"LlamaIndex_AutoDoc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: delete schema\n",
    "client.schema.delete_class(doc_metadata_index_name)\n",
    "client.schema.delete_class(doc_chunks_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Metadata Schema\n",
    "\n",
    "This is required for autoretrieval; we put this in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Github Issues\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"state\",\n",
    "            description=\"Whether the issue is `open` or `closed`\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"year\",\n",
    "            description=\"The year issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"month\",\n",
    "            description=\"The month issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"day\",\n",
    "            description=\"The day issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"assignee\",\n",
    "            description=\"The assignee of the ticket\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"size\",\n",
    "            description=\"How big the issue is (XS, S, M, L, XL, XXL)\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LlamaPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llama_pack import download_llama_pack\n",
    "\n",
    "# MultiDocAutoRetrieverPack = download_llama_pack(\n",
    "#     \"MultiDocAutoRetrieverPack\", \n",
    "#     \"./multidoc_autoretriever_pack\",\n",
    "#     llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_multi_doc_autoretrieval_pack/llama_hub\"\n",
    "# )\n",
    "\n",
    "from llama_hub.llama_packs.multidoc_autoretrieval.base import MultiDocAutoRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed metadata nodes.\n",
      "Indexed source document nodes.\n",
      "Setup autoretriever over metadata.\n",
      "Setup per-document retriever.\n",
      "Setup recursive retriever.\n"
     ]
    }
   ],
   "source": [
    "pack = MultiDocAutoRetrieverPack(\n",
    "    client,\n",
    "    doc_metadata_index_name,\n",
    "    doc_chunks_index_name,\n",
    "    new_docs,\n",
    "    docs,\n",
    "    vector_store_info,\n",
    "    auto_retriever_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"similarity_top_k\": 2,\n",
    "        \"empty_query_top_k\": 10,\n",
    "    },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LlamaPack\n",
    "\n",
    "Now let's try the LlamaPack on some queries! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiDocAutoRetrieverPack' object has no attribute '_query_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about some issues on 12/11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "File \u001b[0;32m~/Programming/llama-hub/llama_hub/llama_packs/multidoc_autoretrieval/base.py:187\u001b[0m, in \u001b[0;36mMultiDocAutoRetrieverPack.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    Runs queries against the index.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        Any: A response from the query engine.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_engine\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiDocAutoRetrieverPack' object has no attribute '_query_engine'"
     ]
    }
   ],
   "source": [
    "response = pack.run(\"Tell me about some issues on 12/11\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pack.run(\n",
    "    \"Tell me about some open issues related to agents\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever-only\n",
    "\n",
    "We can also get the retriever module and just run that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Tell me about some open issues related to agents\n",
      "\u001b[0mUsing query str: agents\n",
      "Using filters: [('state', '==', 'open')]\n",
      "\u001b[1;3;38;5;200mRetrieved node with id, entering: 9653\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9653: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Feature Request]: Add multi-filter single key solution\n",
      "### Feature Description\n",
      "\n",
      "This is following up on what the bot suggested in this ticket: [https://github.com/run-llama/llama_index/issues/9627](https://github.com/run-llama/llama_index/issues/9627).\n",
      "\n",
      "I need functionality for multi-filter, single-key with chroma in particular. Something like this, for example:\n",
      "key=\"Month\", value=[\"September\", \"October\"] with an OR filter condition and an IN operator\n",
      "As far as I understand, this is not currently supported.\n",
      "\n",
      "I have the following working solution and am hoping this or something similar could be merged into the repo:\n",
      "\n",
      "1) vector_stores/types.py\n",
      "\n",
      "```\n",
      "class MetadataFilter(BaseModel):\n",
      "    key: str\n",
      "    value: Union[StrictInt, StrictFloat, StrictStr, List[Union[StrictInt, StrictFloat, StrictStr]]]\n",
      "    operator: FilterOperator = FilterOperator.EQ\n",
      "```\n",
      "\n",
      "2) vector_stores/chroma.py\n",
      "```\n",
      "def _transform_chroma_filter_operator(operator: str) -> str:\n",
      "    \"\"\"Translate standard metadata filter operator to Chroma specific spec.\"\"\"\n",
      "    if operator == \"!=\":\n",
      "        return \"$ne\"\n",
      "    elif operator == \"==\":\n",
      "        return \"$eq\"\n",
      "    elif operator == \">\":\n",
      "        return \"$gt\"\n",
      "    elif operator == \"<\":\n",
      "        return \"$lt\"\n",
      "    elif operator == \">=\":\n",
      "        return \"$gte\"\n",
      "    elif operator == \"<=\":\n",
      "        return \"$lte\"\n",
      "    elif operator == \"in\":\n",
      "        return \"$in\"\n",
      "    else:\n",
      "        raise ValueError(f\"Filter operator {operator} not supported\")\n",
      "```\n",
      "\n",
      "\n",
      "3) vector_stores/chroma.py\n",
      "```\n",
      "\n",
      "def _to_chroma_filter(\n",
      "    standard_filters: MetadataFilters,\n",
      ") -> dict:\n",
      "    \"\"\"Translate standard metadata filters to Chroma specific spec.\"\"\"\n",
      "    filters = {}\n",
      "    filters_list = []\n",
      "    condition = standard_filters.condition or \"and\"\n",
      "    condition = _transform_chroma_filter_condition(condition)\n",
      "\n",
      "    if standard_filters.filters:\n",
      "        for filter in standard_filters.filters:\n",
      "            if filter.operator:\n",
      "                operator = _transform_chroma_filter_operator(filter.operator)\n",
      "                # Handle list values for 'in' operator\n",
      "                if filter.operator == FilterOperator.IN and isinstance(filter.value, list):\n",
      "                    filters_list.append({filter.key: {\"$in\": filter.value}})\n",
      "                else:\n",
      "                    filters_list.append({filter.key: {operator: filter.value}})\n",
      "            else:\n",
      "                # Assuming default behavior for filters without an explicit operator\n",
      "                filters_list.append({filter.key: filter.value})\n",
      "\n",
      "    if len(filters_list) == 1:\n",
      "        # If there is only one filter, return it directly\n",
      "        return filters_list[0]\n",
      "    elif len(filters_list) > 1:\n",
      "        # Combine multiple filters based on the specified condition\n",
      "        filters[condition] = filters_list\n",
      "\n",
      "    return filters\n",
      "```\n",
      "\n",
      "\n",
      "And then this gets initialized like this:\n",
      "\n",
      "```\n",
      "\n",
      "metadata_filters = MetadataFilters(\n",
      "    filters=[\n",
      "        MetadataFilter(key=\"Month\", value=[\"September\", \"October\"], operator=FilterOperator.IN)\n",
      "    ],\n",
      "    condition=FilterCondition.OR\n",
      ")\n",
      "```\n",
      "\n",
      "The functionality this doesn't support yet is something like this:\n",
      "key=\"Day\", value=[7, 14] with an AND filter and operators [GTE, LTE].\n",
      "\n",
      "Would love to have that functionality as well and then need to extend it to the other vector stores.\n",
      "\n",
      "Thanks again\n",
      "\n",
      "### Reason\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Value of Feature\n",
      "\n",
      "_No response_\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 9655\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9655: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Add new relational node parse for markdown\n",
      "# Description\n",
      "\n",
      "Add a new relational node parser for markdown working as similarly as possible to the unstructured_node_element node parser, so that we can unify/generalize  the approach in the future.\n",
      "\n",
      "## Type of Change\n",
      "\n",
      "Please delete options that are not relevant.\n",
      "\n",
      "- [X] New feature (non-breaking change which adds functionality)\n",
      "- [ ] This change requires a documentation update ?\n",
      "\n",
      "# How Has This Been Tested?\n",
      "\n",
      "Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\n",
      "\n",
      "- [X] Added new unit/integration tests\n",
      "- [ ] Added new notebook (that tests end-to-end)\n",
      "- [X] I stared at the code and made sure it makes sense\n",
      "\n",
      "# Suggested Checklist:\n",
      "\n",
      "- [x] I have performed a self-review of my own code\n",
      "- [X] I have commented my code, particularly in hard-to-understand areas\n",
      "- [ ] I have made corresponding changes to the documentation\n",
      "- [ ] I have added Google Colab support for the newly added notebooks.\n",
      "- [X] My changes generate no new warnings\n",
      "- [X] I have added tests that prove my fix is effective or that my feature works\n",
      "- [X] New and existing unit tests pass locally with my changes\n",
      "- [X] I ran `make format; make lint` to appease the lint gods\n",
      "\u001b[0mNumber of source nodes: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'created_at': '2023-12-21T15:55:48Z',\n",
       " 'url': 'https://api.github.com/repos/run-llama/llama_index/issues/9653',\n",
       " 'source': 'https://github.com/run-llama/llama_index/issues/9653',\n",
       " 'labels': ['enhancement', 'triage'],\n",
       " 'index_id': '9653'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = pack.get_modules()[\"recursive_retriever\"]\n",
    "nodes = retriever.retrieve(\"Tell me about some open issues related to agents\")\n",
    "print(f\"Number of source nodes: {len(nodes)}\")\n",
    "nodes[0].node.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hub",
   "language": "python",
   "name": "llama_hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
