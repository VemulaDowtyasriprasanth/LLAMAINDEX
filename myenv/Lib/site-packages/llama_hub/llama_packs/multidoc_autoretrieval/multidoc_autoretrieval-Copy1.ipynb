{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidoc Autoretrieval Pack\n",
    "\n",
    "This is the LlamaPack version of our structured hierarchical retrieval guide in the [core repo](https://docs.llamaindex.ai/en/stable/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll load in LlamaIndex Github issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\n",
    "    \"GITHUB_TOKEN\"\n",
    "] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 issues in the repo page 1\n",
      "Resulted in 100 documents\n",
      "Found 100 issues in the repo page 2\n",
      "Resulted in 200 documents\n",
      "Found 100 issues in the repo page 3\n",
      "Resulted in 300 documents\n",
      "Found 9 issues in the repo page 4\n",
      "Resulted in 309 documents\n",
      "No more issues found, stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_hub.github_repo_issues import (\n",
    "    GitHubRepositoryIssuesReader,\n",
    "    GitHubIssuesClient,\n",
    ")\n",
    "\n",
    "github_client = GitHubIssuesClient()\n",
    "loader = GitHubRepositoryIssuesReader(\n",
    "    github_client,\n",
    "    owner=\"run-llama\",\n",
    "    repo=\"llama_index\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "orig_docs = loader.load_data()\n",
    "\n",
    "limit = 100\n",
    "# limit = 10\n",
    "\n",
    "docs = []\n",
    "for idx, doc in enumerate(orig_docs):\n",
    "    doc.metadata[\"index_id\"] = doc.id_\n",
    "    if idx >= limit:\n",
    "        break\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from llama_index import SummaryIndex, Document, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.async_utils import run_jobs\n",
    "\n",
    "\n",
    "async def aprocess_doc(doc, include_summary: bool = True):\n",
    "    \"\"\"Process doc.\"\"\"\n",
    "    print(f\"Processing {doc.id_}\")\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    date_tokens = metadata[\"created_at\"].split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_tokens[0])\n",
    "    month = int(date_tokens[1])\n",
    "    day = int(date_tokens[2])\n",
    "\n",
    "    assignee = \"\" if \"assignee\" not in doc.metadata else doc.metadata[\"assignee\"]\n",
    "    size = \"\"\n",
    "    if len(doc.metadata[\"labels\"]) > 0:\n",
    "        size_arr = [l for l in doc.metadata[\"labels\"] if \"size:\" in l]\n",
    "        size = size_arr[0].split(\":\")[1] if len(size_arr) > 0 else \"\"\n",
    "    new_metadata = {\n",
    "        \"state\": metadata[\"state\"],\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"assignee\": assignee,\n",
    "        \"size\": size,\n",
    "        \"index_id\": doc.id_,\n",
    "    }\n",
    "\n",
    "    # now extract out summary\n",
    "    summary_index = SummaryIndex.from_documents([doc])\n",
    "    query_str = \"Give a one-sentence concise summary of this issue.\"\n",
    "    query_engine = summary_index.as_query_engine(\n",
    "        service_context=ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\"))\n",
    "    )\n",
    "    summary_txt = str(query_engine.query(query_str))\n",
    "\n",
    "    new_doc = Document(text=summary_txt, metadata=new_metadata)\n",
    "    return new_doc\n",
    "\n",
    "\n",
    "async def aprocess_docs(docs):\n",
    "    \"\"\"Process metadata on docs.\"\"\"\n",
    "\n",
    "    new_docs = []\n",
    "    tasks = []\n",
    "    for doc in docs:\n",
    "        task = aprocess_doc(doc)\n",
    "        tasks.append(task)\n",
    "\n",
    "    new_docs = await run_jobs(tasks, show_progress=True, workers=5)\n",
    "\n",
    "    # new_docs = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9244\n",
      "Processing 9417\n",
      "Processing 9618\n",
      "Processing 9491\n",
      "Processing 9408\n",
      "Processing 9611\n",
      "Processing 9627\n",
      "Processing 9372\n",
      "Processing 9623\n",
      "Processing 9415\n",
      "Processing 9620\n",
      "Processing 9414\n",
      "Processing 9097\n",
      "Processing 9525\n",
      "Processing 9339\n",
      "Processing 9427\n",
      "Processing 9398\n",
      "Processing 9613\n",
      "Processing 9353\n",
      "Processing 9612\n",
      "Processing 8832\n",
      "Processing 9348\n",
      "Processing 9609\n",
      "Processing 9604\n",
      "Processing 7457\n",
      "Processing 9426\n",
      "Processing 9383\n",
      "Processing 9664\n",
      "Processing 9425\n",
      "Processing 9419\n",
      "Processing 9405\n",
      "Processing 9684\n",
      "Processing 9373\n",
      "Processing 9546\n",
      "Processing 9565\n",
      "Processing 9488\n",
      "Processing 9560\n",
      "Processing 9269\n",
      "Processing 8802\n",
      "Processing 9510\n",
      "Processing 9343\n",
      "Processing 9523\n",
      "Processing 9416\n",
      "Processing 9421\n",
      "Processing 9522\n",
      "Processing 9653\n",
      "Processing 9520\n",
      "Processing 9435\n",
      "Processing 9571\n",
      "Processing 9358\n",
      "Processing 9385\n",
      "Processing 9685\n",
      "Processing 9380\n",
      "Processing 9352\n",
      "Processing 9477\n",
      "Processing 9626\n",
      "Processing 9368\n",
      "Processing 8893\n",
      "Processing 9543\n",
      "Processing 9638\n",
      "Processing 9312\n",
      "Processing 8551\n",
      "Processing 9219\n",
      "Processing 9470\n",
      "Processing 7744\n",
      "Processing 9518\n",
      "Processing 8536\n",
      "Processing 9679\n",
      "Processing 9658\n",
      "Processing 7244\n",
      "Processing 9678\n",
      "Processing 1239\n",
      "Processing 9519\n",
      "Processing 9670\n",
      "Processing 9492\n",
      "Processing 9509\n",
      "Processing 9164\n",
      "Processing 9507\n",
      "Processing 9490\n",
      "Processing 9483\n",
      "Processing 7726\n",
      "Processing 9481\n",
      "Processing 9652\n",
      "Processing 9469\n",
      "Processing 9651\n",
      "Processing 9650\n",
      "Processing 9475\n",
      "Processing 9649\n",
      "Processing 9603\n",
      "Processing 9471\n",
      "Processing 9625\n",
      "Processing 9472\n",
      "Processing 9450\n",
      "Processing 9583\n",
      "Processing 9394\n",
      "Processing 9439\n",
      "Processing 7720\n",
      "Processing 9413\n",
      "Processing 9431\n",
      "Processing 8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 100/100 [01:48<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "new_docs = await aprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'year': 2023,\n",
       " 'month': 12,\n",
       " 'day': 21,\n",
       " 'assignee': '',\n",
       " 'size': 'L',\n",
       " 'index_id': '9658'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs[5].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weaviate Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "# cloud\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"RR3SptbaO2l5Xqb2GbEZtUKXOVRcrDEYhAHw\")\n",
    "client = weaviate.Client(\n",
    "    \"https://jerry-cluster-gk9v5ken.weaviate.network\",\n",
    "    auth_client_secret=auth_config,\n",
    ")\n",
    "\n",
    "doc_metadata_index_name = \"LlamaIndex_auto\"\n",
    "doc_chunks_index_name = \"LlamaIndex_AutoDoc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: delete schema\n",
    "client.schema.delete_class(doc_metadata_index_name)\n",
    "client.schema.delete_class(doc_chunks_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Metadata Schema\n",
    "\n",
    "This is required for autoretrieval; we put this in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Github Issues\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"state\",\n",
    "            description=\"Whether the issue is `open` or `closed`\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"year\",\n",
    "            description=\"The year issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"month\",\n",
    "            description=\"The month issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"day\",\n",
    "            description=\"The day issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"assignee\",\n",
    "            description=\"The assignee of the ticket\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"size\",\n",
    "            description=\"How big the issue is (XS, S, M, L, XL, XXL)\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LlamaPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llama_pack import download_llama_pack\n",
    "\n",
    "# MultiDocAutoRetrieverPack = download_llama_pack(\n",
    "#     \"MultiDocAutoRetrieverPack\",\n",
    "#     \"./multidoc_autoretriever_pack\",\n",
    "#     llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_multi_doc_autoretrieval_pack/llama_hub\"\n",
    "# )\n",
    "\n",
    "from llama_hub.llama_packs.multidoc_autoretrieval.base import MultiDocAutoRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed metadata nodes.\n",
      "Indexed source document nodes.\n",
      "Setup autoretriever over metadata.\n",
      "Setup per-document retriever.\n",
      "Setup recursive retriever.\n"
     ]
    }
   ],
   "source": [
    "pack = MultiDocAutoRetrieverPack(\n",
    "    client,\n",
    "    doc_metadata_index_name,\n",
    "    doc_chunks_index_name,\n",
    "    new_docs,\n",
    "    docs,\n",
    "    vector_store_info,\n",
    "    auto_retriever_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"similarity_top_k\": 2,\n",
    "        \"empty_query_top_k\": 10,\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LlamaPack\n",
    "\n",
    "Now let's try the LlamaPack on some queries! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Tell me about some issues on 12/11\n",
      "\u001b[0mUsing query str: issues\n",
      "Using filters: [('month', '==', 12), ('day', '==', 11)]\n",
      "\u001b[1;3;38;5;200mRetrieved node with id, entering: 9425\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9425: Tell me about some issues on 12/11\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Feature Request]: Make llama-index compartible with models finetuned and hosted on modal.com\n",
      "### Feature Description\n",
      "\n",
      "Modal.com is a cloud computing service that allows you to finetune and host models on their workers. They provide inference points for any models finetuned on their platform.\n",
      "\n",
      "### Reason\n",
      "\n",
      "I have not tried implementing the feature. I just read about the capabilities on modal.com and thought it would be a good integration feature for llama-index to allow for more configuration.\n",
      "\n",
      "### Value of Feature\n",
      "\n",
      "An integration feature to allow users who host their models on modal to use llama-index for their RAG and prompt engineering pipelines.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 9439\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9439: Tell me about some issues on 12/11\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Bug]: Metadata filter not working with Elastic search indexing \n",
      "### Bug Description\n",
      "\n",
      "While retrieving from ES with multiple metadatafilter condition(OR/AND) its not taking it into account. It always performs an AND operation even if its explicitly mentioned OR.\n",
      "Example below code should filter and retrieve only 'mafia' or \"Stephen King\" bit its not doing as expected.\n",
      "\n",
      "filters = MetadataFilters(\n",
      "    filters=[\n",
      "        MetadataFilter(key=\"theme\", value=\"Mafia\"),\n",
      "        MetadataFilter(key=\"author\", value=\"Stephen King\"),\n",
      "    ],\n",
      "    condition=FilterCondition.OR,\n",
      ")\n",
      "\n",
      "retriever = index.as_retriever(filters=filters)\n",
      "\n",
      "### Version\n",
      "\n",
      "0.9.13\n",
      "\n",
      "### Steps to Reproduce\n",
      "\n",
      "nodes = [\n",
      "TextNode(\n",
      "text=\"The Shawshank Redemption\",\n",
      "metadata={\n",
      "\"author\": \"Stephen King\",\n",
      "\"theme\": \"Friendship\",\n",
      "},\n",
      "),\n",
      "TextNode(\n",
      "text=\"The Godfather\",\n",
      "metadata={\n",
      "\"director\": \"Francis Ford Coppola\",\n",
      "\"theme\": \"Mafia\",\n",
      "},\n",
      "),\n",
      "TextNode(\n",
      "text=\"Inception\",\n",
      "metadata={\n",
      "\"director\": \"Christopher Nolan\",\n",
      "},\n",
      "),\n",
      "]\n",
      "\n",
      "filters = MetadataFilters(\n",
      "    filters=[\n",
      "        MetadataFilter(key=\"theme\", value=\"Mafia\"),\n",
      "        MetadataFilter(key=\"author\", value=\"Stephen King\"),\n",
      "    ],\n",
      "    condition=FilterCondition.OR,\n",
      ")\n",
      "\n",
      "retriever = index.as_retriever(filters=filters)\n",
      "\n",
      "### Relevant Logs/Tracbacks\n",
      "\n",
      "_No response_\n",
      "\u001b[0mThere were two issues created on 12/11. One was a feature request to make llama-index compatible with models finetuned and hosted on modal.com. The reason for this feature request was that the user read about the capabilities of modal.com and thought it would be a good integration feature for llama-index. The value of this feature would be to allow users who host their models on modal.com to use llama-index for their RAG and prompt engineering pipelines.\n",
      "\n",
      "The other issue created on 12/11 was a bug report regarding the metadata filter not working correctly with Elastic search indexing. The bug description mentioned that when retrieving from ES with multiple metadata filter conditions (OR/AND), it always performs an AND operation even if it's explicitly mentioned as OR. The version mentioned in the bug report was 0.9.13. The steps to reproduce the bug were also provided, along with the relevant code snippets. However, there was no response or relevant logs/tracebacks provided for this bug report.\n"
     ]
    }
   ],
   "source": [
    "response = pack.run(\"Tell me about some issues on 12/11\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Tell me about some open issues related to agents\n",
      "\u001b[0mUsing query str: agents\n",
      "Using filters: [('state', '==', 'open')]\n",
      "\u001b[1;3;38;5;200mRetrieved node with id, entering: 9472\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9472: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Feature Request]: Add stop words to ReAct agent\n",
      "### Feature Description\n",
      "\n",
      "The ReAct agent does not use any stop words and the current API does not allow these to be passed to the LLM API.\n",
      "When using the ReAct agent chat abstraction the LLM often will generate an entire conversation before this output is collected by llama-index and then trimmed to the first `Thought:`, `Action:` set.\n",
      "\n",
      "This is very, very slow for some models.\n",
      "\n",
      "A better approach would be to use any available stop word setting in the APIs llama-index calls, or to instead use a streaming approach and implement stop words when possible this way.\n",
      "\n",
      "Additionally stop words should be plumbed up to the chat, query, etc API. This could probably be its own issue.\n",
      "\n",
      "### Reason\n",
      "\n",
      "`ReActOutputParser` selects the first `Thought:`, `Action:` set to act on. This hides that the LLM is doing a lot of useless work.\n",
      "\n",
      "`ReActAgent` should probably inject a stop word. If you build a chat or query from this the LLM will do a lot of work before its output is truncated to the first `Thought:`, `Action:` block.\n",
      "\n",
      "### Value of Feature\n",
      "\n",
      "LLM usage is expensive and especially slow when working locally. Currently with a variety of models, the ReAct agent is very inefficient because it generates large outputs containing many `Thought:`, `Action:` blocks and truncates to the first one. It should just avoid generating these large blocks with a stop word or by using streaming if available and stopping after the first block.\n",
      "\n",
      "This would reduce cost and significantly increase speed for local inference.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 9565\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9565: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Question]: Connecting to Mixtral 8x7b Chat on together.ai\n",
      "### Question Validation\n",
      "\n",
      "- [X] I have searched both the documentation and discord for an answer.\n",
      "\n",
      "### Question\n",
      "\n",
      "I am trying to connect to mistral chat model on together.ai\n",
      "\n",
      "model is defined as OpenAILike\n",
      "llm = OpenAILike(\n",
      "    model=\"DiscoResearch/DiscoLM-mixtral-8x7b-v2\",\n",
      "    api_base=\"https://api.together.xyz/v1\",\n",
      "    api_key=\"<secret key>\",\n",
      "    temperatue=0.1\n",
      ")\n",
      "\n",
      "but I am not getting any responses as I suspect that model is expecting specific prompt template.\n",
      "Anyone managed to make it work, quick sample would be appreciated ?\n",
      "\u001b[0mThere are two open issues related to agents. The first issue is a feature request to add stop words to the ReAct agent. The issue describes that the ReAct agent does not currently use any stop words, which results in slow performance for some models. The request suggests using stop words in the APIs llama-index calls or implementing a streaming approach to improve efficiency. The second issue is a question about connecting to the Mistral 8x7b Chat model on together.ai. The user is seeking assistance in making the model work and is looking for a quick sample.\n"
     ]
    }
   ],
   "source": [
    "response = pack.run(\"Tell me about some open issues related to agents\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever-only\n",
    "\n",
    "We can also get the retriever module and just run that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Tell me about some open issues related to agents\n",
      "\u001b[0mUsing query str: agents\n",
      "Using filters: [('state', '==', 'open')]\n",
      "\u001b[1;3;38;5;200mRetrieved node with id, entering: 9653\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9653: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: [Feature Request]: Add multi-filter single key solution\n",
      "### Feature Description\n",
      "\n",
      "This is following up on what the bot suggested in this ticket: [https://github.com/run-llama/llama_index/issues/9627](https://github.com/run-llama/llama_index/issues/9627).\n",
      "\n",
      "I need functionality for multi-filter, single-key with chroma in particular. Something like this, for example:\n",
      "key=\"Month\", value=[\"September\", \"October\"] with an OR filter condition and an IN operator\n",
      "As far as I understand, this is not currently supported.\n",
      "\n",
      "I have the following working solution and am hoping this or something similar could be merged into the repo:\n",
      "\n",
      "1) vector_stores/types.py\n",
      "\n",
      "```\n",
      "class MetadataFilter(BaseModel):\n",
      "    key: str\n",
      "    value: Union[StrictInt, StrictFloat, StrictStr, List[Union[StrictInt, StrictFloat, StrictStr]]]\n",
      "    operator: FilterOperator = FilterOperator.EQ\n",
      "```\n",
      "\n",
      "2) vector_stores/chroma.py\n",
      "```\n",
      "def _transform_chroma_filter_operator(operator: str) -> str:\n",
      "    \"\"\"Translate standard metadata filter operator to Chroma specific spec.\"\"\"\n",
      "    if operator == \"!=\":\n",
      "        return \"$ne\"\n",
      "    elif operator == \"==\":\n",
      "        return \"$eq\"\n",
      "    elif operator == \">\":\n",
      "        return \"$gt\"\n",
      "    elif operator == \"<\":\n",
      "        return \"$lt\"\n",
      "    elif operator == \">=\":\n",
      "        return \"$gte\"\n",
      "    elif operator == \"<=\":\n",
      "        return \"$lte\"\n",
      "    elif operator == \"in\":\n",
      "        return \"$in\"\n",
      "    else:\n",
      "        raise ValueError(f\"Filter operator {operator} not supported\")\n",
      "```\n",
      "\n",
      "\n",
      "3) vector_stores/chroma.py\n",
      "```\n",
      "\n",
      "def _to_chroma_filter(\n",
      "    standard_filters: MetadataFilters,\n",
      ") -> dict:\n",
      "    \"\"\"Translate standard metadata filters to Chroma specific spec.\"\"\"\n",
      "    filters = {}\n",
      "    filters_list = []\n",
      "    condition = standard_filters.condition or \"and\"\n",
      "    condition = _transform_chroma_filter_condition(condition)\n",
      "\n",
      "    if standard_filters.filters:\n",
      "        for filter in standard_filters.filters:\n",
      "            if filter.operator:\n",
      "                operator = _transform_chroma_filter_operator(filter.operator)\n",
      "                # Handle list values for 'in' operator\n",
      "                if filter.operator == FilterOperator.IN and isinstance(filter.value, list):\n",
      "                    filters_list.append({filter.key: {\"$in\": filter.value}})\n",
      "                else:\n",
      "                    filters_list.append({filter.key: {operator: filter.value}})\n",
      "            else:\n",
      "                # Assuming default behavior for filters without an explicit operator\n",
      "                filters_list.append({filter.key: filter.value})\n",
      "\n",
      "    if len(filters_list) == 1:\n",
      "        # If there is only one filter, return it directly\n",
      "        return filters_list[0]\n",
      "    elif len(filters_list) > 1:\n",
      "        # Combine multiple filters based on the specified condition\n",
      "        filters[condition] = filters_list\n",
      "\n",
      "    return filters\n",
      "```\n",
      "\n",
      "\n",
      "And then this gets initialized like this:\n",
      "\n",
      "```\n",
      "\n",
      "metadata_filters = MetadataFilters(\n",
      "    filters=[\n",
      "        MetadataFilter(key=\"Month\", value=[\"September\", \"October\"], operator=FilterOperator.IN)\n",
      "    ],\n",
      "    condition=FilterCondition.OR\n",
      ")\n",
      "```\n",
      "\n",
      "The functionality this doesn't support yet is something like this:\n",
      "key=\"Day\", value=[7, 14] with an AND filter and operators [GTE, LTE].\n",
      "\n",
      "Would love to have that functionality as well and then need to extend it to the other vector stores.\n",
      "\n",
      "Thanks again\n",
      "\n",
      "### Reason\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Value of Feature\n",
      "\n",
      "_No response_\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 9655\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 9655: Tell me about some open issues related to agents\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Add new relational node parse for markdown\n",
      "# Description\n",
      "\n",
      "Add a new relational node parser for markdown working as similarly as possible to the unstructured_node_element node parser, so that we can unify/generalize  the approach in the future.\n",
      "\n",
      "## Type of Change\n",
      "\n",
      "Please delete options that are not relevant.\n",
      "\n",
      "- [X] New feature (non-breaking change which adds functionality)\n",
      "- [ ] This change requires a documentation update ?\n",
      "\n",
      "# How Has This Been Tested?\n",
      "\n",
      "Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\n",
      "\n",
      "- [X] Added new unit/integration tests\n",
      "- [ ] Added new notebook (that tests end-to-end)\n",
      "- [X] I stared at the code and made sure it makes sense\n",
      "\n",
      "# Suggested Checklist:\n",
      "\n",
      "- [x] I have performed a self-review of my own code\n",
      "- [X] I have commented my code, particularly in hard-to-understand areas\n",
      "- [ ] I have made corresponding changes to the documentation\n",
      "- [ ] I have added Google Colab support for the newly added notebooks.\n",
      "- [X] My changes generate no new warnings\n",
      "- [X] I have added tests that prove my fix is effective or that my feature works\n",
      "- [X] New and existing unit tests pass locally with my changes\n",
      "- [X] I ran `make format; make lint` to appease the lint gods\n",
      "\u001b[0mNumber of source nodes: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'created_at': '2023-12-21T15:55:48Z',\n",
       " 'url': 'https://api.github.com/repos/run-llama/llama_index/issues/9653',\n",
       " 'source': 'https://github.com/run-llama/llama_index/issues/9653',\n",
       " 'labels': ['enhancement', 'triage'],\n",
       " 'index_id': '9653'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = pack.get_modules()[\"recursive_retriever\"]\n",
    "nodes = retriever.retrieve(\"Tell me about some open issues related to agents\")\n",
    "print(f\"Number of source nodes: {len(nodes)}\")\n",
    "nodes[0].node.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hub",
   "language": "python",
   "name": "llama_hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
