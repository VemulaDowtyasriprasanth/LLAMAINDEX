{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a58112-8a18-49ad-9c8c-2088778613d0",
   "metadata": {},
   "source": [
    "# RAG Fusion Query Pipeline\n",
    "\n",
    "This notebook shows how to implement RAG Fusion using the LlamaIndex Query Pipeline syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5bdbf5-d525-42e2-ab4a-19234c023491",
   "metadata": {},
   "source": [
    "## Setup / Load Data\n",
    "\n",
    "We load in the pg_essay.txt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b91e78-d733-44dd-b9a2-7c6eab3aee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-09 21:50:55--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: â€˜pg_essay.txtâ€™\n",
      "\n",
      "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-01-09 21:50:56 (5.83 MB/s) - â€˜pg_essay.txtâ€™ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1785ffb1-5fc2-4855-854a-238003ff848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc7134-a34b-41de-b7e6-c85a6e356b56",
   "metadata": {},
   "source": [
    "### [Optional] Setup Tracing\n",
    "\n",
    "We also setup tracing through Arize Phoenix to look at our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32462fb-07e0-4e5d-871c-98a782363330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://127.0.0.1:6006/\n",
      "ðŸ“º To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "px.launch_app()\n",
    "import llama_index\n",
    "\n",
    "llama_index.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071ae58a-f9e2-42c7-a22d-fb4dcbac74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    FnComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from typing import Dict, Any, List, Optional\n",
    "from llama_index.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.llms.llm import LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import Document, VectorStoreIndex, ServiceContext\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    results: List[List[NodeWithScore]],\n",
    ") -> List[NodeWithScore]:\n",
    "    \"\"\"Apply reciprocal rank fusion.\n",
    "\n",
    "    The original paper uses k=60 for best results:\n",
    "    https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n",
    "    \"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results:\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "chunk_sizes = [512]\n",
    "query_engines = []\n",
    "retrievers = {}\n",
    "for chunk_size in chunk_sizes:\n",
    "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "    query_engines.append(vector_index.as_query_engine())\n",
    "\n",
    "    retrievers[str(chunk_size)] = vector_index.as_retriever()\n",
    "\n",
    "# define rerank component\n",
    "rerank_component = FnComponent(fn=reciprocal_rank_fusion)\n",
    "\n",
    "# construct query pipeline\n",
    "p = QueryPipeline()\n",
    "module_dict = {\n",
    "    **retrievers,\n",
    "    \"input\": InputComponent(),\n",
    "    \"summarizer\": TreeSummarize(),\n",
    "    # NOTE: Join args\n",
    "    \"join\": ArgPackComponent(),\n",
    "    \"reranker\": rerank_component,\n",
    "}\n",
    "p.add_modules(module_dict)\n",
    "# add links from input to retriever (id'ed by chunk_size)\n",
    "for chunk_size in chunk_sizes:\n",
    "    p.add_link(\"input\", str(chunk_size))\n",
    "    p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))\n",
    "p.add_link(\"join\", \"reranker\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1266724-2878-46f5-a721-703a56c7cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on YC, hacked, and wrote essays during their time at YC.\n"
     ]
    }
   ],
   "source": [
    "response = p.run(query=\"What did the author do during YC?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af9071-0cf3-4b87-a26b-9e76d997acb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5c7aac-1870-47c8-af82-55d7893e8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    FnComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from typing import Dict, Any, List, Optional\n",
    "from llama_index.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.llms.llm import LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import Document, VectorStoreIndex, ServiceContext\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "# construct query pipeline\n",
    "p = QueryPipeline()\n",
    "module_dict = {\n",
    "    # \"llm\": llm,\n",
    "    # \"prompt_tmpl\": prompt_tmpl,\n",
    "    \"input\": InputComponent(),\n",
    "    \"summarizer\": TreeSummarize(service_context=service_context),\n",
    "    \"retriever\": retriever,\n",
    "}\n",
    "p.add_modules(module_dict)\n",
    "# p.add_link(\"input\", \"prompt_tmpl\")\n",
    "# p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "# p.add_link(\"llm\", \"retriever\")\n",
    "# p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n",
    "# p.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea1a347-fa6a-4978-81d6-57fe54eed7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on Y Combinator (YC) and had multiple responsibilities during his time there. Initially, he intended to split his time between hacking, writing essays, and working on YC. However, as YC grew and he became more excited about it, it started to take up a significant portion of his attention. He worked on selecting and helping founders, as well as writing all of YC's internal software. Additionally, he mentioned that YC's biggest source of stress for him was Hacker News (HN), which he created as a news aggregator for startup founders. Despite the stress, working on YC was engaging and allowed him to learn a lot about startups.\n"
     ]
    }
   ],
   "source": [
    "# response = p.run(topic=\"Yc\")\n",
    "# print(str(response))\n",
    "response = p.run(input=\"What did the author do during his time at YC?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62732a47-94bd-4a88-b372-e99c67cc35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 62153dde-a31f-4566-8456-1d9d37ffda46 with input: \n",
      "topic: apples\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module a1ba6997-2b57-4107-b0f5-1849fb252b79 with input: \n",
      "messages: Ask if jerry likes the following apples\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module f1e4f64e-c253-439a-9e6d-8008bf8511be with input: \n",
      "query_str: assistant: Does Jerry like these apples?\n",
      "\n",
      "\u001b[0mYes, Jerry likes these apples.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    FnComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from typing import Dict, Any, List, Optional\n",
    "from llama_index.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.llms.llm import LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import Document, VectorStoreIndex, ServiceContext\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "from llama_index.schema import NodeWithScore, TextNode\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt_str = \"Ask if jerry likes the following {topic}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "# construct query pipeline\n",
    "\n",
    "test_node = NodeWithScore(node=TextNode(text=\"Jerry likes apples.\"))\n",
    "# summarize_c = TreeSummarize(service_context=service_context).as_query_component(partial={\"nodes\": [test_node]})\n",
    "summarize_c = TreeSummarize().as_query_component(partial={\"nodes\": [test_node]})\n",
    "# p = QueryPipeline(chain=[summarize_c])\n",
    "# response = p.run(query_str=\"What does jerry like\")\n",
    "# print(str(response))\n",
    "\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm, summarize_c], verbose=True)\n",
    "response = p.run(topic=\"apples\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9299c-06d6-4710-afb0-3cc13761f358",
   "metadata": {},
   "source": [
    "## Setup Llama Pack\n",
    "\n",
    "Next we download the LlamaPack. All the code is in the downloaded directory - we encourage you to take a look to see the QueryPipeline syntax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5143a5f-7cd3-4aac-99e7-a357f3239f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use `download_llama_pack`\n",
    "# from llama_index.llama_pack import download_llama_pack\n",
    "\n",
    "# RAGFusionPipelinePack = download_llama_pack(\n",
    "#     \"RAGFusionPipelinePack\",\n",
    "#     \"./rag_fusion_pipeline_pack\",\n",
    "#     skip_load=True,\n",
    "#     # leave the below line commented out if using the notebook on main\n",
    "#     llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llm_compiler_pack/llama_hub\"\n",
    "# )\n",
    "\n",
    "# Option 2: Import from llama_hub package\n",
    "from llama_hub.llama_packs.query.rag_fusion_pipeline.base import RAGFusionPipelinePack\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e4786b6-6add-4caf-9f9c-c2d8f455702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pack = RAGFusionPipelinePack(documents, llm=OpenAI(model=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a84473-0b2f-4649-be8e-7a603f526f02",
   "metadata": {},
   "source": [
    "## Inspecting the Code\n",
    "\n",
    "If we take a look at how it's setup (in your downloaded directory, you'll see the following code using our QueryPipeline syntax). \n",
    "\n",
    "`retrievers` is a dictionary mapping a chunk size to retrievers (chunk sizes: 128, 256, 512, 1024). \n",
    "\n",
    "```python\n",
    "# construct query pipeline\n",
    "p = QueryPipeline()\n",
    "module_dict = {\n",
    "    **self.retrievers,\n",
    "    \"input\": InputComponent(),\n",
    "    \"summarizer\": TreeSummarize(),\n",
    "    # NOTE: Join args\n",
    "    \"join\": ArgPackComponent(),\n",
    "    \"reranker\": rerank_component,\n",
    "}\n",
    "p.add_modules(module_dict)\n",
    "# add links from input to retriever (id'ed by chunk_size)\n",
    "for chunk_size in self.chunk_sizes:\n",
    "    p.add_link(\"input\", str(chunk_size))\n",
    "    p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))\n",
    "p.add_link(\"join\", \"reranker\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
    "```\n",
    "\n",
    "We visualize the DAG below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34143d87-163f-4246-8ee5-15940d17c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_dag.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"rag_dag.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x177b104c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(pack.query_pipeline.dag)\n",
    "net.show(\"rag_dag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065ef0c4-0c9e-4611-8de4-98b2a7fe3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author wrote short stories and tried writing programs on an IBM 1401 computer during their school days. They also mentioned painting still lives in their bedroom at night.\n"
     ]
    }
   ],
   "source": [
    "response = pack.run(query=\"What did the author do growing up?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4ef382-fd11-4a82-8f5d-b2c53a21c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92cdad-9c17-4aa2-8fa5-1c6d949f80ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hub",
   "language": "python",
   "name": "llama_hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
